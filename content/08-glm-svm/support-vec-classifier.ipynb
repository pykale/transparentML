{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector classifiers\n",
    "\n",
    "This section introduces the concept of an optimal separating hyperplane, the maximal margin classifier, and the support vector classifier, laying the groundwork for the support vector machine. A separating hyperplane is a hyperplane that separates two classes of data. The maximal margin classifier is a separating hyperplane that maximizes the distance between the hyperplane and the nearest data points from either class. The maximal margin classifier is a special case of a support vector machine.\n",
    "\n",
    "Watch the 11-minute video below for a visual explanation of maximal margin, soft margin, and support vector classifier.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/efR1C6CvhmE?start=40&end=743\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining maximal margin, soft margin, and support vector classifier by StatQuest](https://www.youtube.com/embed/efR1C6CvhmE?start=40&end=743), embedded according to [YouTube's Terms of Service](https://www.youtube.com/static?gl=CA&template=terms).\n",
    "```\n",
    "\n",
    "## Hyperplane for classification\n",
    "\n",
    "Firstly, we define a hyperplane and introduce the concept of an optimal separating hyperplane.\n",
    "\n",
    "### Hyperplane and its two sides\n",
    "\n",
    "In a $D$-dimensional space, a hyperplane is a flat affine subspace of dimension $D − 1$. For example, in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line. In three dimensions, a hyperplane is a flat two-dimensional subspace—that is, a plane. In $D > 3$ dimensions, it can be hard to visualise a hyperplane, but the notion of a $(D − 1)$-dimensional flat subspace still applies.\n",
    "\n",
    "The mathematical definition of a hyperplane is quite simple. In two dimensions, a hyperplane is defined by the equation,\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 = 0\n",
    "$$\n",
    "\n",
    "where $\\beta_0, \\beta_1, \\beta_2$ are parameters. When we say that the above equation \"defines\" the hyperplane, we mean that any $\\mathbf{x} = [x_1, x_2]^\\top $ satisfying the equation is a point on the hyperplane. Note the equation above is simply the equation of a line, since indeed in two dimensions a hyperplane is a line. In the $D$-dimensional case, the equation defining a hyperplane is\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_D x_D = 0\n",
    "$$\n",
    "\n",
    "where $\\beta_0, \\beta_1, \\beta_2, \\cdots, \\beta_D$ are parameters. In this equation, any $\\mathbf{x} = [x_1, x_2, \\cdots, x_D]^\\top$ satisfying the above equation is a point on the hyperplane. If $\\mathbf{x} $ does not satisfy the equation, e.g.\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_D x_D > 0\n",
    "$$\n",
    "\n",
    "then $\\mathbf{x} $ is _on one side of the hyperplane_, and if\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_D x_D < 0\n",
    "$$\n",
    "\n",
    "then $\\mathbf{x} $ is _on the other side of the hyperplane_. So we can think of the hyperplane as dividing $D$-dimensional space into two halves. One can easily determine on which side of the hyperplane a point lies by simply calculating\n",
    "the sign of the left hand side of the hyperplane equation. \n",
    "\n",
    "\n",
    "### Classification using a separating hyperplane\n",
    "\n",
    "Suppose we have a set of observations (samples) $\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_N$ in $D$-dimensional space\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_1 = \\begin{bmatrix} x_{11} \\\\ x_{12} \\\\ \\vdots \\\\ x_{1D} \\end{bmatrix}, \\cdots, \\mathbf{x}_N = \\begin{bmatrix} x_{N1} \\\\ x_{N2} \\\\ \\vdots \\\\ x_{ND} \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "These observations fall into two classes, $y_1, \\cdots, y_N \\in \\{-1, 1\\}$, where 1 represents one class and -1 the other class. We also have a test observation $ \\mathbf{x}^* = [x^*_1, \\cdots, x^*_N]^\\top$ that we would like to classify. We can do so by finding the hyperplane that separates the two classes. Such a hyperplane is known as a _separating hyperplane_. {numref}`svm1` below shows a hyperplane in two dimensions.\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/pykale/transparentML/main/content/images/svm/svm1.png\n",
    "---\n",
    "width: 400px\n",
    "name: svm1\n",
    "---\n",
    "The hyperplane $1 + 2x_1 + 3x_2 = 0$ is shown. The blue region (top right) is the set of points for which $1 + 2x_1 + 3x_2 > 0$, and the purple region (bottom left) is the set of points for which $1 + 2x_1 + 3x_2$ < 0 (source of figures in this section: [https://trevorhastie.github.io/ISLR/](https://trevorhastie.github.io/ISLR/)).\n",
    "```\n",
    "\n",
    "We can label the observations from the blue class as $ y_n = 1 $ and those from the purple class as $ y_n = −1 $. Then a separating hyperplane has the property that\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} + \\cdots + \\beta_D x_{nD} > 0, \\text{ if } y_n = 1\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} + \\cdots + \\beta_D x_{nD} < 0, \\text{ if } y_n = -1.\n",
    "$$\n",
    "\n",
    "Equivalently, we can write this compactly as\n",
    "\n",
    "$$\n",
    "y_n(\\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} + \\cdots + \\beta_D x_{nD}) > 0, \\text{ for } n = 1, \\cdots, N.\n",
    "$$\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/pykale/transparentML/main/content/images/svm/svm2.png\n",
    "---\n",
    "width: 700px\n",
    "name: svm2\n",
    "---\n",
    "Two classes of observations are shown in blue and in purple, respectively. Left: Three separating hyperplanes, out of many possible, are shown in black. Right: A separating hyperplane is shown in black. The blue and purple grid indicates the decision rule made by a classifier based on this separating hyperplane: a test observation that falls in the blue portion of the grid will be assigned to the blue class, and one that falls in the purple portion of the grid will be assigned to the purple class.\n",
    "```\n",
    "\n",
    "As shown in {numref}`svm2` above, if a separating hyperplane exists, we can use it to construct a very natural classifier: a test observation is assigned a class depending on which side of the hyperplane it is located. \n",
    "\n",
    "### The maximal margin classifier (hard margin)\n",
    "\n",
    "In general, if the data can be perfectly separated using a hyperplane, then there will in fact exist an infinite number of such hyperplanes. This is because a given separating hyperplane can usually be shifted a tiny bit up or down, or rotated, without coming into contact with any of the observations.\n",
    "\n",
    "A natural choice is the maximal margin hyperplane (also known as the optimal separating hyperplane), which is the separating hyperplane that is farthest from the training observations. That is, we can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the _margin_. The _maximal margin hyperplane_ is the separating hyperplane for which the margin is largest, i.e. the hyperplane that has the farthest minimum distance to the training observations. We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known as the _maximal margin classifier_. We hope that a classifier that has a large margin on the training data will also have a large margin on the test data, and hence will classify the test observations correctly. \n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/pykale/transparentML/main/content/images/svm/svm3.png\n",
    "---\n",
    "width: 400px\n",
    "name: svm3\n",
    "---\n",
    "Two classes of observations are shown in blue and in purple, respectively. The _maximal margin hyperplane_ is shown as a solid line. The margin is the distance from the solid line to either of the dashed lines. The two blue points and the purple point that lie on the dashed lines are the _support vectors_, and the distance from those points to the hyperplane is indicated by arrows. The purple and blue grid indicates the decision rule made by a classifier based on this separating hyperplane.\n",
    "```\n",
    "\n",
    "{numref}`svm3` above shows a maximal margin hyperplane. Comparing to the right-hand panel of {numref}`svm2`, we see that the maximal margin hyperplane in {numref}`svm3` results in a greater minimal distance between the observations and the separating hyperplane, i.e. a larger _margin_. We can also see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These three observations are known as _support vectors_, and the distance from the support vectors to the hyperplane is known as the _margin width_.\n",
    "\n",
    "### Construction of maximal margin classifier\n",
    "\n",
    "The maximal margin hyperplane is the solution to the following optimisation problem\n",
    "\n",
    "```{math}\n",
    ":label: eq:max-margin-classifier\n",
    "\\begin{aligned}\n",
    "& \\max_{\\beta_0, \\beta_1, \\cdots, \\beta_D} M \\\\\n",
    "& \\text{subject to } \\sum_{d=1}^D \\beta_d^2 = 1 ,\\\\\n",
    "& y_n(\\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} + \\cdots + \\beta_D x_{nD}) \\geq M, \\text{ for } n = 1, \\cdots, N.\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "where $ y_n(\\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} + \\cdots + \\beta_D x_{nD}) $ in the constraint represents the perpendicular distance from the $n\\text{th}$ observation to the hyperplane. This constraint guarantees that each observation is on the correct side of the hyperplane and at least a distance $ M $ from the hyperplane. Therefore, $M$ represents the margin of our hyperplane, and the optimisation problem chooses $ \\beta_0, \\beta_1, \\cdots, \\beta_D $ to maximise M. This is exactly the definition of the maximal margin hyperplane! The problem {eq}`eq:max-margin-classifier` can be solved efficiently via quadratic programming, but details of this optimisation are outside of the scope of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector classifier (soft margin)\n",
    "\n",
    "### The non-separable case\n",
    "\n",
    "The maximal margin classifier is a very natural way to perform classification, if a separating hyperplane exists. However, in many cases, no separating hyperplane exists, and so there is no maximal margin classifier. For the example in {numref}`svm4` below, the observations that belong to two classes are not separable by a hyperplane.\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/pykale/transparentML/main/content/images/svm/svm4.png\n",
    "---\n",
    "width: 400px\n",
    "name: svm4\n",
    "---\n",
    "Two classes of observations are shown in blue and in purple, respectively. The two classes are not separable by a hyperplane, and so the maximal margin classifier cannot be used.\n",
    "```\n",
    "\n",
    "Even if a separating hyperplane does exist, it is also possible that a classifier based on a separating hyperplane might not be desirable. As shown in the example in {numref}`svm5` below, a classifier based on a separating hyperplane perfectly classifying all of the training observations can lead to sensitivity to individual observations, a symptom of  _overfitting_.\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/pykale/transparentML/main/content/images/svm/svm5.png\n",
    "---\n",
    "width: 700px\n",
    "name: svm5\n",
    "---\n",
    "Left: Two classes of observations are shown in blue and in purple, along with the maximal margin hyperplane. Right: An additional blue observation has been added, leading to a dramatic shift in the maximal margin hyperplane shown as a solid line. The dashed line indicates the maximal margin hyperplane that was obtained in the absence of this additional point.\n",
    "```\n",
    "\n",
    "As we can see, the addition of a single observation in the right panel of {numref}`svm5` leads to a dramatic change in the maximal margin hyperplane, and the resulting maximal margin hyperplane has only a tiny margin. As the distance of an observation from the hyperplane can be seen as a measure of our confidence that the observation was correctly classified, this tiny margin indicates that the classifier is not very confident about its classification of the training observations. \n",
    "\n",
    "Based on the above observations, we might be willing to consider allowing some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane, rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin. This is the idea behind the _support vector classifier_, also known as the _soft margin classifier_.\n",
    "\n",
    "An example is shown in the left-hand panel of {numref}`svm6` below. Most of the observations are on the correct side of the margin. However, a small subset of the observations (1 and 8) are on the wrong side of the margin.\n",
    "\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/pykale/transparentML/main/content/images/svm/svm6.png\n",
    "---\n",
    "width: 700px\n",
    "name: svm6\n",
    "---\n",
    "Left: A support vector classifier was fit to a small dataset. The hyperplane is shown as a solid line and the margins are shown as dashed lines. Purple observations: Observations 3, 4, 5, and 6 are on the correct side of the margin, observation 2 is on the margin, and observation 1 is on the wrong side of the margin. Blue observations: Observations 7 and 10 are on the correct side of the margin, observation 9 is on the margin, and observation 8 is on the wrong side of the margin. No observations are on the wrong side of the hyperplane. Right: Same as left panel with two additional points, 11 and 12. These two observations are on the wrong side of the hyperplane and the wrong side of the margin.\n",
    "```\n",
    "\n",
    "An observation can be not only on the wrong side of the margin, but also on the wrong side of the hyperplane. In fact, when there is no separating hyperplane, such a situation is inevitable. Observations on the wrong side of the hyperplane correspond to training observations that are misclassified by the support vector classifier. The right-hand panel of {numref}`svm6` above illustrates such a scenario.\n",
    "\n",
    "\n",
    "### Construction of support vector classifier\n",
    "\n",
    "The support vector classifier classifies a test observation depending on which side of a hyperplane it lies. The hyperplane is chosen to correctly separate most of the training observations into the two classes, but may misclassify a few observations. It is the solution to the \"soft margin\" optimisation problem below that is a modification of the \"hard margin\" optimisation problem {eq}`eq:max-margin-classifier` for the maximal margin classifier:\n",
    "\n",
    "```{math}\n",
    ":label: eq:soft-margin-classifier\n",
    "\\begin{aligned}\n",
    "& \\max_{\\beta_0, \\beta_1, \\cdots, \\beta_D, \\epsilon_1, \\cdots, \\epsilon_N} M \\\\\n",
    "& \\text{subject to } \\sum_{d=1}^D \\beta_d^2 = 1 ,\\\\\n",
    "& y_n(\\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} + \\cdots + \\beta_D x_{nD}) \\geq M(1 - \\epsilon_n),\\\\\n",
    "& \\epsilon_n \\geq 0, \\sum_{n = 1}^N \\epsilon_n \\leq \\Omega, \\text{ for } n = 1, \\cdots, N,\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "where $ \\epsilon_n $ is a [slack variable](https://en.wikipedia.org/wiki/Slack_variable) that measures the amount by which the $n\\text{th}$ observation is on the wrong side of the margin. The slack variables are constrained to be non-negative, and the constraint $ \\sum_{n = 1}^N \\epsilon_n \\leq \\Omega $ places an upper bound on the total amount by which the observations can be on the wrong side of the margin. The parameter $ \\Omega $ is a tuning (hyper)parameter that controls the trade-off between the two competing goals of the support vector classifier: _maximising the margin_ and _minimising the number of training observations that are misclassified_. $ \\Omega $ can also be viewed as a _budget_ for the amount that the margin can be violated by the observations. When $ \\Omega $ is very large (highly tolerant), the support vector classifier will attempt to maximise the margin, and so it will misclassify a larger number of training observations. When $ \\Omega $ is very small (highly intolerant), the support vector classifier will attempt to minimise the number of training observations that are misclassified, and so it will have a smaller margin. In practice, $ \\Omega $ is often tuned using cross-validation. {numref}`svm7` below illustrates the effect of $ \\Omega $ on the support vector classifier.\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/pykale/transparentML/main/content/images/svm/svm7.png\n",
    "---\n",
    "width: 700px\n",
    "name: svm7\n",
    "---\n",
    "A support vector classifier was fit using four different values of the tuning parameter $ \\Omega $ of Equation {eq}`eq:soft-margin-classifier`. The largest value of $ \\Omega $ was used in the top left panel, and smaller values were used in the top right, bottom left, and bottom right panels. When $ \\Omega $ is large, then there is a high tolerance for observations being on the wrong side of the margin, and so the margin will be large. As $ \\Omega $ decreases, the tolerance for observations being on the wrong side of the margin decreases, and the margin narrows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: support vector classifiers on toy data\n",
    " <!-- using `scikit-learn` -->\n",
    "\n",
    "In this example, we will use the `scikit-learn` package to fit support vector classifiers to a small toy dataset. \n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot a classifier with support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc(svc, X, y, h=0.02, pad=0.25, show=True):\n",
    "    x_min, x_max = X[:, 0].min() - pad, X[:, 0].max() + pad\n",
    "    y_min, y_max = X[:, 1].min() - pad, X[:, 1].max() + pad\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=70, c=y, cmap=plt.cm.Paired)\n",
    "    # Support vectors indicated in plot by vertical lines\n",
    "    sv = svc.support_vectors_\n",
    "    plt.scatter(\n",
    "        sv[:, 0], sv[:, 1], c=\"k\", marker=\"x\", s=100, alpha=0.5\n",
    "    )  # , linewidths=1)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    print(\"Number of support vectors: \", svc.support_.size)\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate synthetic data randomly: 20 observations of 2 features and divide into two classes. Set a seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "X = np.random.randn(20, 2)\n",
    "y = np.repeat([1, -1], 10)\n",
    "\n",
    "X[y == -1] = X[y == -1] + 1\n",
    "plt.scatter(X[:, 0], X[:, 1], s=70, c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector visualisation and interpretation\n",
    "\n",
    "Fit a support vector classifier and visualise decision boundary and support vectors with data points. We will study different values of hyperparameter $ \\Omega $ in Equation {eq}`eq:soft-margin-classifier` to see its effect on the decision boundary. \n",
    "\n",
    "**Note:** The `C` setting of the [`SVC` class](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) in `scikit-learn` is inversely proportional to $ \\Omega $. The larger the value of `C`, the smaller the value of $ \\Omega $ (the less tolerance for observations being on the wrong side of the margin). The smaller the value of `C`, the larger the value of $ \\Omega $ (the more tolerance for observations being on the wrong side of the margin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1.0, kernel=\"linear\")\n",
    "svc.fit(X, y)\n",
    "\n",
    "plot_svc(svc, X, y, show=False)\n",
    "plt.scatter(\n",
    "    X[5, 0], X[5, 1], s=70, facecolors=\"none\", marker=\"o\", edgecolors=\"k\", linewidths=3\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input: \", X[5, :])\n",
    "print(\"Intercept: \", svc.intercept_[0], \" Coefficients: \", svc.coef_[0])\n",
    "print(\"Prediction score: \", svc.decision_function(X[5, :].reshape(1, -1))[0])\n",
    "print(\"Prediction: \", svc.predict(X[5, :].reshape(1, -1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} System transparency\n",
    ":class: important\n",
    "\n",
    "The coefficients of the support vector classifier are $ \\beta_0 \\approx 0.5729, \\beta_1 \\approx -0.7327, \\beta_2 \\approx -0.7033 $\n",
    "\n",
    "- For the data point $[-1.19276461, -0.20487651]$ marked by a black circle, its prediction score is $ -0.7327 \\times -1.19276461 + -0.7033 \\times -0.20487651 + 0.5729 \\approx 1.5910 $, which is positive. So the predicted label is $ 1 $.\n",
    "\n",
    "- According to the definition, the data points on the hyperplane satisfies $ \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} = 0 $, i.e. $ 0.5729 -0.7327 \\times x_1 -0.7033 \\times x_2 = 0 $. To get a data point $ \\mathbf{x} = [x_1, x_2]^\\top $ of class $ 1 $, the relationship between $ x_1 $ and $ x_2 $ should satisfy:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "\\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} & > 0 \\\\\n",
    "\\beta_1 x_{1} & > - \\beta_0 - \\beta_2 x_{2} \\\\\n",
    "x_{1} & > \\frac{- \\beta_0 - \\beta_2 x_{2}}{\\beta_1} \\\\\n",
    "x_{1} & > \\frac{- 0.5729 - (-0.7033 \\times x_2)}{-0.7327} \\\\\n",
    "x_1 & > 0.7819 - 0.9598 \\times x_2.\n",
    "\\end{aligned}\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we need data points $ \\mathbf{x} = [x_1, x_2]^\\top $ satisfy $ x_1 < 0.7819 - 0.9598 \\times x_2 $ for class $ -1 $. Note that the data point $ \\mathbf{x} = [x_1, x_2]^\\top $ belongs to class $ 1 $ but violates the margin if $ 0 < 0.5729 -0.7327 \\times x_1 -0.7033 \\times x_2 < 1 $, and the same applies to data points belongs to class $ -1 $ if $ -1 < 0.5729 -0.7327 \\times x_1 -0.7033 \\times x_2 < 0 $.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use a smaller value of $ \\Omega $ (`C=0.1`) to see how the decision boundary changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2 = SVC(C=0.1, kernel=\"linear\")\n",
    "svc2.fit(X, y)\n",
    "plot_svc(svc2, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, when using a smaller `C=0.1`, we have a larger $ \\Omega $ and a larger tolerance for observations being on the wrong side of the margin. As a result, the margin is wider, resulting in more support vectors. We can select the optimal hyperparameter `C` in `scikit-learn` (inversely proportional to $ \\Omega $) using cross-validation, as we do below using `GridSearchCV` from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "tuned_parameters = [{\"C\": [0.001, 0.01, 0.1, 1, 5, 10, 100]}]\n",
    "clf = GridSearchCV(\n",
    "    SVC(kernel=\"linear\"),\n",
    "    tuned_parameters,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "clf.fit(X, y)\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the optimal hyperparameter `C` in `scikit-learn` (inversely proportional to $ \\Omega $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with support vector classifiers\n",
    "\n",
    "Generate a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_test = np.random.randn(20, 2)\n",
    "y_test = np.random.choice([-1, 1], 20)\n",
    "X_test[y_test == 1] = X_test[y_test == 1] - 1\n",
    "\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=70, c=y_test, cmap=plt.cm.Paired)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Predict the class labels for the test dataset and display the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc2 : C = 0.1\n",
    "y_pred = svc2.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svc.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat with a smaller value of `C` (larger $ \\Omega $ and larger tolerance) to see how the confusion matrix changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc3 = SVC(C=0.001, kernel=\"linear\")\n",
    "svc3.fit(X, y)\n",
    "\n",
    "y_pred = svc3.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svc.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction results are the same as the previous case for this particular example.\n",
    "\n",
    "### Decision boundary on linearly separable data\n",
    "\n",
    "Let us modify the test dataset so that the classes are linearly separable with a hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[y_test == 1] = X_test[y_test == 1] - 1\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=70, c=y_test, cmap=plt.cm.Paired)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `C` to be very large (meaning small $ \\Omega $ and small tolerance) to see whether the decision boundary can separate the two classes perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc4 = SVC(C=10.0, kernel=\"linear\")\n",
    "svc4.fit(X_test, y_test)\n",
    "plot_svc(svc4, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we have a perfect separation of the two classes.\n",
    "\n",
    "Let us increase the margin tolerance (reduce `C` in `scikit-learn`) to see how the decision boundary changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc5 = SVC(C=1, kernel=\"linear\")\n",
    "svc5.fit(X_test, y_test)\n",
    "plot_svc(svc5, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is one misclassification. Thus, this version (with `C=1`) has lowered variance (and overfitting) at the expense of increased bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** All the following exercises use the [Heart](https://github.com/pykale/transparentML/blob/main/data/Heart.csv) dataset.\n",
    "\n",
    "Load the **Heart** dataset, convert the values of variables (predictors) from categorical values to numerical values, and inspect the first five rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(2022)\n",
    "\n",
    "heart_url = \"https://github.com/pykale/transparentML/raw/main/data/Heart.csv\"\n",
    "\n",
    "heart_df = pd.read_csv(heart_url, index_col=0).dropna()\n",
    "# converting categories\n",
    "heart_df[\"ChestPain\"] = heart_df[\"ChestPain\"].factorize()[0]\n",
    "heart_df[\"Thal\"] = heart_df[\"Thal\"].factorize()[0]\n",
    "heart_df[\"AHD\"] = heart_df[\"AHD\"].factorize()[0]\n",
    "\n",
    "heart_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. Split the loaded dataset into training and testing sets in an $80:20$ ratio, then train a model using SVC using the hyperparameter $C = 1$, and show the number of support vectors and accuracy of this trained model on the test set. (Use $2022$ as the random seed value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(2022)\n",
    "\n",
    "X = heart_df.drop([\"AHD\"], axis=1)\n",
    "y = heart_df[\"AHD\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y.ravel(), test_size=0.2, random_state=2022\n",
    ")\n",
    "\n",
    "svc = SVC(C=1, kernel=\"linear\", random_state=2022)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Number of Support vectors: \", len(svc.support_vectors_))\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Train another model in the same manner as in **Exercise 2**, but with the hyperparameter regularisation strength $C$ set to $0.01$. Show the number of support vectors and accuracy of this trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "svc2 = SVC(C=0.01, kernel=\"linear\", random_state=2022)\n",
    "svc2.fit(X_train, y_train)\n",
    "print(\"Number of Support vectors: \", len(svc2.support_vectors_))\n",
    "\n",
    "y_pred = svc2.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Using GridSearchCV, fine-tune the support vector classifier's hyperparameter $C$ on the training dataset (using $10$-fold cross-validation) for $C$ values of $0.001$, $0.01$, $0.1$, and $5$, and then display the best hyperparameters. (Use the accuracy scoring for choosing the best model and its hyperparameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_parameters = [{\"C\": [0.001, 0.01, 0.1, 1, 5]}]\n",
    "clf = GridSearchCV(\n",
    "    SVC(kernel=\"linear\"),\n",
    "    tuned_parameters,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "clf.fit(X, y)\n",
    "print(\"\\nOptimal value of C : \", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Finally, using the best hyperparameter from **Exercise 4**, train another SVC model on the training dataset and show the accuracy of this trained model on the test set. Also, show the confusion matrix for this trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "svc_tuned = SVC(C=0.1, kernel=\"linear\")\n",
    "svc_tuned.fit(X_train, y_train)\n",
    "print(\"Number of Support vectors: \", len(svc_tuned.support_vectors_))\n",
    "\n",
    "y_pred = svc_tuned.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svc_tuned.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
