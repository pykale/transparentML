{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression trees\n",
    "\n",
    "We will start with regression trees, a type of decision tree model that can be used for regression problems. \n",
    "\n",
    "Watch the 22-minute video below for a visual explanation of regression trees.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/g9c66TUylZ4?start=41\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> \n",
    "\n",
    "[Explaining Regression Trees by StatQuest](https://www.youtube.com/embed/g9c66TUylZ4?start=41), embedded according to [YouTube's Terms of Service](https://www.youtube.com/static?gl=CA&template=terms).\n",
    "```\n",
    "\n",
    "## Regression trees for salary prediction\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [Hitters dataset](https://github.com/pykale/transparentML/blob/main/data/Hitters.csv) (click to explore) to predict a baseball player's `Salary` based on `Years` (the number of years that the player has played in the major leagues) and `Hits` (the number of hits that the player has made in the major leagues).\n",
    "\n",
    "Load the dataset and _remove the rows with missing values_. Inspect the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_url = \"https://github.com/pykale/transparentML/raw/main/data/Hitters.csv\"\n",
    "\n",
    "hitters_df = pd.read_csv(hitters_url).dropna()\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees typically do not require feature scaling. However, take a look at the distribution of the `Salary` variable on the original scale and on the log scale, via a histogram below. We can see that the distribution of `Salary` is skewed (to the left), and that the log distribution is more symmetric, closer to a bell shape, i.e. the normal/Gaussian distribution that is preferred in many machine learning models. Therefore, we will use the _log-transform_ of `Salary` as the target variable in our regression tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hitters_df[[\"Years\", \"Hits\"]].values\n",
    "y = np.log(hitters_df[\"Salary\"].values)  # log transform the target variable\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "ax1.hist(hitters_df.Salary.values, bins=20)\n",
    "ax1.set_xlabel(\"Salary\")\n",
    "ax2.hist(y, bins=20)\n",
    "ax2.set_xlabel(\"Log(Salary)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a regression tree to the data. We will use the `DecisionTreeRegressor` class from the `sklearn.tree` module. We will set the `max_leaf_nodes` hyperparameter to 3, which means that the tree will have at most 3 leaf nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One attractive feature of a tree-based model is visualisation.  Let us visualise the learnt tree using the [`plot_tree` function](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) from the `sklearn.tree` module. We will set the `filled` parameter to `True` to colour the leaf nodes according to the predicted value (the larger the value, the darker the colour). We will set the `feature_names` parameter to `['Years', 'Hits']` to label the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))  # customize according to the size of your tree\n",
    "plot_tree(regr, filled=True, feature_names=[\"Years\", \"Hits\"], fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above depicts a regression tree fitted to the data. The tree has three leaf nodes, and each leaf node corresponds to a region in the feature space. The predicted value for each region is the average value of the `Salary` variable in the training data that falls into that region. We can examine the system logic of this simple regression tree to reveal its system transparency.\n",
    "\n",
    "```{admonition} System transparency\n",
    ":class: important\n",
    "\n",
    "- When `Years` $=5$ and `Hits` $=100$, the predicted `Salary` $=$ \\$$1000\\times e^{5.998}\\approx 403,000$, which can be read out from the tree above by following the right branch from the top first and then the left branch of the next level, reaching the leaf node in the middle. \n",
    "- To produce an estimated `Salary` of \\$$1000\\times e^{6.6}\\approx 735,095$, we find the leaf node that has a predicted value closest to $6.6$, which is the right most leaf node, and follow the path from that leaf node to the top to get that we should have `Years` $>4.5$ and `Hits` $>117.5$, e.g. `Years` $=5$ and `Hits` $=120$, for a `Salary` closest to \\$$735,095$. As we can see here, the estimation from the tree above is quite coarse, and we can get a more accurate estimation by increasing the number of leaf nodes.\n",
    "```\n",
    "\n",
    "We can see that this tree consists of a series of splitting rules starting from the top. Each rule is used to divide the feature space into two regions. For example, the first splitting rule is `Years <= 4.5`, which means that if the number of years that a player has played in the major leagues is less than or equal to 4.5, then the player will be assigned to the left child node, with a predicted salary of \\$$1000\\times e^{5.107}\\approx 165,000$. Otherwise, the player will be assigned to the right child node. The second splitting rule is `Hits <= 117.5`, which means that if the number of hits that a player has made in the major leagues is less than or equal to  117.5 then the player will be assigned to the left child node, with a predicted salary of \\$$1000\\times e^{5.998}\\approx 403,000$. Otherwise, the player will be assigned to the right child node, with a predicted salary of \\$$1000\\times e^{6.740}\\approx 845,000$.\n",
    "\n",
    "```{admonition} How to interpret this tree?\n",
    ":class: tip, dropdown\n",
    "From the above, `Years` is the most important factor (among two factors considered) in determining Salary, and players with less experience earn lower salaries than more experienced players. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries. \n",
    "```\n",
    "\n",
    "Thus, the tree is a piecewise linear function of the features. The predicted value for a new data point is the average value of the `Salary` variable in the training data that falls into the region that the new data point falls into. It is easier to interpret, and has a nice graphical representation.\n",
    "\n",
    "We can further visualise the tree by plotting the regions in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_df.plot(\"Years\", \"Hits\", kind=\"scatter\", color=\"orange\", figsize=(7, 6))\n",
    "plt.xlim(0, 25)\n",
    "plt.ylim(ymin=-5)\n",
    "plt.xticks([1, 4.5, 24])\n",
    "plt.yticks([1, 117.5, 238])\n",
    "plt.vlines(4.5, ymin=-5, ymax=250)\n",
    "plt.hlines(117.5, xmin=4.5, xmax=25)\n",
    "plt.annotate(\"R1\", xy=(2, 117.5), fontsize=\"xx-large\")\n",
    "plt.annotate(\"R2\", xy=(11, 60), fontsize=\"xx-large\")\n",
    "plt.annotate(\"R3\", xy=(11, 170), fontsize=\"xx-large\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} How to interpret this plot?\n",
    ":class: tip, dropdown\n",
    "Thus, the tree divides the feature space into tree regions: R1, R2, and R3. The first region R1 is the region where the number of years that a player has played in the major leagues is less than or equal to 4.5. The second region R2 is the region where the number of years that a player has played in the major leagues is greater than 4.5, and the number of hits that a player has made in the major leagues is less or equal to 117.5. The third region R3 is the region where the number of years that a player has played in the major leagues is greater than 4.5, and the number of hits that a player has made in the major leagues is greater than 117.5.\n",
    "```\n",
    "\n",
    "Decision trees are typically drawn upside down, with the _root node_ at the top and the _leaves_ at the bottom. The root node is the first splitting rule, and the leaves are the _terminal nodes_, regions R1, R2, and R3 in this example. The points along the tree where the feature space is divided into two regions are called _internal nodes_ or _splitting nodes_. The splitting rules are the conditions that are used to divide the feature space into two regions. The splitting rules are represented by the lines that connect the internal nodes to the leaves. These lines are called _branches_ and\n",
    "the splitting rules are also called _decision rules_.\n",
    "\n",
    "## How to build/learn a regression tree?\n",
    "\n",
    "How to build (i.e. learn) a regression tree? In other words, how to find $J$ regions best dividing (partitioning) the feature space for prediction? Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into $J$ regions. Instead, again, we take a top-down, greedy approach like in the [stepwise feature selection](https://pykale.github.io/transparentML/06-ftr-select-regularise/feature-select.html#stepwise-selection) that is known as [recursive binary splitting (recursive partitioning)](https://en.wikipedia.org/wiki/Recursive_partitioning). \n",
    "\n",
    "### Recursive binary splitting\n",
    "\n",
    "In recursive binary splitting, we start with the entire feature space as the root node (one single region), and then we successively split the feature space into two regions, in $J-1$ steps. In each step $j$, we consider all features and all possible values of the threshold $T_d$ (cutpoint, or splitting point) for each feature $x_d$, and then choose the feature and threshold such that if using it to split one of the existing $j$ regions, the resulting tree has the lowest cost (best performance). In other words, the splitting rule is chosen by finding the the feature $j$ out of a total of $D$ features $x_1, \\cdots , x_D$, with a threshold $T_j$ that splits one existing region to minimise a _cost function_ (_splitting criterion_). Thus, each step $j$ consists of $D$ sub-steps, one for each feature. In each sub-step $d$, we find the optimal threshold $T_d$ that minimises the cost function for feature $x_d$ after splitting an existing region. Then we choose the $j$th feature $x_j$ (with its optimal threshold $T_j$) that minimises the cost function. The splitting rule for step $j$ is then given by $x_j \\leq T_j$.\n",
    "\n",
    "This recursive binary splitting method is [greedy](https://en.wikipedia.org/wiki/Greedy_algorithm) because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step, as in [stepwise feature selection](https://pykale.github.io/transparentML/06-ftr-select-regularise/feature-select.html#stepwise-selection). Therefore, it is also susceptible to overfitting.\n",
    "\n",
    "### Cost function\n",
    "\n",
    "Cost functions for regression trees can be any performance metric for regression, such as the residual sum of squares (RSS) defined in [simple linear regression](https://pykale.github.io/transparentML/02-linear-reg/simple-linear-regression.html#estimating-the-coefficients) (Equation {eq}`RSSdef`) or the mean squared error (MSE) defined in [simple linear regression](https://pykale.github.io/transparentML/02-linear-reg/simple-linear-regression.html#assessing-model-accuracy-via-r-2) (Equation {eq}`MSEdef`). \n",
    "\n",
    "### When to stop splitting?\n",
    "\n",
    "In regression trees, instead of specifying the number of regions $J$ in advance, we stop splitting using a stopping criteria. The most common stopping criteria is to stop splitting when the number of data points in a region is less than or equal to a pre-specified number $R$ (e.g. 5). This is called _pre-pruning_ or early stopping. Another stopping criteria is to stop splitting when the cost function (or its changes over the last $K$ steps) is less than or equal to a pre-specified number $\\epsilon$ (e.g. 0.01). This is called _post-pruning_ or late stopping.\n",
    "\n",
    "You may refer to the advantages and disadvantages of decision trees and tips on practical use in the [`scikit-learn` documentation](https://scikit-learn.org/stable/modules/tree.html#tree).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression trees for income prediction\n",
    "\n",
    "Next, let us study the [Boston dataset](https://github.com/pykale/transparentML/blob/main/data/Boston.csv)  (click to explore). In this dataset, we want to predict the median value of owner-occupied homes in $1000s in Boston, based on 13 features (predictors) such as crime rate, average number of rooms per dwelling, and proportion of non-retail business acres per town. \n",
    "\n",
    "Load the dataset and inspect the first few rows of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_url = \"https://github.com/pykale/transparentML/raw/main/data/Boston.csv\"\n",
    "\n",
    "boston_df = pd.read_csv(boston_url)\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the structure of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training (50%) and test sets and fit a regression tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = boston_df.drop(\"medv\", axis=1)\n",
    "y2 = boston_df.medv\n",
    "train_size = 0.5\n",
    "max_depth = 3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2, train_size=train_size)\n",
    "regr_tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "regr_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 10))  # customize according to the size of your tree\n",
    "plot_tree(regr_tree, filled=True, feature_names=X2.columns, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the test set MSEs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr_tree.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** All the following exercises use the [Carseats](https://github.com/pykale/transparentML/blob/main/data/Carseats.csv) dataset.\n",
    "\n",
    "Load the **Carseats** dataset, convert the values of variables (predictors) from categorical data to numerical data, and inspect the first five rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "carseat_url = \"https://github.com/pykale/transparentML/raw/main/data/Carseats.csv\"\n",
    "\n",
    "carseat_df = pd.read_csv(carseat_url)\n",
    "carseat_df[\"ShelveLoc\"] = carseat_df[\"ShelveLoc\"].factorize()[0]\n",
    "carseat_df[\"Urban\"] = carseat_df[\"Urban\"].factorize()[0]\n",
    "carseat_df[\"US\"] = carseat_df[\"US\"].factorize()[0]\n",
    "carseat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Create a training set using $80\\%$ of the data from **Exercise 1** and use the remaining $20\\%$ for testing. Fit a **Decision Tree Regressor** to the data, predicting the **Sales** of the carseats. Calculate the **MSE** and $R^2$ score of the model. Use $2022$ as the random state value for **train_test_split** and your **DecisionTreeRegressor** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "np.random.seed(2022)\n",
    "X2 = carseat_df.drop(\"Sales\", axis=1)\n",
    "y2 = carseat_df.Sales\n",
    "train_size = 0.8\n",
    "max_depth = 3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X2, y2, train_size=train_size, random_state=2022\n",
    ")\n",
    "regr_tree = DecisionTreeRegressor(max_depth=max_depth, random_state=2022)\n",
    "regr_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regr_tree.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Now, plot the **model tree** of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(28, 10))  # customize according to the size of your tree\n",
    "plot_tree(regr_tree, filled=True, feature_names=X2.columns, fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
