{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Although having \"regression\" in its name, logistic regression is a classification (rather than regression) algorithm. Instead of modelling the qualitative response $y$ directly, logistic regression models the probability that $y$ belongs to a particular class (category).\n",
    "\n",
    "Watch the 8-minute video below for a visual explanation of logistic regression:\n",
    "\n",
    "```{admonition} Video\n",
    "\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/yIYKR4sgzI8?start=24\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining Logistic Regression by StatQuest](https://www.youtube.com/embed/yIYKR4sgzI8?start=24), embedded according to [YouTube's Terms of Service](https://www.youtube.com/static?gl=CA&template=terms).\n",
    "\n",
    "```\n",
    "\n",
    "## Import libraries and load data\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, RocCurveDisplay\n",
    "\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [Default dataset](https://github.com/pykale/transparentML/blob/main/data/Default.csv) and inspect the first three rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Default.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# Note: factorize() returns two objects: a label array and an array with the unique values. We are only interested in the first object.\n",
    "df[\"default2\"] = df.default.factorize()[0]\n",
    "df[\"student2\"] = df.student.factorize()[0]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression models the probability that the response $y$ belongs to a particular class (category) using linear _regression_ via a _logistic_ function, hence the name _logistic regression_. For binary classification, let us use 1 to denote the positive class (e.g. \"Yes\") and 0 to denote the negative class (e.g. \"No\").\n",
    "\n",
    "### Logit function transformation\n",
    "\n",
    "Let $\\pi$ be the probability of a positive outcome \n",
    "\n",
    "$$\n",
    "\\pi = \\mathbb{P}(y=1| x).\n",
    "$$\n",
    "\n",
    "Then the probability of a negative outcome $ \\mathbb{P}(y=0| x) = 1-\\pi $. \n",
    "\n",
    "The [odds](https://en.wikipedia.org/wiki/Odds) of a positive outcome is the ratio of the probability of a positive outcome to the probability of a negative outcome, i.e.\n",
    "\n",
    "$$\n",
    "\\frac{\\pi}{1-\\pi}.\n",
    "$$\n",
    "\n",
    "The log of the odds is called the [logit](https://en.wikipedia.org/wiki/Logit) and the logit function is defined as :\n",
    "\n",
    "$$\n",
    "\\mathrm{logit}(\\pi)=\\log \\frac{\\pi}{1-\\pi}\n",
    "$$ \n",
    "\n",
    "The logit function is a monotonic transformation of the probability $\\pi$. It maps the probability $\\pi$ ranging from 0 to 1 to the real line ranging from $-\\infty$ to $\\infty$. The logit function is also known as the log-odds function.\n",
    "\n",
    "### Logistic function\n",
    "\n",
    "In simple linear regression, we have modelled the relationship between outcome $y$ and feature $x$ with a linear equation:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x.\n",
    "$$\n",
    "\n",
    "This linear regression model is not able to produce a binary outcome for binary classification or a probability (in the range from 0 to 1 only). With the logit function above, we can transform the probability $\\pi$ ranging from 0 to 1 to the logit function $\\mathrm{logit}(\\pi)$ ranging from $-\\infty$ to $\\infty$. Now we can use the linear regression model to model the logit function $\\mathrm{logit}(\\pi)$ instead of the probability $\\pi$ or $y$ directly. The logit function is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{logit}(\\pi)=\\log \\frac{\\pi}{1-\\pi} = \\beta_0 + \\beta_1 x.\n",
    "$$\n",
    "\n",
    "The [logistic function](https://en.wikipedia.org/wiki/Logistic_function), also known as the sigmoid function, is the inverse of the logit function. It maps the logit function $\\mathrm{logit}(\\pi)$ ranging from $-\\infty$ to $\\infty$ to the probability $\\pi$ ranging from 0 to 1:\n",
    "\n",
    "$$\n",
    "\\pi = \\mathbb{P}(y = 1 | x) = \\mathrm{logit}^{-1}(\\beta_0 + \\beta_1 x) = \\mathrm{logistic}(\\beta_0 + \\beta_1 x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1)}}.\n",
    "$$\n",
    "\n",
    "<!-- For classification, we cannot use linear regression to model $y$ directly, because $y$ is either. Instead, we can model the logit of the probability of a positive outcome:\n",
    "\n",
    " we prefer probabilities between 0 and 1, so we wrap the right side of the equation into the logistic function. This forces the output to assume only values between 0 and 1 -->\n",
    "\n",
    "\n",
    "<!-- Specifically, instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1. \n",
    "\n",
    "$$\n",
    "\\textrm{logistic}(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the coefficients to make predictions\n",
    "\n",
    "The coefficients of a logistic regression model can be estimated by [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (MLE). The likelihood function is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta_0, \\beta_1) = \\prod_{i:y_i=1} \\mathbb{P}(y_i = 1) \\prod_{i:y_i=0} (1 - \\mathbb{P}(y_i = 1)).\n",
    "$$\n",
    "\n",
    "The mathematical details are beyond the scope of this course. If interested, please read Section 4.3 of the [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/) book for more details of the optimization for logistic regression.\n",
    "\n",
    "To make predictions, taking the classification problem on the [Default dataset](https://github.com/pykale/transparentML/blob/main/data/Default.csv), the probability of default given balance predicted by a logistic regression model is\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{default} = \\text{Yes} \\mid \\text{balance}, \\beta_0, \\beta_1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\times \\text{balance})}}.\n",
    "$$\n",
    "\n",
    "### Example explanation of system transparency\n",
    "\n",
    "Let us to see the curve of the logistic function on the [Default dataset](https://github.com/pykale/transparentML/blob/main/data/Default.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x=\"balance\", y=\"default2\", logistic=True)\n",
    "plt.ylabel(\"Probability of default\")\n",
    "plt.xlabel(\"Balance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an S-shaped curve with the vertical axis ranging from 0 to 1, indicating the probability of a positive outcome (`default`) with respect to the horizontal axis $x$ (`balance`). \n",
    "\n",
    "In practice, we can use the `scikit-learn` or `statsmodels` package to fit a logistic regression model and make predictions. \n",
    "\n",
    "Let us fit a logistic regression model using the [`scikit-learn` API for logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver=\"newton-cg\")\n",
    "X_train = df.balance.values.reshape(-1, 1)\n",
    "X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1, 1)\n",
    "y = df.default2\n",
    "clf.fit(X_train, y)\n",
    "print(clf)\n",
    "print(\"classes: \", clf.classes_)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the fitted logistic regression model and study the system transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = clf.predict_proba(X_test)\n",
    "\n",
    "plt.scatter(X_train, y, color=\"orange\")\n",
    "plt.plot(X_test, prob[:, 1], color=\"lightblue\")\n",
    "\n",
    "plt.hlines(\n",
    "    1,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "plt.hlines(\n",
    "    0,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.hlines(\n",
    "    0.5,\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=plt.gca().xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    -clf.intercept_ / clf.coef_[0],\n",
    "    ymin=plt.gca().yaxis.get_data_interval()[0],\n",
    "    ymax=plt.gca().yaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.hlines(\n",
    "    (clf.predict_proba(np.asarray(1500).reshape(-1, 1)))[0][1],\n",
    "    xmin=plt.gca().xaxis.get_data_interval()[0],\n",
    "    xmax=1500,\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    1500,\n",
    "    ymin=plt.gca().yaxis.get_data_interval()[0],\n",
    "    ymax=(clf.predict_proba(np.asarray(1500).reshape(-1, 1)))[0][1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Probability of default\")\n",
    "plt.xlabel(\"Balance\")\n",
    "plt.yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "plt.xlim(xmin=-100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we learnt a logistic regression model $f(x)$ with two parameters, $\\beta_0$ and $\\beta_1$, from the data, where\n",
    "- $\\beta_0 = -10.65133019 $ \n",
    "- $\\beta_1 = 0.00549892 $ \n",
    "\n",
    "Using these two estimated parameters, we can examine the system logic of the logistic regression model to reveal its system transparency.\n",
    "\n",
    "```{admonition} System transparency\n",
    ":class: important\n",
    "\n",
    "- When `balance`=1500 , the predicted probability of `default` is \n",
    "\n",
    "$$ f(1500) = \\frac{1}{1 + e^{\\beta_0 + \\beta_1 \\times 1500}} = \\frac{1}{1 + e^{-10.65133019 + 0.00549892 \\times 1500}} = 0.08294763. $$\n",
    "\n",
    "- When the probability of `default` 0.5 (the commonly used threshold for binary classification), the corresponding balance can be calculated using the _log odds_ equation above:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "\\ln \\left(\\frac{\\pi}{1-\\pi}\\right) &= \\beta_0 + \\beta_1 \\times x \\\\\n",
    "\\ln \\left(\\frac{0.5}{1-0.5}\\right) &= \\beta_0 + \\beta_1 \\times x \\\\\n",
    "x & = \\frac{- \\beta_0}{\\beta_1} \\\\\n",
    "x & = \\frac{- (-10.65133019)}{0.00549892} \\\\\n",
    "x & = 1936.9858426745614.\n",
    "\\end{aligned}\n",
    "\\end{align}\n",
    "\n",
    "Therefore, to produce result `Yes`, i.e. for the probability of `default` $>$ 0.5, the `balance` should be greater than 1936.9858426745614. Likewise, to produce result `No`, i.e. the probability of `default` $<$ 0.5, the `balance` should be less than 1936.9858426745614.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit a logistic regression model using the [`statsmodels` API for logistic regression] next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ balance\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the estimated parameters are the same as those from the `scikit-learn` API. Note the two last columns, \"[0.025\" and \"0.975]\". These are Confidence Intervals, the very concept we covered in the last chapter on Linear Regression! Now, let us fit a logistic regression model using `student` status as the input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ student\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient associated with `student` is positive, indicating that the probability of `default` is higher for students than non-students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple logistic regression\n",
    "\n",
    "The logistic regression model can be extended to multiple features. A generalised logistic regression model for an instance $\\mathbf{x} = [x_1, x_2, \\dots, x_D]^\\top$ is\n",
    "\n",
    "$$\n",
    "\\ln \\left( \\frac{\\pi}{1-\\pi} \\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_D x_D.\n",
    "$$\n",
    "\n",
    "Let us fit a multiple logistic regression model using the `scikit-learn` API first.\n",
    "\n",
    "Let us plot the fitted multiple logistic regression model and study the system transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train = df.loc[:, [\"balance\", \"income\", \"student2\"]]\n",
    "y = df.default2\n",
    "\n",
    "clf = LogisticRegression(solver=\"newton-cg\", penalty=\"none\", max_iter=1000)\n",
    "clf.fit(X_train, y)\n",
    "print(clf)\n",
    "print(\"classes: \", clf.classes_)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit a multiple logistic regression model using the `statsmodels` API next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = logit(\"default2 ~ balance + income + student\", df).fit()\n",
    "est.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar parameters are estimated as using the `scikit-learn` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of logistic regression\n",
    "\n",
    "The evaluation of logistic regression is similar to the evaluation of linear regression. The main difference is that the evaluation metrics are different. For example, the mean squared error (MSE) is not a good metric for classification. Here we introduce two common metrics for classification: accuracy and area under [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (ROC) curve, i.e. AUC (or more precisely, AUROC).\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "The accuracy is the fraction of correct predictions. It is defined as\n",
    "\n",
    "$$\n",
    "\\text{accuracy} = \\frac{\\text{number of correct predictions}}{\\text{total number of predictions}}.\n",
    "$$\n",
    "\n",
    "Let us compute the training accuracy of the multiple logistic regression model trained using `scikit-learn` API above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "accuarcy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy: \", accuarcy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area under the ROC curve (AUC)\n",
    "\n",
    "The receiver operating characteristic (ROC) curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The TPR and FPR are defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{TPR} &= \\frac{\\text{number of true positives}}{\\text{number of positives}} \\\\\n",
    "\\text{FPR} &= \\frac{\\text{number of false positives}}{\\text{number of negatives}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The area under the ROC curve (AUC) is a popular measure of classifier performance. The AUC is defined as the area under the ROC curve. It is a number between 0 and 1, where 0.5 is the random guess and 1 is the perfect prediction. The AUC is a good metric for evaluating the classification since it considers all possible classification thresholds.\n",
    "\n",
    "Let us plot the ROC curve of the multiple logistic regression model trained using `scikit-learn` API above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = clf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y, y_prob)\n",
    "print(\"AUC: \", roc_auc)\n",
    "\n",
    "clf_disp = RocCurveDisplay.from_estimator(clf, X_train, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clearly the trade-off between TPR and FPR. The AUC is 0.95, indicating that the model is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. Logistic regression is a **classification** algorithm.\n",
    "\n",
    "        a. True\n",
    "        \n",
    "        b. False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**a. True**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. All the following exercises will use the [Weekly](https://github.com/pykale/transparentML/blob/main/data/Weekly.csv) dataset.\n",
    "\n",
    "Load the dataset, convert the categories to numerical values, and fit a logistic regression model using **Volume** variable as a predictor and **Direction** variable as the response using **scikit-learn or statsmodel**. Find out the fitted model's **weight** and **bias**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Weekly.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "df[\"Direction\"] = df.Direction.factorize()[0]\n",
    "\n",
    "clf = LogisticRegression(solver=\"newton-cg\")\n",
    "X_train = df.Volume.values.reshape(-1, 1)\n",
    "y = df.Direction\n",
    "clf.fit(X_train, y)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**. Plot the **ROC curve** and find out the **training accuracy** and **AUC** of the fitted model from **Exercise 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, RocCurveDisplay\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "accuarcy = accuracy_score(y, y_pred)\n",
    "print(\"Training Accuracy: \", accuarcy)\n",
    "\n",
    "y_prob = clf.predict_proba(X_train)[:, 1]\n",
    "roc_auc = roc_auc_score(y, y_prob)\n",
    "print(\"AUC: \", roc_auc)\n",
    "\n",
    "clf_disp = RocCurveDisplay.from_estimator(clf, X_train, y)\n",
    "plt.show()\n",
    "\n",
    "# From the ROC curve we can say that the result is almost random as it is almost close to 50%.\n",
    "# The model from Exercise-2 has not learnt anything at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4**. Train another logistic regression model using **Direction** as a response and all the other features as predictors. Print out all the **parameter values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = df.drop([\"Direction\"], axis=1)\n",
    "\n",
    "y = df.Direction\n",
    "\n",
    "clf = LogisticRegression(solver=\"newton-cg\")\n",
    "clf.fit(X_train, y)\n",
    "print(\"coefficients: \", clf.coef_)\n",
    "print(\"intercept :\", clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5**. Plot the **ROC curve** and find out the **training accuracy** and **AUC** metrics of the fitted model from **Exercise 4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "accuarcy = accuracy_score(y, y_pred)\n",
    "print(\"Training Accuracy: \", accuarcy)\n",
    "\n",
    "y_prob = clf.predict_proba(X_train)[:, 1]\n",
    "roc_auc = roc_auc_score(y, y_prob)\n",
    "print(\"AUC: \", roc_auc)\n",
    "\n",
    "clf_disp = RocCurveDisplay.from_estimator(clf, X_train, y)\n",
    "plt.show()\n",
    "\n",
    "# The model from Exercise 4 learnt very well on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6**. You might notice that the model with all variables is much better than the model from **Exercise 2**. Use the skills you have learnt to perform further analysis on the data to explain why. **Hint**: Consider correlations (See section [2.2.4](https://pykale.github.io/transparentML/02-linear-reg/multi-linear-regression.html#interpreting-the-results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "print(corr_matrix)\n",
    "# Here from the correlation result we can see that, \"Today\" has a 0.72 correlation with \"Direction\"\n",
    "# which is the main reason for getting high performance\n",
    "\n",
    "y = df.Direction\n",
    "\n",
    "# model with out \"Today\"\n",
    "X_train = df.drop([\"Direction\", \"Today\"], axis=1)\n",
    "clf.fit(X_train, y)\n",
    "clf_disp = RocCurveDisplay.from_estimator(clf, X_train, y)\n",
    "# Without today feature the model performed very bad\n",
    "\n",
    "# model with only \"Today\"\n",
    "X_train = df[\"Today\"].values.reshape(-1, 1)\n",
    "clf.fit(X_train, y)\n",
    "clf_disp = RocCurveDisplay.from_estimator(clf, X_train, y)\n",
    "# with today feature the model performed very well\n",
    "# from this experiment we analyzed the important of correlation between feature and target variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykale",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
