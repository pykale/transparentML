{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbors classification\n",
    "\n",
    "Before we end this introduction, let us look at a simple example of a supervised machine learning model, the K-Nearest Neighbors (KNN) model. It is a _non-parametric_ method used for classification and regression. In both cases, the input consists of the $k$ closest training examples in the feature space. The output depends on whether KNN is used for classification or regression. Here, we focus on the classification case.\n",
    "\n",
    "Watch the 5-min video below to help you understand the KNN algorithm.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/HVXime0nQeI?start=21&end=305\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining K-Nearest Neighbors, by StatQuest](https://www.youtube.com/embed/HVXime0nQeI?start=21&end=305)\n",
    "```\n",
    "\n",
    "Then study the following sections to learn more about KNN.\n",
    "\n",
    "### Ingredients & transparency\n",
    "For all machine learning models covered in this course, we aim to talk about their ingredients and transparency in a standard way to facilitate understanding their similarities and differences. For transparency, we will focus on the system transparency, i.e. system logic. Process transparency is not specific to ML models and it will be discussed when we cover the ML process/lifecycle.\n",
    "\n",
    "The ingredients of a KNN model are the training data. The transparency of a KNN model is the distance between the test point and the training points.\n",
    "\n",
    "```{admonition} Ingredients\n",
    "- Input: features of data samples\n",
    "- Output: class labels of data samples\n",
    "- Model: assigned dominant class label of the $K$ nearest neighbors (in training data) of a test sample to it\n",
    "  - Hyperparameter(s): the number of nearest neighbors $K$\n",
    "  - Parameters: nonparametric (i.e., no parameters to learn, only store the training data)\n",
    "- Loss function: minimise distance between samples\n",
    "- Learning algorithm: sorting distances between samples\n",
    "```\n",
    "\n",
    "```{admonition} Transparency\n",
    "System logic\n",
    "- Condition to produce certain output: to produce a sample with label $c$, find a data point in the input space where the majority of its nearest neighbors have label $c$\n",
    "```\n",
    "\n",
    "### Example: iris classification\n",
    "\n",
    "We adapt the [KNN example from scikit-learn](https://scikit-learn.org/1.0/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py) to illustrate the use of KNN for classification. \n",
    "\n",
    "To do so, we use the Iris dataset, which is a classic dataset in machine learning and statistics. It is included in scikit-learn and we load it as follows.\n",
    "\n",
    "```{admonition} Launch \n",
    "Click the rocket symbol (<i class=\"fas fa-rocket\"></i>) to launch this page as an interactive notebook in Google Colab (faster but requiring a Google account) or Binder.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Libraries\n",
    "\n",
    "Get ready by importing the application programming interfaces (APIs) needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data\n",
    "\n",
    " Let us work on one of the classical dataset in machine learning: the [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). It can be loaded directly from the scikit-learn library. You can also click [Iris](https://github.com/pykale/transparentML/blob/main/data/Iris.csv) to view the dataset directly in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning\n",
    "\n",
    "In the KNN model, there is only one hyperparameter, the number of neighbors $K$, which is `n_neighbors` in `scikit-learn`. We set it to 15. In the following, we will study the effect of this hyperparameter on the model performance by examining five different values of $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visulisation in 2D\n",
    "\n",
    "For the purpose of visulisation to get intuition, we will work with the first two features only so that we can do visualisation of the data samples in a 2D plot. We can take the first two features by [indexing and slicing](https://pykale.github.io/transparentML/00-prereq/basic-python.html#indexing-and-slicing) as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "h = 0.02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and testing\n",
    "\n",
    "We create an instance of the KNN Classifier with each of the five $K$ values and fit the data. Then we make prediction of class labels on values from the grid, all/most of them are not seen in the training. We store the prediction results with different colours for different classes to visualise the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in [1, 5, 15, 50, 100]:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "    clf.fit(X, y)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    sns.scatterplot(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        hue=iris.target_names[y],\n",
    "        palette=cmap_bold,\n",
    "        alpha=1.0,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    # Add an example data point for insights\n",
    "    plt.scatter(4.9, 2.6, marker=\"x\", color=\"red\", s=100)\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\n",
    "        \"Decision boundaries of 3-class KNN classification with $K$ = %i\"\n",
    "        % (n_neighbors)\n",
    "    )\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example explanation of system transparency\n",
    "\n",
    "In the above, we highlighted an example test data point at `septal_length = 4.9cm` and `septal_width = 2.6cm`, marked as a red cross in the figures. Let us examine the system logic with $K=1$ to reveal its system transparency. \n",
    "\n",
    "```{admonition} System transparency\n",
    ":class: important\n",
    "- The model predicts the test point (at the red cross) to be a `virginica` because the nearest neighbor to the red cross is a darkblue circle, which is a `virginica`.\n",
    "- The model would have predict the test point to be a `versicolor` if its `septal_length` is `5.1cm`, or the model would have predict the test point to be a `setosa` if its `septal_width` is `2.9cm`. \n",
    "```    \n",
    "\n",
    "When $K=5$, the model predicts the test point to be a `versicolor` because the majority of the five nearest neighbors to the red cross are `versicolor` (green circles). When $K=15$, the model predicts the test point to be a `setosa` because the majority of the 15 nearest neighbors to the red cross are `setosa` (orange circles). From the figures, we can tell how a different prediction would have been made by changing the features of the test point.\n",
    "\n",
    "##### Other observations\n",
    "\n",
    "We can make the following observations from the above figures.\n",
    "\n",
    "- We can see that the decision boundaries are not smooth. This is because the KNN model is a non-parametric model, which means that it does not make any assumption on the underlying data distribution. This is also the reason why the model is called _lazy_. KNN actually does not learn a discriminative function from the training data but memorises the training dataset instead. This is why the model is also called _instance-based_. The model makes prediction by comparing the test sample with all training samples and assigning the label that is most common among its $K$ nearest neighbors.\n",
    "- KNN is fully transparent in the sense that it is easy to understand the logic behind the model and we can answer the question of for what values of the input variables would the system return a specific value of interest. However, it is not efficient in terms of computational cost. The computational cost of KNN is $O(n)$, where $n$ is the number of training samples. This is because the model needs to compute the distance between the test sample and all training samples. This is also the reason why KNN is not suitable for large datasets.\n",
    "- **Bias-variance trade-off**: When $K$ = 1, the decision boundary is _overly flexible_. This corresponds to a classifier that has _low bias_ but _very high variance_. As $K$ grows, the method becomes _less flexible_ and produces a decision boundary that is close to (piece-wise) linear. This corresponds to a _low-variance_ but _high-bias_ classifier. This trade-off is common in machine learning and we will discuss it in more details in the following sections. Typically, a $K$ neither too big (e.g. 100) nor too small (e.g. 1), such as $K=5$ will give the best performance. Choosing the correct level of flexibility (typically through _hyperparameter tuning_ after selecting a model) is critical to the success of any machine learning method.\n",
    "\n",
    "Despite the fact that it is a very simple approach, KNN can often produce classifiers with good performance. This is because the decision boundary is very flexible and can adapt to the data distribution. However, it is not suitable for large datasets because of its high computational cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "To be completed in the next cycle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. How many features are there in total for the iris dataset? Write code below to find out or verify."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1bda8b49161908208a6e3087b59be7e64a5f9b1866581a6eaef662e2ef65b61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
