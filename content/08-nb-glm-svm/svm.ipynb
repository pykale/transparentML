{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines\n",
    "\n",
    "## Non-linear decision boundaries and kernel methods\n",
    "\n",
    "### Classification with non-linear decision boundaries\n",
    "\n",
    "The support vector classifier is a natural approach for classification in the two-class setting, if the boundary between the two classes is linear. However, in practice we are sometimes faced with non-linear class boundaries, as the example shown in the left panel of {numref}`svm8`. \n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/pykale/transparentML/main/content/images/svm/svm8.png\n",
    "---\n",
    "width: 700px\n",
    "name: svm8\n",
    "figclass: margin-caption\n",
    "---\n",
    "Left: A support vector classifier was fit to a small data set. The hyperplane is shown as a solid line and the margins are shown as dashed lines. Purple observations: Observations 3, 4, 5, and 6 are on the correct side of the margin, observation 2 is on the margin, and observation 1 is on the wrong side of the margin. Blue observations: Observations 7 and 10 are on the correct side of the margin, observation 9 is on the margin, and observation 8 is on the wrong side of the margin. No observations are on the wrong side of the hyperplane. Right: Same as left panel with two additional points, 11 and 12. These two observations are on the wrong side of the hyperplane and the wrong side of the margin.\n",
    "```\n",
    "\n",
    "In {doc}`Linear regression <../02-linear-reg/extension-limitation>`, we discussed using higher-order polynomials as a way to fit a non-linear relationship between the predictors and the response. Foe example, rather than fitting a support vector classifier using $D$ features: $ x_{1}, x_{2}, \\ldots, x_{D} $, we could fit a support vector classifier using $ 2 \\times D $ features: $ x_{1}, x_{2}, \\ldots, x_{D'}, x_{1}^2, x_{2}^2, \\ldots, x_{D'}^2 $. Then the optimisation problem becomes\n",
    "\n",
    "```{math}\n",
    ":label: eq:svm-polynomial\n",
    "\\begin{aligned}\n",
    "& \\max_{\\beta_0, \\beta_{1,1}, \\ldots, \\beta_{D,2}, \\epsilon_1, \\ldots, \\epsilon_N} M \\\\\n",
    "& \\text{subject to } y_i\\left(\\beta_0 + \\sum_{j=1}^{D} \\beta_{j,1} x_{ij} + \\beta_{j,2} x_{ij}^2\\right) \\geq M(1 - \\epsilon_i), \\\\\n",
    "& \\sum_{j=1}^{D} \\beta_{j,1}^2 + \\beta_{j,2}^2 = 1, \\epsilon_i \\geq 0, \\sum_{i = 1}^N \\epsilon_i \\leq C, \\text{ for } i = 1, \\ldots, N.\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "The decision boundary that results from Equation {eq}`eq:svm-polynomial` is in fact linear. But in the original feature space, the decision boundary is of the form $ q(x) = 0 $, where $ q(\\cdot) $ is a quadratic polynomial, and its solutions are generally non-linear.\n",
    "\n",
    "\n",
    "### Kernel methods\n",
    "\n",
    "The _support vector machine (SVM)_ is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using _kernels_. As described above, the main idea is to enlarge our feature space in order to accommodate a non-linear boundary between the classes. The kernel approach that we describe here is simply an efficient computational approach for enacting this idea.\n",
    "\n",
    "We have not discussed exactly how the support vector classifier is computed because the details become somewhat technical. However, it turns\n",
    "out that the solution to the support vector classifier problem {eq}`eq:soft-margin-classifier` involves only the inner products of the observations (as opposed to the observations themselves). The inner product of two $D$-vectors $\\mathbf{a}$ and $\\mathbf{b}$ is defined as $⟨a, b⟩ = \\sum_{i=1}^{D} a_i b_i $. Thus the inner product of two observations $\\mathbf{x}_i$ and $\\mathbf{x}_{i'}$ is $⟨\\mathbf{x}_i, \\mathbf{x}_{i'}⟩ = \\sum_{j=1}^{D} x_{ij} x_{i'j} $. As a result, the linear svm classifier can be written as\n",
    "\n",
    "```{math}\n",
    ":label: eq:svm-inner-product\n",
    "\\begin{equation}\n",
    "f(x) = \\beta_0 + \\sum_{i=1}^{N} \\alpha_i y_i ⟨\\mathbf{x}, \\mathbf{x}_i⟩,\n",
    "\\end{equation}\n",
    "```\n",
    "where there are $N$ parameters $\\alpha_i, \\text{ for } i = 1, \\ldots N $, one per training observation. To estimate the parameters $\\alpha_i$, all we need are the inner products of the training observations. The inner product can be denoted in the following generalised form:\n",
    "\n",
    "$$\n",
    "⟨x_i, x_{i'}⟩ =k(x_i, x_{i'}),\n",
    "$$\n",
    "\n",
    "where $ k(\\cdot, \\cdot) $ is some function that we will refer to as a _kernel_, which quantifies the similarity of two observations. For instance, we could simply take\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_{i'}) = \\sum_{j=1}^{D} x_{ij} x_{i'j},\n",
    "$$\n",
    "\n",
    "which knows as the _linear kernel_. However, we could also take\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_{i'}) = \\left(1 + \\sum_{j=1}^{D} x_{ij} x_{i'j}\\right)^d,\n",
    "$$\n",
    "\n",
    "which is known as _polynomial kernel_  of degree $d$, and $ d > 1 $ is a positive integer. Another popular choice is the radial kernel, which takes the form\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_{i'}) = \\exp\\left(-\\gamma \\sum_{j=1}^{D} (x_{ij} - x_{i'j})^2\\right),\n",
    "$$\n",
    "\n",
    "where $ \\gamma > 0 $ is a positive tuning parameter. The radial kernel is also known as the _Gaussian kernel_. {numref}`svm9` shows the decision boundaries that result from using the polynomial (left) and radial kernels (right).\n",
    "\n",
    "When the support vector classifier is combined with a non-linear kernel, the resulting classifier is known as a support vector machine:\n",
    "\n",
    "```{math}\n",
    ":label: eq:svm\n",
    "\\begin{equation}\n",
    "f(x) = \\beta_0 + \\sum_{i=1}^{N} \\alpha_i k(\\mathbf{x}, \\mathbf{x}_i).\n",
    "\\end{equation}\n",
    "```\n",
    "\n",
    "```{figure} https://raw.githubusercontent.com/pykale/transparentML/main/content/images/svm/svm9.png\n",
    "---\n",
    "width: 700px\n",
    "name: svm9\n",
    "figclass: margin-caption\n",
    "---\n",
    "Left: An SVM with a polynomial kernel of degree 3 is applied to the non-linear data from {numref}`svm8`, resulting in a far more appropriate decision rule. Right: An SVM with a radial kernel is applied. In this example, either kernel is capable of capturing the decision boundary.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: SVMs on toy data using `scikit-learn`\n",
    "\n",
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot a classifier with support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc(svc, X, y, h=0.02, pad=0.25):\n",
    "    x_min, x_max = X[:, 0].min() - pad, X[:, 0].max() + pad\n",
    "    y_min, y_max = X[:, 1].min() - pad, X[:, 1].max() + pad\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=70, c=y, cmap=plt.cm.Paired)\n",
    "    # Support vectors indicated in plot by vertical lines\n",
    "    sv = svc.support_vectors_\n",
    "    plt.scatter(\n",
    "        sv[:, 0], sv[:, 1], c=\"k\", marker=\"x\", s=100, alpha=0.5\n",
    "    )  # , linewidths=1)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.show()\n",
    "    print(\"Number of support vectors: \", svc.support_.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)\n",
    "X = np.random.randn(200, 2)\n",
    "X[:100] = X[:100] + 2\n",
    "X[101:150] = X[101:150] - 2\n",
    "y = np.concatenate([np.repeat(-1, 150), np.repeat(1, 50)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=70, c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=1.0, kernel=\"rbf\", gamma=1)\n",
    "svm.fit(X_train, y_train)\n",
    "plot_svc(svm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing C parameter, allowing more flexibility\n",
    "svm2 = SVC(C=100, kernel=\"rbf\", gamma=1.0)\n",
    "svm2.fit(X_train, y_train)\n",
    "plot_svc(svm2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the parameters by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {\"C\": [0.01, 0.1, 1, 10, 100], \"gamma\": [0.5, 1, 2, 3, 4]}\n",
    "clf = GridSearchCV(\n",
    "    SVC(kernel=\"rbf\"),\n",
    "    tuned_parameters,\n",
    "    cv=10,\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the best combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, clf.best_estimator_.predict(X_test))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15% of test observations misclassified\n",
    "clf.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating with ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the ROC curves of two models on train/test data with different hyper-parameter $\\gamma$ of Gaussian kernels (RBF kernel). \n",
    "<!-- One model is more flexible than the other. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm3 = SVC(C=1, kernel=\"rbf\", gamma=2)\n",
    "svm3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More flexible model\n",
    "svm4 = SVC(C=1, kernel=\"rbf\", gamma=50)\n",
    "svm4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_score3 = svm3.decision_function(X_train)\n",
    "y_train_score4 = svm4.decision_function(X_train)\n",
    "\n",
    "false_pos_rate3, true_pos_rate3, _ = roc_curve(y_train, y_train_score3)\n",
    "roc_auc3 = auc(false_pos_rate3, true_pos_rate3)\n",
    "\n",
    "false_pos_rate4, true_pos_rate4, _ = roc_curve(y_train, y_train_score4)\n",
    "roc_auc4 = auc(false_pos_rate4, true_pos_rate4)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax1.plot(\n",
    "    false_pos_rate3,\n",
    "    true_pos_rate3,\n",
    "    label=\"SVM $\\gamma = 1$ ROC curve (area = %0.2f)\" % roc_auc3,\n",
    "    color=\"b\",\n",
    ")\n",
    "ax1.plot(\n",
    "    false_pos_rate4,\n",
    "    true_pos_rate4,\n",
    "    label=\"SVM $\\gamma = 50$ ROC curve (area = %0.2f)\" % roc_auc4,\n",
    "    color=\"r\",\n",
    ")\n",
    "ax1.set_title(\"Training Data\")\n",
    "\n",
    "y_test_score3 = svm3.decision_function(X_test)\n",
    "y_test_score4 = svm4.decision_function(X_test)\n",
    "\n",
    "false_pos_rate3, true_pos_rate3, _ = roc_curve(y_test, y_test_score3)\n",
    "roc_auc3 = auc(false_pos_rate3, true_pos_rate3)\n",
    "\n",
    "false_pos_rate4, true_pos_rate4, _ = roc_curve(y_test, y_test_score4)\n",
    "roc_auc4 = auc(false_pos_rate4, true_pos_rate4)\n",
    "\n",
    "ax2.plot(\n",
    "    false_pos_rate3,\n",
    "    true_pos_rate3,\n",
    "    label=\"SVM $\\gamma = 1$ ROC curve (area = %0.2f)\" % roc_auc3,\n",
    "    color=\"b\",\n",
    ")\n",
    "ax2.plot(\n",
    "    false_pos_rate4,\n",
    "    true_pos_rate4,\n",
    "    label=\"SVM $\\gamma = 50$ ROC curve (area = %0.2f)\" % roc_auc4,\n",
    "    color=\"r\",\n",
    ")\n",
    "ax2.set_title(\"Test Data\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.plot([0, 1], [0, 1], \"k--\")\n",
    "    ax.set_xlim([-0.05, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more flexible model ($ \\gamma = 50 $) scores better on training data but worse on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMs with more than two classes\n",
    "\n",
    "So far, our discussion has been limited to the case of binary classification: that is, classification in the two-class setting. How can we extend SVMs to the more general case where we have some arbitrary number of classes? It turns out that the concept of separating hyperplanes upon which SVMs are based does not lend itself naturally to more than two classes. Though a number of proposals for extending SVMs to the K-class case have been made, the two most popular are the one-versus-one and one-versus-all approaches. \n",
    "\n",
    "### One-vs-one\n",
    "\n",
    "Suppose that we would like to perform classification using SVMs, and there are $K$ > 2 classes. A one-versus-one or all-pairs approach constructs $K(K-1)/2$ binary classifiers, one for each pair of classes. For example, if there are three classes, then we would construct three binary classifiers, one for each pair of classes. The first classifier would distinguish between class 1 and class 2, the second classifier would distinguish between class 1 and class 3, and the third classifier would distinguish between class 2 and class 3. To classify a new observation, we would apply each of the three classifiers, and assign the observation to the class that receives the most votes.\n",
    "\n",
    "\n",
    "### One-vs-all\n",
    "\n",
    "The one-versus-all approach is an alternative procedure for applying SVMs in the case of $K$ > 2 classes. We fit $K$ SVMs, each time comparing one of the $K$ classes to the remaining $K − 1$ classes. For example, if there are three classes, then we would fit three SVMs, one for each class. The first SVM would separate class 1 from classes 2 and 3, the second SVM would separate class 2 from classes 1 and 3, and the third SVM would separate class 3 from classes 1 and 2. To classify a new observation, we would apply each of the three classifiers, and assign the observation to the class that receives the most votes.\n",
    "\n",
    "The following code shows how to train a SVM using `scikit-learn`, the strategy for multi-class classification is \"one vs one\" internally. However, we can get \"one vs rest\" hyper-plane by setting `decision_function_shape='ovr'` in the `SVC` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third class of observations\n",
    "np.random.seed(8)\n",
    "XX = np.vstack([X, np.random.randn(50, 2)])\n",
    "yy = np.hstack([y, np.repeat(0, 50)])\n",
    "XX[yy == 0] = XX[yy == 0] + 4\n",
    "\n",
    "plt.scatter(XX[:, 0], XX[:, 1], s=70, c=yy, cmap=plt.cm.prism)\n",
    "plt.xlabel(\"XX1\")\n",
    "plt.ylabel(\"XX2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm5 = SVC(C=1, kernel=\"rbf\")\n",
    "svm5.fit(XX, yy)\n",
    "plot_svc(svm5, XX, yy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pykale')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
