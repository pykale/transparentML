{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $K$-means clustering \n",
    "\n",
    "In this chapter, we will introduce the $K$-means clustering algorithm. \n",
    "\n",
    "Watch the 8-minute video below for a visual explanation of $K$-means clustering.\n",
    "\n",
    "```{admonition} Video\n",
    "\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/4b5d3muPQmA?start=14\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining $K$-Means Clustering, by StatQuest](https://www.youtube.com/embed/4b5d3muPQmA?start=14)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "_Clustering_ refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. Of course, to make this concrete, we must define what it means for two or more observations to be similar or different. Indeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied. \n",
    "\n",
    "For instance, suppose that we have a set of n observations, each with $D$ features. We might define the similarity between two observations as the Euclidean distance between them, which is given by features. The n observations could correspond to tissue samples for patients with breast cancer, and the $D$ features could correspond to measurements collected for each tissue sample; these could be clinical measurements, such as tumour stage or grade, or they could be gene expression measurements. We may have a reason to believe that there is some heterogeneity among the n tissue samples; for instance, perhaps there are a few different unknown subtypes of breast cancer. Clustering could be used to find these subgroups. This is an unsupervised problem because we are trying to discover structure—in this case, distinct clusters—on the basis of a data set. The goal in supervised problems, on the other hand, is to try to predict some outcome vector such as survival time or response to drug treatment.\n",
    "\n",
    "Both clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:\n",
    "\n",
    "- PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;\n",
    "- Clustering looks to find homogeneous subgroups among the observations.\n",
    "\n",
    "Another application of clustering arises in marketing. We may have access to a large number of measurements (e.g. median household income, occupation, distance from nearest urban area, and so forth) for a large number of people. Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set.\n",
    "\n",
    "Since clustering is popular in many fields, there exist a great number of clustering methods. In this section we focus on perhaps the two best-known clustering approaches: $K$-_means clustering_ and _hierarchical clustering_. In $K$-means clustering, we seek to partition the observations into a pre-specified number of clusters. On the other hand, in hierarchical clustering, we do not know in advance how many clusters we want; in fact, we end up with a tree-like visual representation of the observations, called a dendrogram, that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to n. There are advantages and disadvantages to each of these clustering approaches, which we highlight in this chapter.\n",
    "\n",
    "In general, we can cluster observations on the basis of the features in order to identify subgroups among the observations, or we can cluster features on the basis of the observations in order to discover subgroups among the features. In what follows, for simplicity we will discuss clustering observations on the basis of the features, though the converse can be performed by simply transposing the data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $K$-means clustering\n",
    "\n",
    "$K$-means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters. To perform $K$-means clustering, we must first specify the desired number of clusters $K$; then the $K$-means algorithm will assign each observation to exactly one of the $K$ clusters. Figure 12.7 shows the results obtained from performing $K$-means\n",
    "clustering on a simulated example consisting of 150 observations in two dimensions, using three different values of K.\n",
    "\n",
    "The $K$-means clustering procedure results from a simple and intuitive mathematical problem. We begin by defining some notation. Let $C_1, \\ldots, C_K$ denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:\n",
    "\n",
    "1. $C_1 \\cup C_2 \\cup \\ldots \\cup C_K = {1, \\ldots , N} $. In other words, each observation belongs to at least one of the $K$ clusters.\n",
    "2. $C_k \\cap C_{k′} = \\Phi \\text{ for all } k \\neq k′$. In other words, the clusters are non-overlapping: no observation belongs to more than one cluster.\n",
    "\n",
    "For instance, if the ith observation is in the $k\\text{th}$ cluster, then $i \\in C_k $. The idea behind $K$-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. The within-cluster variation\n",
    "for cluster $C_k$ is a measure $W(C_k)$ of the amount by which the observations within a cluster differ from each other. Hence we want to solve the problem\n",
    "\n",
    "```{math}\n",
    ":label: kmeans-problem\n",
    "\\min_{C_1, \\ldots, C_K} \\sum_{k=1}^K W(C_k).\n",
    "```\n",
    "\n",
    "In words, this formula says that we want to partition the observations into $K$ clusters such that the total within-cluster variation, summed over all $K$ clusters, is as small as possible.\n",
    "\n",
    "In order to make solving Equation {eq}`kmeans-problem` actionable we need to define the within-cluster variation. There are many possible ways to define this concept, but by far the most common choice involves _squared Euclidean distance_. That is, we define\n",
    "\n",
    "```{math}\n",
    ":label: within-cluster-var\n",
    "W(C_k) = \\frac{1}{|C_k|} \\sum_{i,j \\in C_k} \\sum_{d=1}^{D} \\left( x_{i,d} - x_{j,d} \\right)^2,\n",
    "```\n",
    "\n",
    "where $|C_k|$ denotes the number of observations in the $k\\text{th}$  cluster. In other words, the within-cluster variation for the kth cluster is the sum of all of the pairwise squared Euclidean distances between the observations in the $k\\text{th}$ cluster, divided by the total number of observations in the $k\\text{th}$  cluster.\n",
    "\n",
    "Combining Equations {eq}`kmeans-problem` and {eq}`within-cluster-var`, gives the optimization problem that defines\n",
    "K-means clustering,\n",
    "\n",
    "```{math}\n",
    ":label: kmeans-obj\n",
    "\\min_{C_1, \\ldots, C_K} \\left\\{\\sum_{k=1}^K \\frac{1}{|C_k|} \\sum_{i,j \\in C_k} \\sum_{d=1}^{D} \\left( x_{i,d} - x_{j,d} \\right)^2\\right\\}.\n",
    "```\n",
    "\n",
    "Now, we would like to find an algorithm to solve Equation {eq}`kmeans-obj`—that is, a method to partition the observations into K clusters such that the objective of Equation {eq}`kmeans-obj` is minimised. This is in fact a very difficult problem to solve precisely, since there are almost $K^N$ ways to partition $N$ observations into $K$ clusters. This is a huge number unless $K$ and $N$ are tiny! Fortunately, a very simple algorithm can be shown to provide a local optimum—a pretty good\n",
    "solution—to the $K$-means optimisation problem {eq}`kmeans-obj`. This approach is laid out in {prf:ref}`alg:kmeans-algorithm`.\n",
    "\n",
    "\n",
    "```{prf:algorithm} K-Means Clustering\n",
    ":label: alg:kmeans-algorithm\n",
    "\n",
    "- Randomly assign a number, from $1$ to $K$, to each of the observations. These serve as initial cluster assignments for the observations.\n",
    "\n",
    "- Iterate until the cluster assignments stop changing:\n",
    "\n",
    "    - For each of the $K$ clusters, compute the cluster centroid, which is the the vector of the $D$ feature means of the observations in the $k\\text{th}$ cluster.\n",
    "\n",
    "    - Assign each observation to the cluster for which the squared Euclidean distance between the observation and the cluster centroid is smallest.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "np.random.seed(21)\n",
    "X = np.random.standard_normal((50, 2))\n",
    "X[:25, 0] = X[:25, 0] + 3\n",
    "X[:25, 1] = X[:25, 1] - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "km1 = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "km1.fit(X)\n",
    "\n",
    "n_clusters = 3\n",
    "km2 = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "km2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(km1.labels_)\n",
    "print(dir(km1))  # we can use dir to see other saved attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.scatter(X[:, 0], X[:, 1], s=40, c=km1.labels_, cmap=plt.cm.prism)\n",
    "ax1.set_title(\"$K$-means Clustering Results with K=2\")\n",
    "ax1.scatter(\n",
    "    km1.cluster_centers_[:, 0],\n",
    "    km1.cluster_centers_[:, 1],\n",
    "    marker=\"+\",\n",
    "    s=100,\n",
    "    c=\"k\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "ax2.scatter(X[:, 0], X[:, 1], s=40, c=km2.labels_, cmap=plt.cm.prism)\n",
    "ax2.set_title(\"$K$-means Clustering Results with K=3\")\n",
    "ax2.scatter(\n",
    "    km2.cluster_centers_[:, 0],\n",
    "    km2.cluster_centers_[:, 1],\n",
    "    marker=\"+\",\n",
    "    s=100,\n",
    "    c=\"k\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 18))\n",
    "\n",
    "for linkage, cluster, ax in zip(\n",
    "    [hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)],\n",
    "    [\"c1\", \"c2\", \"c3\"],\n",
    "    [ax1, ax2, ax3],\n",
    "):\n",
    "    cluster = hierarchy.dendrogram(linkage, ax=ax, color_threshold=0)\n",
    "\n",
    "ax1.set_title(\"Complete Linkage\")\n",
    "ax2.set_title(\"Average Linkage\")\n",
    "ax3.set_title(\"Single Linkage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering the Observations of the NCI60 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"https://github.com/pykale/transparentML/raw/main/data/NCI60_data.csv\")\n",
    "y = pd.read_csv(\"https://github.com/pykale/transparentML/raw/main/data/NCI60_labs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), index=y.iloc[:, 0], columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 20))\n",
    "\n",
    "for linkage, cluster, ax in zip(\n",
    "    [hierarchy.complete(X_scaled), hierarchy.average(X), hierarchy.single(X_scaled)],\n",
    "    [\"c1\", \"c2\", \"c3\"],\n",
    "    [ax1, ax2, ax3],\n",
    "):\n",
    "    cluster = hierarchy.dendrogram(\n",
    "        linkage,\n",
    "        labels=X_scaled.index,\n",
    "        orientation=\"right\",\n",
    "        color_threshold=0,\n",
    "        leaf_font_size=10,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "ax1.set_title(\"Complete Linkage\")\n",
    "ax2.set_title(\"Average Linkage\")\n",
    "ax3.set_title(\"Single Linkage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "cut4 = hierarchy.dendrogram(\n",
    "    hierarchy.complete(X_scaled),\n",
    "    labels=X_scaled.index,\n",
    "    orientation=\"right\",\n",
    "    color_threshold=140,\n",
    "    leaf_font_size=10,\n",
    ")\n",
    "plt.vlines(\n",
    "    140, 0, plt.gca().yaxis.get_data_interval()[1], colors=\"r\", linestyles=\"dashed\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(21)\n",
    "km3 = KMeans(n_clusters=4, n_init=50)\n",
    "km3.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km3.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_scaled_ = StandardScaler().fit_transform(X)\n",
    "df_plot = pd.DataFrame(pca.fit_transform(X_scaled_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "pca_cluster = hierarchy.dendrogram(\n",
    "    hierarchy.complete(X_scaled),\n",
    "    labels=X_scaled.index,\n",
    "    orientation=\"right\",\n",
    "    color_threshold=100,\n",
    "    leaf_font_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchy based on Principal Components 1 to 5\n",
    "plt.figure(figsize=(10, 20))\n",
    "pca_cluster = hierarchy.dendrogram(\n",
    "    hierarchy.complete(df_plot.iloc[:, :5]),\n",
    "    labels=X_scaled.index,\n",
    "    orientation=\"right\",\n",
    "    color_threshold=100,\n",
    "    leaf_font_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "min 3 max 5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pykale')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
