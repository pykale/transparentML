{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction\n",
    "\n",
    "This section introduces principal components analysis (PCA), which is a technique for reducing the dimension of an data matrix $\\mathbf{X} \\in \\mathbb{R}^{D \\times N}$. \n",
    "<!-- The first principal component direction of the data is that along which the observations vary the most. -->\n",
    "When faced with a large set of correlated variables, principal components allow us to summarise this set with a smaller number of representative variables that collectively explain most of the variability in the original set.\n",
    "\n",
    "PCA refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. PCA is an unsupervised approach, since\n",
    "it involves only a set of features $x_1, \\ldots, x_D$ and no associated response $y$. Apart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualisation (visualisation of the observations or visualisation of the variables). It can also be used as a tool for data imputation — that is, for filling in missing values in a data matrix.\n",
    "\n",
    "Watch the 5-minute video below for a visual explanation of principal components analysis.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/HMOI_lkzW08?start=10\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining main ideas behine principal components analysis, by StatQuest](https://www.youtube.com/embed/HMOI_lkzW08?start=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal components analysis (PCA)\n",
    "\n",
    "Suppose that we wish to visualise observations with measurements on a set of $ D $ features, $x_1, x_2, \\ldots, x_D $, as part of an exploratory data analysis. We could do this by examining two-dimensional scatterplots of the data, each of which contains the $N$ observations’ measurements on two of the features. However, there are $ D(D-1)/2 $ such scatterplots, and it is difficult to visualise more than a few of them at a time. If $ D $ is large, then it will certainly not be possible to look at all of them; moreover, most likely none of them will be informative since they each contain just a small fraction of the total information present in the dataset. Clearly, a better method is required to visualise the $N$ observations when $D$ is large. In particular, we would like to find a low-dimensional representation of the data that captures as much of the information as possible. For instance, if we can obtain a two-dimensional representation of the data that captures most of the information, then we can plot the observations in this low-dimensional space.\n",
    "\n",
    "PCA provides a tool to do just this. It finds a low-dimensional representation of a dataset that contains as much as possible of the variation. The idea is that each of the $N$ observations lives in $D$-dimensional space, but not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. Each of the dimensions found by PCA is a linear combination of the $ D $ features. We now explain the manner in which these dimensions, or principal components, are found.\n",
    "\n",
    "The first principal component of a set of features $x_1, \\ldots, x_D$ is the normalised linear combination of the features\n",
    "\n",
    "$$\n",
    "z_1 = \\phi_{1,1}x_1 + \\phi_{1,2}x_2 + \\cdots + \\phi_{1,D}x_D\n",
    "$$\n",
    "\n",
    "that has the largest variance. By normalised, we mean that $ \\sum_{d=1}^D \\phi_{d,1}^2 = 1$. We refer to the elements $\\phi_{n, 1}, \\ldots, \\phi_{n, D} $ as the loadings of the first principal loading component; together, the loadings make up the principal component loading vector, $\\boldsymbol{\\phi} = [\\phi_{1,1}, \\ldots, \\phi_{1,D}]^\\top$. We constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a $D \\times N$ dataset $\\mathbf{X}$, how do we compute the first principal component? Since we are only interested in variance, we assume that each of the variables in $ \\mathbf{X} $ has been centred to have mean zero (that is, the column means of $ \\mathbf{X} $ are zero). We then look for the linear combination of the sample feature values of the form\n",
    "\n",
    "$$\n",
    "z_{n,1} = \\phi_{1,1}x_{n,1} + \\phi_{2,1}x_{n,2} + \\cdots + \\phi_{D,1}x_{n,D}\n",
    "$$\n",
    "\n",
    "that has the largest sample variance, subject to the constraint that $\\sum_{d=1}^D \\phi_{d,1}^2 = 1$. In other words, the first principal component loading vector solves the optimisation problem\n",
    "\n",
    "```{math}\n",
    ":label: eq:1stpc\n",
    "\\begin{equation}\n",
    "\\arg\\max_{\\boldsymbol{\\phi}} \\left\\{\\frac{1}{N} \\sum_{n=1}^N \\left(\\sum_{d=1}^D \\phi_{d,1} x_{d,n} \\right)^2\\right\\} \\quad \\text{subject to} \\quad \\sum_{d=1}^D \\phi_{d,1}^2 = 1\n",
    "\\end{equation}\n",
    "```\n",
    "\n",
    "where the objective can also be written as $\\frac{1}{N} \\sum_{n=1}^N z_{n,1}^2 $. Equation {eq}`eq:1stpc` maximises the sample variance of the $N$ values of $ z_{n,1} $. We refer $ z_{1,1}, \\ldots, z_{N,1} $ as the first principal component scores. Equation {eq}`eq:1stpc` can be solved via a standard technique in linear algebra, [_eigen decomposition_](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix), the mathematical details of which are beyond the scope of this course. \n",
    "\n",
    "There is a nice geometric interpretation for the first principal component. The loading vector $ \\boldsymbol{\\phi}_1 $ with elements $ \\phi_{1,1}, \\ldots, \\phi_{D, 1} $ defines a direction in feature space along which the data vary the most. If we project the $N$ data points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ onto this direction, the projected values are the principal component scores $ z_{1, 1} , \\ldots , z_{N, 1}$ themselves. \n",
    "\n",
    "After the first principal component $ z_1 $ of the features has been determined, we can find the second principal component $ z_2 $. The second principal component is the linear combination of $ x_1 , \\ldots, x_D $ that has maximal variance out of all linear combinations that are uncorrelated with $ z_1 $. The second principal component scores $ z_{1,2}, z_{2,2}, \\ldots , z_{N,2}$ take the form\n",
    "\n",
    "$$\n",
    "z_{n,1} = \\phi_{1,2}x_{n,1} + \\phi_{2,2}x_{n,2} + \\cdots + \\phi_{D,2}x_{n,D},\n",
    "$$\n",
    "\n",
    "where $ \\boldsymbol{\\phi}_2 $ is the second principal component loading vector, with elements $ \\phi_{1,2}, \\ldots, \\phi_{D, 2} $. It turns out that constraining $ z_2 $ to be uncorrelated with $ z_1 $ is equivalent to constraining the direction $ \\boldsymbol{\\phi}_2 $ to be orthogonal (perpendicular) to the direction $ \\boldsymbol{\\phi}_1 $.\n",
    "\n",
    "Once we have computed the principal components, we can plot them against each other in order to produce low-dimensional views of the data.\n",
    "For instance, we can plot the score vector $ z_1 $ against $ z_2 $, $ z_1 $ against $ z_3 $, $ z_2 $ against $ z_3 $, and so forth. Geometrically, this amounts to projecting the original data down onto the subspace spanned by $ \\boldsymbol{\\phi}_1 $, $ \\boldsymbol{\\phi}_2 $, and $ \\boldsymbol{\\phi}_3 $, and plotting the projected points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on `USArrests` dataset\n",
    "\n",
    "We illustrate the use of PCA on the [USArrests dataset](https://github.com/pykale/transparentML/blob/main/data/USArrests.csv). For each of the 50 states in the United States, the dataset contains the number of arrests per 100,000 residents for each of three crimes: `Assault`, `Murder`, and `Rape`. We also record `UrbanPop` (the percent of the population in each state living in urban areas). The principal component score vectors have length $N = 50$, and the principal component loading vectors have length $D = 4$. PCA was performed after standardizing each variable to have mean zero and standard deviation one.\n",
    "\n",
    "**Import libraries and load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/USArrests.csv\"\n",
    "\n",
    "USArrests = pd.read_csv(data_url, header=0, index_col=0)\n",
    "USArrests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(USArrests.mean())\n",
    "print(USArrests.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(\n",
    "    scaler.fit_transform(USArrests), index=USArrests.index, columns=USArrests.columns\n",
    ")\n",
    "# The loading vectors (i.e. these are the projection of the data onto the principal components)\n",
    "pca_loadings = pd.DataFrame(\n",
    "    PCA().fit(X).components_.T,\n",
    "    index=USArrests.columns,\n",
    "    columns=[\"V1\", \"V2\", \"V3\", \"V4\"],\n",
    ")\n",
    "pca_loadings\n",
    "\n",
    "\"\"\" \n",
    "Depends on the version of python/module, you may see a flipped loading vector in signs. \n",
    "This is normal because the orientation of the principal components is not deterministic. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the PCA model and transform X to get the principal components\n",
    "pca = PCA()\n",
    "df_plot = pd.DataFrame(\n",
    "    pca.fit_transform(X), columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\"], index=X.index\n",
    ")\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(9, 7))\n",
    "\n",
    "ax1.set_xlim(-3.5, 3.5)\n",
    "ax1.set_ylim(-3.5, 3.5)\n",
    "\n",
    "# plot Principal Components 1 and 2\n",
    "for i in df_plot.index:\n",
    "    ax1.annotate(i, (df_plot.PC1.loc[i], -df_plot.PC2.loc[i]), ha=\"center\")\n",
    "\n",
    "# plot reference lines\n",
    "ax1.hlines(0, -3.5, 3.5, linestyles=\"dotted\", colors=\"grey\")\n",
    "ax1.vlines(0, -3.5, 3.5, linestyles=\"dotted\", colors=\"grey\")\n",
    "\n",
    "ax1.set_xlabel(\"First Principal Component\")\n",
    "ax1.set_ylabel(\"Second Principal Component\")\n",
    "\n",
    "# plot Principal Component loading vectors, using a second y-axis.\n",
    "ax2 = ax1.twinx().twiny()\n",
    "\n",
    "ax2.set_ylim(-1, 1)\n",
    "ax2.set_xlim(-1, 1)\n",
    "ax2.tick_params(axis=\"y\", colors=\"orange\")\n",
    "ax2.set_xlabel(\"Principal Component loading vectors\", color=\"orange\")\n",
    "\n",
    "# plot labels for vectors. Variable 'a' is a small offset parameter to separate arrow tip and text.\n",
    "a = 1.07\n",
    "for i in pca_loadings[[\"V1\", \"V2\"]].index:\n",
    "    ax2.annotate(\n",
    "        i, (pca_loadings.V1.loc[i] * a, -pca_loadings.V2.loc[i] * a), color=\"orange\"\n",
    "    )\n",
    "\n",
    "# plot vectors\n",
    "ax2.arrow(0, 0, pca_loadings.V1[0], -pca_loadings.V2[0])\n",
    "ax2.arrow(0, 0, pca_loadings.V1[1], -pca_loadings.V2[1])\n",
    "ax2.arrow(0, 0, pca_loadings.V1[2], -pca_loadings.V2[2])\n",
    "ax2.arrow(0, 0, pca_loadings.V1[3], -pca_loadings.V2[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we can see that the first loading vector places approximately equal weight on `Assault`, `Murder`, and `Rape`, but with much less weight on `UrbanPop`. Hence this component roughly corresponds to a measure of overall rates of serious crimes. The second loading vector places most of its weight on `UrbanPop` and much less weight on the other three features. Hence, this component roughly corresponds to the level of urbanisation of the state. Overall, we see that the crime-related variables ( `Murder`, `Assault`, and `Rape`) are located close to each other, and that the `UrbanPop` variable is far from the other three. This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the `UrbanPop` variable is less correlated with the other three.\n",
    "\n",
    "We can examine differences between the states via the two principal component score vectors shown in the figure above. Our discussion of the loading vectors suggests that states with large positive scores on the first component, such as California, Nevada and Florida, have high crime rates, while states like North Dakota, with negative scores on the first component, have low crime rates. California also has a high score on the second component, indicating a high level of urbanisation, while the opposite is true for states like Mississippi. States close to zero on both components, such as Indiana, have approximately average levels of both crime and urbanisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The proportion of variance explained\n",
    "\n",
    "We can now ask a natural question: how much of the information in a given dataset is lost by projecting the observations onto the first few principal components? That is, how much of the variance in the data is not contained in the first few principal components? More generally, we are interested in knowing the proportion of variance explained (PVE) by each principal component. The total variance present in a dataset (assuming that the variables have been centred to have mean zero) is defined as\n",
    "\n",
    "$$\n",
    "\\sum_{d=1}^D \\textrm{Var}(\\mathbf{x}_d) = \\sum_{d=1}^D \\frac{1}{N} \\sum_{n=1}^N x_{d,n}^2.\n",
    "$$\n",
    "\n",
    "and the variance explained by the $i\\textrm{th}$ principal component is\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{n=1}^N z_{n,i}^2 = \\frac{1}{N}\\sum_{n=1}^N \\left(\\sum_{d=1}^D \\phi_{d,i} x_{n,d}\\right)^2.\n",
    "$$\n",
    "\n",
    "Therefore, the PVE of the $i\\textrm{th}$ principal component is given by\n",
    "\n",
    "```{math}\n",
    ":label: eq:pve\n",
    "\\begin{equation}\n",
    "\\frac{\\sum_{n=1}^N z_{n,i}^2}{\\sum_{d=1}^D\\sum_{n=1}^N  x_{d,n}} = \\frac{\\sum_{n=1}^N \\left(\\sum_{d=1}^D \\phi_{d,i} x_{n,d}\\right)^2}{\\sum_{d=1}^D\\sum_{n=1}^N  x_{d,n}}.\n",
    "\\end{equation}\n",
    "```\n",
    "\n",
    "The PVE of each principal component is a positive quantity. In order to compute the cumulative PVE of the first $i$ principal components, we can simply sum Equation {eq}`eq:pve` over each of the first i PVEs. In total, there are $ \\min (N − 1, D) $ principal components, and their PVEs sum to one. \n",
    "\n",
    "In `scikit-learn`, the portion of explained variance to select the number of PCs are  available in the `PCA` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `USArrests` data, the first principal component explains 62.0 % of the variance in the data, and the next principal component explains 24.7 % of the variance. Together, the first two principal components explain almost 87 % of the variance in the data, and the last two principal components explain only 13 % of the variance. Run the code cell below to plot The PVE of each principal component, as well as the cumulative PVE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "plt.plot(\n",
    "    [1, 2, 3, 4], pca.explained_variance_ratio_, \"-o\", label=\"Individual component\"\n",
    ")\n",
    "plt.plot(\n",
    "    [1, 2, 3, 4], np.cumsum(pca.explained_variance_ratio_), \"-s\", label=\"Cumulative\"\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Proportion of Variance Explained\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.xlim(0.75, 4.25)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "plt.legend(loc=2)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "In this case, if we want to preserve 80% of variance of the data, we need to select 2 PCs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on toy data and system transparency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is adapted from [\"Principal Component Regression vs Partial Least Squares Regression\"](https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html#sphx-glr-auto-examples-cross-decomposition-plot-pcr-vs-pls-py) in `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "n_samples = 500\n",
    "cov = [[3, 3], [3, 4]]\n",
    "X = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 8))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.3, label=\"samples\")\n",
    "\n",
    "for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n",
    "    # comp = comp * var  # scale component by its variance explanation power\n",
    "    plt.arrow(\n",
    "        x=0,\n",
    "        y=0,\n",
    "        dx=comp[0],\n",
    "        dy=comp[1],\n",
    "        color=f\"C{i + 1}\",\n",
    "        width=0.1,\n",
    "        label=f\"Component {i}\",\n",
    "        head_width=0.3,\n",
    "    )\n",
    "\n",
    "plt.arrow(\n",
    "    x=0,\n",
    "    y=0,\n",
    "    dx=X[1, 0] * 0.85,\n",
    "    dy=X[1, 1] * 0.85,\n",
    "    color=\"k\",\n",
    "    width=0.1,\n",
    "    head_width=0.3,\n",
    "    label=\"Input Data Sample\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X[1, 0],\n",
    "    X[1, 1],\n",
    "    alpha=0.8,\n",
    "    facecolors=\"none\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"r\",\n",
    "    linewidths=3,\n",
    ")\n",
    "\n",
    "plt.gca().set(\n",
    "    aspect=\"equal\",\n",
    "    title=\"2-dimensional dataset with principal components\",\n",
    "    xlabel=\"first feature\",\n",
    "    ylabel=\"second feature\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The numbers display below round off to 2 decimal places.\")\n",
    "print(\"Data point: \", np.around(X[1, :], 2))\n",
    "print(\n",
    "    \"Scores of the data poirnt: \", np.around(pca.transform(X[1, :].reshape(1, -1)), 2)\n",
    ")\n",
    "print(\"Mean of the training data: \", np.around(pca.mean_, 2))\n",
    "print(\"PCA loadings: \\n\", np.around(pca.components_, 2))\n",
    "print(\"Explained variance: \", np.around(pca.explained_variance_, 2))\n",
    "print(\"Explained variance ratio: \", np.around(pca.explained_variance_ratio_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} System transparency\n",
    ":class: important\n",
    "\n",
    "- For data point $ [-2.78 -0.93]^{\\top} $, the score vector of PCA can be obtained by\n",
    "\n",
    "$$ \\underbrace{\\left[\\begin{matrix} -0.64 & -0.77 \\\\ 0.77 & -0.64 \\end{matrix}\\right]}_{\\text{Loading}}\\left(\\underbrace{\\left[\\begin{matrix} -2.78 \\\\ -0.93 \\end{matrix}\\right]}_{\\text{Input Data}} - \\underbrace{\\left[\\begin{matrix}0.12 \\\\ 0.12 \\end{matrix}\\right]}_{\\text{Mean}}\\right) = \\underbrace{\\left[\\begin{matrix} 2.67 \\\\ -1.54 \\end{matrix}\\right]}_{\\text{Scores}} $$\n",
    "\n",
    "- The above process can be used inversely to obtain the input data from the scores by times the transpose of the loading matrix ($\\left[\\begin{matrix} -0.64 & -0.77   \\\\ 0.77 &  -0.64  \\end{matrix}\\right]^{\\top}$) on both sides:\n",
    "\n",
    "$$ \n",
    "\\underbrace{\\left[\\begin{matrix} -0.64 & -0.77   \\\\ 0.77 &  -0.64  \\end{matrix}\\right]^{\\top}}_{\\text{Loading}} \\underbrace{\\left[\\begin{matrix} 2.67 \\\\ -1.54 \\end{matrix}\\right]}_{\\text{Scores}} + \\underbrace{\\left[\\begin{matrix}0.12 \\\\ 0.12 \\end{matrix}\\right]}_{\\text{Mean}} = \\underbrace{\\left[\\begin{matrix} -2.78 \\\\ -0.93 \\end{matrix}\\right]}_{\\text{Reconstructed Input}}\n",
    "$$\n",
    "\n",
    "The reconstruction can also be interpreted from the geometric perspective. As displayed in the figure above, the input data point $ [-2.78 -0.93]^{\\top} $ (the black vector), can be viewed as the linear combination of the two loading vectors (components 0 and 1 in orange and green, respectively). The scores of the input data point are the coefficients of the linear combination. The reconstructed input data point is the linear combination of the scores and the loading vectors:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\left[\\begin{matrix} -0.64 \\\\ -0.77 \\end{matrix}\\right]}_{\\text{Component 0}} \\times \\underbrace{2.67}_{\\text{Score 0}} + \\underbrace{\\left[\\begin{matrix} 0.77 \\\\ -0.64 \\end{matrix}\\right]}_{\\text{Component 1}} \\times \\underbrace{(-1.54)}_{\\text{Score 1}} + \\underbrace{\\left[\\begin{matrix}0.12 \\\\ 0.12 \\end{matrix}\\right]}_{\\text{Mean}} = \\underbrace{\\left[\\begin{matrix} -2.78 \\\\ -0.93 \\end{matrix}\\right]}_{\\text{Reconstructed Input}}\n",
    "$$\n",
    "\n",
    "\n",
    "Note: numbers are limited to two decimal places for the sake of clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "min 3 max 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA on the NCI60 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"https://github.com/pykale/transparentML/raw/main/data/NCI60_data.csv\")\n",
    "y = pd.read_csv(\"https://github.com/pykale/transparentML/raw/main/data/NCI60_labs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA()\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "df2_plot = pd.DataFrame(pca2.fit_transform(X_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "color_idx = pd.factorize(y.iloc[:, 0])[0]\n",
    "cmap = plt.cm.hsv\n",
    "\n",
    "# left plot\n",
    "ax1.scatter(\n",
    "    df2_plot.iloc[:, 0], -df2_plot.iloc[:, 1], c=color_idx, cmap=cmap, alpha=0.5, s=50\n",
    ")\n",
    "ax1.set_ylabel(\"Principal Component 2\")\n",
    "\n",
    "# right plot\n",
    "ax2.scatter(\n",
    "    df2_plot.iloc[:, 0], df2_plot.iloc[:, 2], c=color_idx, cmap=cmap, alpha=0.5, s=50\n",
    ")\n",
    "ax2.set_ylabel(\"Principal Component 3\")\n",
    "\n",
    "# custom legend for the classes (y) since we do not create scatter plots per class (which could have their own labels).\n",
    "handles = []\n",
    "labels = pd.factorize(y.iloc[:, 0].unique())\n",
    "norm = mpl.colors.Normalize(vmin=0.0, vmax=14.0)\n",
    "\n",
    "for i, v in zip(labels[0], labels[1]):\n",
    "    handles.append(mpl.patches.Patch(color=cmap(norm(i)), label=v, alpha=0.5))\n",
    "\n",
    "ax2.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n",
    "\n",
    "# xlabel for both plots\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(\"Principal Component 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        df2_plot.iloc[:, :5].std(axis=0, ddof=0).array,\n",
    "        pca2.explained_variance_ratio_[:5],\n",
    "        np.cumsum(pca2.explained_variance_ratio_[:5]),\n",
    "    ],\n",
    "    index=[\"Standard Deviation\", \"Proportion of Variance\", \"Cumulative Proportion\"],\n",
    "    columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_plot.iloc[:, :10].var(axis=0, ddof=0).plot(kind=\"bar\", rot=0)\n",
    "plt.ylabel(\"Variances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# left plot\n",
    "ax1.plot(pca2.explained_variance_ratio_, \"-o\")\n",
    "ax1.set_ylabel(\"Proportion of Variance Explained\")\n",
    "ax1.set_ylim(ymin=-0.01)\n",
    "\n",
    "# right plot\n",
    "ax2.plot(np.cumsum(pca2.explained_variance_ratio_), \"-ro\")\n",
    "ax2.set_ylabel(\"Cumulative Proportion of Variance Explained\")\n",
    "ax2.set_ylim(ymax=1.05)\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_xlim(-1, 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- For data point $ [-2.77595836 -0.929101]^{\\top} $, the score vector of PCA can be obtained by \n",
    "\n",
    "$$ \\underbrace{\\left[\\begin{matrix} -0.64402153, -0.76500736 \\\\ 0.76500736 -0.64402153 \\end{matrix}\\right]^{\\top}}_{\\text{Loading}}\\left(\\underbrace{\\left[\\begin{matrix} -2.77595836 \\\\ -0.929101\\end{matrix}\\right]}_{\\text{Input Data}} - \\underbrace{\\left[\\begin{matrix}0.12105555 \\\\ 0.11655374\\end{matrix}\\right]}_{\\text{Mean of Training Data}}\\right) = \\left[\\begin{matrix}-2.77595836 \\\\ -0.929101\\end{matrix}\\right] $$ -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pykale')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
