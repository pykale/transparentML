{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer neural networks & PyTorch\n",
    "\n",
    "[Neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are a class of machine learning models that are inspired by the structure and function of biological neural networks. They can learn complex functions from large amounts of data. We will start our journey of neural networks with something familiar, the [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), which can be viewed as a neural network in probably the simplest form. We will then introduce single-layer and multilayer neural networks as well as the [PyTorch](https://pytorch.org/) software  library for building neural networks.\n",
    "\n",
    "Watch the 16-minute video below for a visual explanation of neural networks.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/CqOfi41LfDw?start=125&end=1090\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining main ideas behind neural networks by StatQuest](https://www.youtube.com/embed/CqOfi41LfDw?start=125&end=1090), embedded according to [YouTube's Terms of Service](https://www.youtube.com/static?gl=CA&template=terms).\n",
    "```\n",
    "\n",
    "## Logistic regression as a neural network\n",
    "\n",
    "Let us consider a simple neural network to classify data points, using the logistic regression model as an example:\n",
    "\n",
    "* Each data point has one feature/variable, so we need one input node on the input layer.\n",
    "* We are not going to use any hidden layer, for simplicity.\n",
    "* We have two possible output classes so the output of the network will be a single value between 0 and 1, which is the estimated probability $\\pi$ for a data point to belong to class 1. Then, the probability to belong to class 0 is simply $1-\\pi$. Therefore, we have one single output neuron, the only neuron in the network.\n",
    "\n",
    "If we use the [logistic (sigmoid) function](https://en.wikipedia.org/wiki/Logistic_function) as the [activation function](https://en.wikipedia.org/wiki/Activation_function) in the output neuron, this neuron will generate a value between 0 and 1, which can be used as a classification probability.\n",
    "\n",
    "We can represent this simple network visually in the following figure:\n",
    "\n",
    "```{figure} https://github.com/cbernet/maldives/raw/master/images/one_neuron.png\n",
    "---\n",
    "height: 250px\n",
    "name: one_neuron\n",
    "---\n",
    "Neural network with one input node and one neuron, with no hidden layer. The neuron first computes the weighted input $z = wx + b$, where $w$ is the weight and $b$ is the bias, and then uses the sigmoid function $\\sigma (z) = 1/(1+e^{-z})$ as the activation function to compute the output of the neuron.\n",
    "```\n",
    "\n",
    "In the output neuron: \n",
    "\n",
    "* The first box performs a change of variable and computes the **weighted input** $z$ of the neuron, $z = wx + b$, where $w$ is the weight and $b$ is the bias.\n",
    "* The second box applies the **activation function**, the sigmoid $\\sigma (z) = 1/(1+e^{-z})$, to the weighted input $z$.\n",
    "* The output of the neuron is the value of the sigmoid function, which is a value between 0 and 1.\n",
    "\n",
    "This simple network has only two parameters, the weight $w$ and the bias $b$, both used in the first box. We see in particular that when the bias $b$ is very large, the neuron will **always be activated/fired**, whatever the input. On the contrary, for very negative biases, the neuron is **dead**. \n",
    "\n",
    "We can write the output simply as a function of $x$, \n",
    "\n",
    "$$f(x) = \\sigma(z) = \\sigma(wx+b).$$\n",
    "\n",
    "This is exactly the **logistic regression** classifier. Thus, the logistic regression model can be viewed as a simple neural network with a single neuron. The neuron is a linear function of the input feature(s), and the sigmoid function is the activation function. Indeed, the logistic regression model and its multi-class extension, the softmax regression model, are standard units in neural networks. \n",
    "\n",
    "## Shallow vs deep learning\n",
    "\n",
    "Shallow learning models learn their parameters directly from the features of the data {cite}`burkov2019hundred`. Most models that we studied in the previous chapters are shallow learning models, such as linear regression, logistic regression, or support vector machines. They are called shallow because they are composed of a single _layer_ of learning units, or a single learning unit. We see a shallow learning model with a single neuron in {numref}`one_neuron`.\n",
    "\n",
    "Deep learning models are neural networks with multiple (typically more than two) hidden layers. The parameters for such deep learning models are not learned directly from the features of the data. Instead, the features are used to compute the input of the first hidden layer, which is then used to compute the input of the second hidden layer, and so on. The output of the last hidden layer is used as the input of the output layer. The output layer is typically a single neuron with a sigmoid activation function, which can be used to compute a classification probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-layer neural networks\n",
    "\n",
    "{numref}`single_layer_nn` shows a single-layer neural network with two ($D=2$) input nodes/units (features), one hidden layer with four ($K=4$) neurons as the hidden nodes/units (latent features), and one output node/unit (target/label). The input layer is the layer that receives the input data. The hidden layer is the layer that computes the weighted input of the output layer, which can be considered as latent features. The output layer is the layer that computes the output of the network from the weighted input provided by the hidden layer.\n",
    "\n",
    "```{figure} https://upload.wikimedia.org/wikipedia/commons/9/99/Neural_network_example.svg\n",
    "---\n",
    "height: 300px\n",
    "name: single_layer_nn\n",
    "---\n",
    "A simple neural network with a single hidden layer. The hidden layer computes activations $a_1, \\cdots, a_K$ ($K=4$ here) that are nonlinear transformations of linear combinations of the input features $x_1, \\cdots, x_D$ ($D=2$ here). The output layer computes the output $\\hat{y}$ from the activations $a_1, \\ldots, a_K$ in a similar way.\n",
    "```\n",
    "\n",
    "As illustrated in {numref}`one_neuron`, the $k$th neuron in the hidden layer computes the weighted input $z_k$ from the input data $\\mathbf{x}$ using the weights $\\mathbf{w}_k$ and bias $b_k$, and applies the activation function $g(z)$ to the weighted input $z_k$ to compute the output of the neuron as follows:\n",
    "\n",
    "$$a_k=h_k(z_k)=g\\left(w_{k0}+\\sum_{d=1}^Dw_{kd}x_d\\right).$$ \n",
    "\n",
    "The output of the hidden layer is the vector of the outputs of the $K$ neurons in the hidden layer. The output of the network is the output of the output layer, which is computed from the output of the hidden layer, $a_1, \\cdots, a_K$, in a similar way as the output of a hidden neuron is computed from the input features.\n",
    "\n",
    "### System transparency\n",
    "\n",
    "This simple neural network derives four new features from the original two features by computing four differently weighted sums of the original two features and then squashing the weighted sums with an activation function for the hidden layer. It then uses these four new features to compute the output of the network by computing their weighted sum and then squashing the weighted sum with another activation function for the output layer. As mentioned in the overview, neural networks are semi-transparent systems where we can see the transformation of the input to the output, but it is difficult or too complicated to invert the transformation to obtain the input from the output.\n",
    "\n",
    "```{admonition} System transparency\n",
    ":class: important\n",
    "\n",
    "- For any data point $ \\mathbf{x} $, we can transform it through the hidden layer to obtain four latent features $ [a_1 \\;\\; a_2 \\;\\; a_3 \\;\\; a_4]^{\\top} $, and then transform these latent features through the output layer to obtain the output $\\hat{y}$. \n",
    "\n",
    "- Due the the presence of the hidden layer, it is difficult and complicated to invert the transformation from the input to the output for even single-layer neural networks. \n",
    "```\n",
    "\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "The activation function $g(z)$ is a function that maps the weighted input $z_k$ to the output of the neuron. It is typically a **_nonlinear_** function. Early neural networks often use the sigmoid function or the hyperbolic tangent function $\\tanh(\\cdot)$ as the activation function. In modern neural networks, the ReLU function is often used as the activation function:\n",
    "\n",
    "$$g(z)=\\mathtt{ReLU}(z)=\\begin{cases}z & \\text{if } z>0\\\\0 & \\text{otherwise.}\\end{cases}$$\n",
    "\n",
    "The ReLU function is not differentiable at $z=0$, but it is computationally more efficient than the sigmoid function and the hyperbolic tangent function.\n",
    "\n",
    "There are many other activation functions, such as the Gaussian Error Linear Unit (GELU). You can refer to the [Table of activation functions](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions) in Wikipedia for a list of activation functions. The following figure shows the ReLU and GELU functions.\n",
    "\n",
    "```{figure} https://upload.wikimedia.org/wikipedia/commons/4/42/ReLU_and_GELU.svg\n",
    "---\n",
    "height: 300px\n",
    "name: activation_functions\n",
    "---\n",
    "Activation functions: ReLU (left) and GELU (right).\n",
    "```\n",
    "\n",
    "The _nonlinearity_ of the activation function is important for neural networks. Without the nonlinearity, i.e., if all the activation functions of a neural network are linear, this neural network will be equivalent to a simple linear model, no matter how many hidden layers it has (since composition of linear functions is linear). The nonlinearity allows the neural network to learn more complex functions. On the other hand, the nonlinearity also makes the neural network more difficult to train (needs more data), more prone to overfitting, and more difficult to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer neural networks\n",
    "\n",
    "The single-layer neural network in {numref}`single_layer_nn` is a special case of a multilayer neural network. It is easier to make conceptual connections between single-layer neural networks and our previous shallow learning models. In the following sections, we will study multilayer neural networks, which have more than one hidden layer and are more powerful than single-layer neural networks. In theory, a single-layer neural network with a large number of hidden units can approximate any function. However, the learning task of discovering a good set of weights for a single-layer neural network is more difficult than that of discovering a good set of weights for a multilayer neural network. \n",
    "\n",
    "In a multilayer neural network, the input layer is the layer that receives the input features. The hidden layers are the layers that compute the weighted input of the output layer in multiple steps, with each step computing the weighted input of the next layer similar to the above in a single-layer neural network. Therefore, there are multiple levels of transformations of the input features and these multiple levels of latent features represent multiple levels of abstraction of the input features. The output layer is the layer that computes the output of the network from the weighted input provided by the last hidden layer. \n",
    "\n",
    "All modern neural networks are multilayer neural networks, although with various architectures. The popularity of neural networks and deep learning is accelerated by the availability of large datasets, the development of efficient training algorithms and hardware for training and inference. Moreover, the availability of open-source software libraries for neural networks and deep learning has made it easier for researchers and practitioners to use these advanced tools in their applications. PyTorch is one of such open-source software libraries.\n",
    "\n",
    "## PyTorch basics\n",
    "\n",
    "[PyTorch](https://en.wikipedia.org/wiki/PyTorch) is an open-source software library for machine learning and particularly deep learning. It is originally developed by Facebook (Meta) and is available under the Apache 2.0 license. In September 2022, PyTorch has been donated to the [Linux Foundation](https://en.wikipedia.org/wiki/Linux_Foundation) by Facebook (Meta), becoming a fully community-driven open-source project. \n",
    "\n",
    "### PyTorch installation\n",
    "\n",
    "You should install PyTorch and TorchVision by selecting the appropriate [installation option](https://pytorch.org/get-started/locally/) that matches your hardware and software needs. For example, if you have a GPU, you should install PyTorch with GPU support (e.g. `conda install pytorch torchvision pytorch-cuda=11.6 -c pytorch -c nvidia` or `pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu116`). If you do not have a GPU, you should install PyTorch with CPU (e.g. `conda install pytorch torchvision cpuonly -c pytorch` or `pip3 install torch torchvision`).\n",
    "\n",
    "Remove or comment off the following installation if you have installed PyTorch and TorchVision already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "\n",
    "If you are not familiar with PyTorch yet, you can get more comfortable with it by going over at least the first two modules of the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) on *Tensors* and *A gentle introduction to `torch.autograd`* to get a basic understanding. In the following, we learn some basics extracted from the tutorial.\n",
    "\n",
    "`torch.Tensor` is a multidimensional array data structure. You may check out the full list of [tensor types](http://pytorch.org/docs/master/tensors.html) and various [tensor operations](https://pytorch.org/docs/stable/torch.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph\n",
    "A computational graph defines and visualises a sequence of operations to go from input to model output. \n",
    "\n",
    "Consider a linear regression model $\\hat{y} = \\mathbf{W}\\mathbf{x} + b$, where $\\mathbf{x}$ is the input, $\\mathbf{W}$ is a weight matrix, $b$ is a bias, and $\\hat{y}$ is the predicted output. As a computational graph, this looks like the following:\n",
    "\n",
    "![Linear Regression Computation Graph](https://imgur.com/IcBhTjS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch dynamically build the computational graph, as shown in the animation below.\n",
    "\n",
    "![DynamicGraph.gif](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd: automatic differentiation\n",
    "\n",
    "**Why differentiation is important?**\n",
    "\n",
    "Differentiation is important because it is a key procedure in **optimisation** to find the optimal solution of a loss function. The process of learning/training aims to minimise a predefined loss, which is a function of the model parameters. We can compute the gradient of the loss function with respect to the model parameters, and then update the model parameters in the direction of the negative gradient. This process is called **gradient descent**. The gradient descent process is repeated until the model parameters converge to the optimal solution.\n",
    "\n",
    "**How automatic differentiation is done in PyTorch?**\n",
    "\n",
    "The PyTorch `autograd` package makes differentiation (almost) transparent to you by providing automatic differentiation for all operations on Tensors, unless you do not want it (to save time and space). \n",
    "\n",
    "A `torch.Tensor` type variable has an attribute `.requires_grad`. Setting this attribute `True` tracks (but not computes yet) all operations on it. After we define the forward pass, and hence the *computational graph*, we call `.backward()` and all the gradients will be computed automatically and accumulated into the `.grad` attribute. \n",
    "\n",
    "This is made possible by the [**chain rule of differentiation**](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "\n",
    "**How to stop automatic differentiation (e.g., when it is not needed)\n",
    "**\n",
    "\n",
    "Calling method `.detach()` of a tensor will detach it from the computation history. We can also wrap the code block in `with torch.no_grad():` so all tensors in the block do not track the gradients, e.g., in the test/evaluation stage.\n",
    "\n",
    "<!-- #### Function\n",
    "\n",
    "`Tensor`s are connected by `Function` to build an acyclic *computational graph* to encode a complete history of computation. The `.grad_fn` attribute of a tensor references a `Function` created the `Tensor`, i.e., this `Tensor` is the output of its ``.grad_fn`` in the computational graph.\n",
    "\n",
    "Learn more about autograd by referring to the [documentation on autograd](https://pytorch.org/docs/stable/autograd.html) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using PyTorch `nn` module\n",
    "\n",
    "In this section, we will implement a basic linear regression model using the PyTorch `nn` module. The `nn` module provides a high-level API to define neural networks and it uses the `autograd` module to automatically compute the gradients of the model parameters. \n",
    "\n",
    "Implementing a linear regression model in PyTorch will help us study PyTorch concepts closely. This part follows the [PyTorch Linear regression example](https://github.com/pytorch/examples/tree/master/regression) that trains a **single fully-connected layer** to fit a 4th degree polynomial.\n",
    "\n",
    "First, generate model parameters, weight and bias. The weight vector and bias are both tensors, 1D and 0D, respectively. We set a seed (2022) for **reproducibility**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(2022)  # For reproducibility\n",
    "\n",
    "POLY_DEGREE = 4\n",
    "W_target = torch.randn(POLY_DEGREE, 1) * 5\n",
    "b_target = torch.randn(1) * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the weight and bias tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W_target)\n",
    "print(b_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the weight tensor is a 1D tensor with 4 elements, and the bias tensor is a 0D tensor. Both have random values.\n",
    "\n",
    "Next, define a number of functions to generate the input (features/variables) and output (target/response). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(x):\n",
    "    \"\"\"Builds features i.e. a matrix with columns [x, x^2, x^3, x^4].\"\"\"\n",
    "    x = x.unsqueeze(1)\n",
    "    return torch.cat([x**i for i in range(1, POLY_DEGREE + 1)], 1)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Approximated function.\"\"\"\n",
    "    return x.mm(W_target) + b_target.item()\n",
    "\n",
    "\n",
    "def poly_desc(W, b):\n",
    "    \"\"\"Creates a string description of a polynomial.\"\"\"\n",
    "    result = \"y = \"\n",
    "    for i, w in enumerate(W):\n",
    "        result += \"{:+.2f} x^{} \".format(w, i + 1)\n",
    "    result += \"{:+.2f}\".format(b[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_batch(batch_size=32):\n",
    "    \"\"\"Builds a batch i.e. (x, f(x)) pair.\"\"\"\n",
    "    random = torch.randn(batch_size)\n",
    "    x = make_features(random)\n",
    "    y = f(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple neural network, which is a **single fully connected** (**FC**) layer, using the [`torch.nn.Linear`](https://pytorch.org/docs/master/nn.html#torch.nn.Linear) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = torch.nn.Linear(W_target.size(0), 1)\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a *network* with four input units, one output unit, with a bias term.\n",
    "    \n",
    "Now generate the data. Let us try to get five pairs of (x,y) first to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, sample_y = get_batch(5)\n",
    "print(sample_x)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the FC layer weights (randomly initialised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fc.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the gradients to zero, perform a forward pass to get prediction, and compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.zero_grad()\n",
    "output = F.smooth_l1_loss(fc(sample_x), sample_y)\n",
    "loss = output.item()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the loss is large and random initialisation did not give a good prediction. Let us do a backpropagation and update model parameters with gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()\n",
    "for param in fc.parameters():\n",
    "    param.data.add_(-0.1 * param.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the updated weights and respective loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fc.weight)\n",
    "output = F.smooth_l1_loss(fc(sample_x), sample_y)\n",
    "loss = output.item()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the loss is reduced and the weights are updated. \n",
    "\n",
    "Now keep feeding more data until the loss is small enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "for batch_idx in count(1):\n",
    "    # Get data\n",
    "    batch_x, batch_y = get_batch()\n",
    "\n",
    "    # Reset gradients\n",
    "    fc.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = F.smooth_l1_loss(fc(batch_x), batch_y)\n",
    "    loss = output.item()\n",
    "\n",
    "    # Backward pass\n",
    "    output.backward()\n",
    "\n",
    "    # Apply gradients\n",
    "    for param in fc.parameters():\n",
    "        param.data.add_(-0.1 * param.grad.data)\n",
    "\n",
    "    # Stop criterion\n",
    "    if loss < 1e-3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss: {:.6f} after {} batches\".format(loss, batch_idx))\n",
    "print(\"==> Learned function:\\t\" + poly_desc(fc.weight.view(-1), fc.bias))\n",
    "print(\"==> Actual function:\\t\" + poly_desc(W_target.view(-1), b_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the loss is small and the weights are close to the true values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer neural network for digit classification\n",
    "\n",
    "Now, let us implement a multilayer neural network in PyTorch for digit classification using the popular [MNIST dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "This part follows the [MNIST handwritten digits classification with MLPs](https://github.com/CSCfi/machine-learning-scripts/blob/master/notebooks/pytorch-mnist-mlp.ipynb) notebook.\n",
    "\n",
    "Get ready by importing other APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using PyTorch version:\", torch.__version__, \" Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching and epoch\n",
    "\n",
    "Neural networks are typically trained using _mini-batches_ for time/memory efficiency and better generalisation. For images, we can compute the *average* loss across a mini-batch of $B$ *multiple* images and take a step to minimise their *average* loss. The number $B$ is called the **batch size**. The actual batch size that we choose depends on many things. We want our batch size to be large enough to not be too \"noisy\", but not so large as to make each iteration too expensive to run. People often choose batch sizes of the form $B=2^k$ (a power of 2)so that it is easy to half or double the batch size.\n",
    "\n",
    "Moreover, neural networks are trained using multiple _epochs_ to further improve generalisation, where each `epoch` is one complete pass through the whole training data to update the parameters. In this way, we use each training data point more than once. \n",
    "\n",
    "### PyTorch data loader and transforms\n",
    "\n",
    "[`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) combines a dataset and a sampler to iterate over the dataset. Thus, we use the `DataLoader` to create an iterator that provides a stream of mini-batches. The `DataLoader` class takes a dataset as input, randomly groups the training data into **mini-batches** with the specified batch size, and returns an iterator over all these mini-batches. Each data point belongs to only one mini-batch. The `DataLoader` class also provides a `shuffle` parameter to shuffle the data after every epoch for better generalisation.\n",
    "\n",
    "The [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) module provides a variety of functions to perform common image transformations. We will use the `ToTensor` function to convert the images to PyTorch tensors. The output of `torchvision` `datasets` after loading are PILImage images of range [0, 1]. `transforms.ToTensor()` Convert a `PIL` Image or `numpy.ndarray` ($H \\times W \\times C$) in the range [0, 255] to torch.FloatTensor of shape $C \\times H \\times W$ in the range [0.0, 1.0].\n",
    "\n",
    "### Load the MNIST dataset\n",
    "\n",
    "Load the MNIST dataset using the `torchvision.datasets.MNIST` class. The dataset is downloaded automatically if it is not available locally. The dataset is split into training and test sets. The training set is used to train the model, and the test set is used to evaluate the model. We hide the output of the following cell for brevity.\n",
    "\n",
    "<!-- Note that we are here using the MNIST test data for validation, instead of for testing the final model. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "root_dir = \"./data\"\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=root_dir, train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=root_dir, train=False, download=True, transform=transform\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the train and test data are provided via data loaders that iterate over the datasets in mini-batches (and epochs). The `batch_size` parameter specifies mini-batch size, i.e. the number of samples to be loaded at a time. The `shuffle` parameter specifies whether the data should be shuffled after every `epoch`.\n",
    "\n",
    "We can verify the size of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_train, y_train in train_loader:\n",
    "    print(\"X_train:\", X_train.size(), \"type:\", X_train.type())\n",
    "    print(\"y_train:\", y_train.size(), \"type:\", y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train`, one element of the training data loader `train_loader`, is a 4th-order tensor of size (batch_size, 1, 28, 28), i.e. it consists of a batch of 32 images of size $1\\times 28\\times 28$ pixels. `y_train`, the other element of `train_loader`, is a vector containing the correct classes (\"0\", \"1\", $\\cdots$, \"9\") for each training digit.\n",
    "\n",
    "Examine the dataset to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.data.shape)\n",
    "print(train_dataset.data.max())  # Max pixel value\n",
    "print(train_dataset.data.min())  # Min pixel value\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the training set has 60,000 images. Each image is a $28\\times 28$ grayscale image, which can be represented as a 1D tensor of size 784 ($28\\times 28$).\n",
    "\n",
    "Let us visualise the first 10 images in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(train_dataset.data[i], cmap=\"gray\")\n",
    "    plt.title(\"Class: \" + str(train_dataset.targets[i].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a multilayer neural network\n",
    "\n",
    "Let us define a multilayer neural network as a Python class. We need to define the `__init__()` and `forward()` methods, and PyTorch will automatically generate a `backward()` method for computing the gradients for the backward pass. We also need to define an optimizer to update the model parameters based on the computed gradients. Here, we use _stochastic gradient descent (with momentum)_ as the optimization algorithm, and set the _learning rate_ to 0.01 and _momentum_ to 0.5. You can find out other optimizers and their parameters from the [`torch.optim` API](https://pytorch.org/docs/stable/optim.html). Finally, we define a loss function, which is the cross-entropy loss for classification.\n",
    "\n",
    "In the following, we use the _dropout_ method to alleviate overfitting. Dropout is a regularization technique where randomly selected neurons are ignored during training. This forces the network to learn features in a distributed way. The `nn.Dropout` module randomly sets input units to 0 with a frequency of `p` at each step during training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultilayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 50)  # 28*28 = 784\n",
    "        self.fc1_drop = nn.Dropout(0.2)  # Dropout layer with 0.2 drop probability\n",
    "        self.fc2 = nn.Linear(50, 50)  # 50 inputs and 50 outputs (next hidden layer)\n",
    "        self.fc2_drop = nn.Dropout(0.2)  # Dropout layer with 0.2 drop probability\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the data (n, 1, 28, 28) -> (n, 784)\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return F.log_softmax(self.fc3(x), dim=1)  # Log softmax\n",
    "\n",
    "\n",
    "model = MultilayerNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model\n",
    "\n",
    "Let us now define functions to `train()` and `test()` the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=200):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero gradient buffers\n",
    "        output = model(data)  # Pass data through the network\n",
    "        loss = criterion(output, target)  # Calculate loss\n",
    "        loss.backward()  # Backpropagate\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.data.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(loss_vector, accuracy_vector):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).data.item()\n",
    "        pred = output.data.max(1)[1]  # Get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    loss_vector.append(test_loss)\n",
    "\n",
    "    accuracy = 100.0 * correct.to(torch.float32) / len(test_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "\n",
    "    print(\n",
    "        \"\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model using the `train()` function. After each epoch, we evaluate the model using `test()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "\n",
    "loss_test, acc_test = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(loss_test, acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualise how the training progressed in terms of the test loss and the test accuracy. The loss is a function of the difference of the network output and the target values and it should decrease over time. The accuracy is the fraction of correct predictions over the total number of predictions and it should increase over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Test loss\")\n",
    "plt.plot(np.arange(1, epochs + 1), loss_test)\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Test accuracy (%)\")\n",
    "plt.plot(np.arange(1, epochs + 1), acc_test)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the test loss is decreasing and the test accuracy is increasing over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [MedMNIST](https://medmnist.com/) dataset, published in **ISBI21**, we are going to use the [BreastMNIST](https://zenodo.org/record/4269852/files/breastmnist.npz?download=1) dataset set for the following exercises, which contains $780$ **breast ultrasound images**. Originally, it was categorised into three classes: **normal, benign, and malignant**, but in BreastMNIST, the task is simplified into a **binary classification** task by combining normal and benign as **positive** and malignant as **negative**. The source dataset was divided into training, validation, and test sets in a $70:10:20$ ratio. The source images of $1 × 500 × 500$ are resized to $1 × 28 × 28$.\n",
    "\n",
    "**1.** Follow the instructions at [https://github.com/MedMNIST/MedMNIST](https://github.com/MedMNIST/MedMNIST) to download and load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Install medmnist\n",
    "!python -m pip install medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the training, validation, and testing datasets with PyTorch Dataloader. Following the method in Section [10.1.7.3](https://pykale.github.io/transparentML/10-deep-cnn-rnn/multilayer-nn.html#load-the-mnist-dataset) and the PyTorch [documentation](https://pytorch.org/vision/stable/transforms.html), create a *Composition* of transforms in torchvision.transforms (`transforms.Compose()`) to convert an image to a tensor, normalise it, and flatten it. \n",
    "\n",
    "Load the training, validation and testing splits, apply your transform to the data. Create training, validation, and testing dataloaders with a `batch_size` of 64.\n",
    "\n",
    " **Hint**: See the [MedMNIST getting started documentation](https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "import medmnist\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# get dataset info and data class\n",
    "DS_INFO = medmnist.INFO[\"breastmnist\"]\n",
    "data_class = getattr(medmnist.dataset, DS_INFO[\"python_class\"])\n",
    "\n",
    "\n",
    "# apply scaling/flattening transforms to data\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5)),  # Normalise the image data\n",
    "        transforms.Lambda(lambda x: torch.flatten(x)),  # Flatten the image\n",
    "    ]\n",
    ")\n",
    "# note these transforms are only applied when we iterate through the data\n",
    "# we can still access the raw data with data_class.imgs\n",
    "\n",
    "# get the splits\n",
    "train_dataset = data_class(split=\"train\", download=True, transform=transform)\n",
    "val_dataset = data_class(split=\"val\", download=True, transform=transform)\n",
    "test_dataset = data_class(split=\"test\", download=True, transform=transform)\n",
    "\n",
    "BATCH_SIZE = 64  # intialised batchsize=64, which means 64 data in a batch\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")  # loading the train dataset with Dataloader\n",
    "\n",
    "valid_loader = data.DataLoader(\n",
    "    dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")  # loading the validation dataset with Dataloader\n",
    "\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")  # loading the test dataset with Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** From the loaded dataset in **Exercise $1$**, display at least **ten images** from the training set, validation set, and testing set for each class, i.e., at least **$20$ images** for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define a function for easy plotting\n",
    "# For visualising data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_images(dataset, num=10):\n",
    "    \"\"\"\n",
    "    Plots num images from each of the 2 classes.\n",
    "    \"\"\"\n",
    "    # get 10 class 0 images\n",
    "    class0_imgs = dataset.imgs[dataset.labels[:, 0] == 0][:num]\n",
    "    # get 10 class 1 images\n",
    "    class1_imgs = dataset.imgs[dataset.labels[:, 0] == 1][:num]\n",
    "\n",
    "    fig, axs = plt.subplots(2, num, figsize=(2 * num, 4))\n",
    "    # add entire plot title\n",
    "    plt.suptitle(dataset.split + \" images\")\n",
    "    for i in range(num):\n",
    "        axs[0, i].imshow(class0_imgs[i])\n",
    "        axs[0, i].axis(\"off\")\n",
    "        # add each plot title\n",
    "        axs[0, i].set_title(DS_INFO[\"label\"][\"0\"])\n",
    "        axs[1, i].imshow(class1_imgs[i])\n",
    "        axs[1, i].axis(\"off\")\n",
    "        axs[1, i].set_title(DS_INFO[\"label\"][\"1\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_images(train_dataset)\n",
    "plot_images(val_dataset)\n",
    "plot_images(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Using the **PyTorch** *nn* module, train a single layer logistic regression model with the number of parameters as the input dimensionality of the data. Train on the training set for 20 epochs and evaluate the model on the validation set using accuracy as the evaluation metric. Draw separate graphs for the **validation loss and accuracy** of the model over each epoch.\n",
    "\n",
    "What is the **accuracy** on the test set of the fully trained logistic regression model? Are the validation results consistent with the test results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# Create a logisticregression model with torch.nn.Module\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.loss_fun = nn.BCELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "input_size = 28 * 28\n",
    "sig_model = LogisticRegression(input_size).to(device)\n",
    "optimizer = torch.optim.SGD(sig_model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "def train(model, epoch):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero gradient buffers\n",
    "        output = model(data)  # Pass data through the network\n",
    "        loss = criterion(output, target.to(torch.float32))  # Calculate loss\n",
    "        loss.backward()  # Backpropagate\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        return print(\"Train Epoch: {} \\tLoss: {:.6f}\".format(epoch, loss.data.item()))\n",
    "\n",
    "\n",
    "def test(model, loss_vector, accuracy_vector, eval_dataloader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss, total_val_correct, total_val_count = 0, 0, 0\n",
    "    for data, target in eval_dataloader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target.to(torch.float32)).data.item()\n",
    "\n",
    "        total_val_correct += torch.sum(torch.eq((output > 0.5).long(), target))\n",
    "        total_val_count += len(target)\n",
    "\n",
    "    test_loss /= len(eval_dataloader)\n",
    "    loss_vector.append(test_loss)\n",
    "\n",
    "    accuracy = (\n",
    "        100.0 * total_val_correct.to(torch.float32) / len(eval_dataloader.dataset)\n",
    "    )\n",
    "    accuracy_vector.append(accuracy)\n",
    "\n",
    "    print(\n",
    "        \"\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, total_val_correct, total_val_count, accuracy\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "loss_test, acc_test = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(sig_model, epoch)\n",
    "    test(sig_model, loss_test, acc_test, valid_loader)\n",
    "\n",
    "if device == torch.device(\"cuda\"):\n",
    "    acc_test = [acc.cpu().numpy() for acc in acc_test]\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Validation loss\")\n",
    "plt.plot(np.arange(1, epochs + 1), loss_test)\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Test accuracy (%)\")\n",
    "plt.plot(np.arange(1, epochs + 1), acc_test)\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "loss_test, acc_test = [], []\n",
    "test(sig_model, loss_test, acc_test, test_loader)\n",
    "print(\n",
    "    \"The accuracy of the sigmoid model on the test set is: \", acc_test[-1].item(), \"%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Now, use the **multilayer neural network** model from [10.1.7.4](https://pykale.github.io/transparentML/10-deep-cnn-rnn/multilayer-nn.html#define-a-multilayer-neural-network) to train a model on the training set and evaluate the model on the validation set using accuracy as the evaluation metrics. Draw separate graphs for the **validation loss and accuracy** of the model over each epoch. Use the **sigmoid** function instead of the **log softmax** function for binary classification, and change the **output dimension** of the final layer accordingly.\n",
    "\n",
    "What is the **accuracy** on the test set of the fully trained multilayer NN model? Are the validation results consistent with the test results? Is it better than the logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "class MultilayerNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultilayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 50)  # 28*28 = 784\n",
    "        self.fc1_drop = nn.Dropout(0.2)  # Dropout layer with 0.2 drop probability\n",
    "        self.fc2 = nn.Linear(50, 50)  # 50 inputs and 50 outputs (next hidden layer)\n",
    "        self.fc2_drop = nn.Dropout(0.2)  # Dropout layer with 0.2 drop probability\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the data (n, 1, 28, 28) -> (n, 784)\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        x = self.fc3(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "model_nn = MultilayerNN().to(device)\n",
    "optimizer = torch.optim.SGD(model_nn.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "loss_test, acc_test = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model_nn, epoch)\n",
    "    test(model_nn, loss_test, acc_test, valid_loader)\n",
    "\n",
    "\n",
    "if device == torch.device(\"cuda\"):\n",
    "    acc_test = [acc.cpu().numpy() for acc in acc_test]\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Validation loss\")\n",
    "plt.plot(np.arange(1, epochs + 1), loss_test)\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation accuracy (%)\")\n",
    "plt.plot(np.arange(1, epochs + 1), acc_test)\n",
    "plt.grid()\n",
    "\n",
    "loss_test, acc_test = [], []\n",
    "test(model_nn, loss_test, acc_test, test_loader)\n",
    "print(\n",
    "    \"The accuracy of the multilayer NN model on the test set is: \",\n",
    "    acc_test[-1].item(),\n",
    "    \"%\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
