{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer neural networks and PyTorch\n",
    "\n",
    "[Neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are a class of machine learning models that are inspired by the structure and function of biological neural networks. They can learn complex functions from large amounts of data. We will start our journey of neural networks with the simplest neural network, the [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), which is a single-layer neural network that we have learned [previously](https://pykale.github.io/transparentML/03-logistic-reg/overview.html). We will then introduce multilayer neural networks and the [PyTorch](https://pytorch.org/) library to build our neural networks.\n",
    "\n",
    "Watch the 16-minute video below for a visual explanation of neural networks.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/CqOfi41LfDw?start=125&end=1090\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining main ideas behind neural networks, by StatQuest](https://www.youtube.com/embed/CqOfi41LfDw?start=125&end=1090)\n",
    "```\n",
    "\n",
    "## Logistic regression as a neural network\n",
    "\n",
    "Let us consider a simple neural network to classify data points, using the logistic regression model as an example:\n",
    "\n",
    "* Each data point has one feature/variable, so we need one input node on the input layer.\n",
    "* We are not going to use any hidden layer, for simplicity.\n",
    "* We have two possible output classes so the output of the network will be a single value between 0 and 1, which is the estimated probability $\\pi$ for a data point to belong to class 1. Then, the probability to belong to class 0 is simply $1-\\pi$. Therefore, we have one single output neuron, the only neuron in the network.\n",
    "\n",
    "If we use the [logistic (sigmoid) function](https://en.wikipedia.org/wiki/Logistic_function) as the [activation function](https://en.wikipedia.org/wiki/Activation_function) in the output neuron, this neuron will generate a value between 0 and 1, which can be used as a classification probability.\n",
    "\n",
    "We can represent this simple network visually in the following figure:\n",
    "\n",
    "```{figure} https://github.com/cbernet/maldives/raw/master/images/one_neuron.png\n",
    "---\n",
    "height: 250px\n",
    "name: one_neuron\n",
    "---\n",
    "Neural network with one input node and one neuron, with no hidden layer. The neuron first computes the weighted input $z = wx + b$, where $w$ is the weight and $b$ is the bias, and then uses the sigmoid function $\\sigma (z) = 1/(1+e^{-z})$ as the activation function to compute the output of the neuron.\n",
    "```\n",
    "\n",
    "In the output neuron: \n",
    "\n",
    "* The first box performs a change of variable and computes the **weighted input** $z$ of the neuron, $z = wx + b$, where $w$ is the weight and $b$ is the bias.\n",
    "* The second box applies the **activation function**, the sigmoid $\\sigma (z) = 1/(1+e^{-z})$, to the weighted input $z$.\n",
    "* The output of the neuron is the value of the sigmoid function, which is a value between 0 and 1.\n",
    "\n",
    "This simple network has only two parameters, the weight $w$ and the bias $b$, both used in the first box. We see in particular that when the bias $b$ is very large, the neuron will **always be activated**, whatever the input. On the contrary, for very negative biases, the neuron is **dead**. \n",
    "\n",
    "We can write the output simply as a function of $x$, \n",
    "\n",
    "$$f(x) = \\sigma(z) = \\sigma(wx+b).$$\n",
    "\n",
    "This is exactly the **logistic regression** classifier. Thus, the logistic regression model can be viewed as a single-layer neural network with a single neuron. The neuron is a linear function of the input feature(s), and the sigmoid function is the activation function. Indeed, the logistic regression model and its multi-class extension, the softmax regression model, are standard units in neural networks. \n",
    "\n",
    "## Shallow vs deep learning\n",
    "\n",
    "Shallow learning models learn their parameters directly from the features of the data {cite}`burkov2019hundred`. Most models that we studied in the previous chapters are shallow learning models, such as linear regression, logistic regression, or support vector machines. They are called shallow because they are composed of a single _layer_ of learning units, or a single learning unit. We see a shallow learning model with a single neuron in {numref}`one_neuron`.\n",
    "\n",
    "Deep learning models are neural networks with multiple (typically more than two) hidden layers. The parameters for such deep learning models are not learned directly from the features of the data. Instead, the features are used to compute the input of the first hidden layer, which is then used to compute the input of the second hidden layer, and so on. The output of the last hidden layer is used as the input of the output layer. The output layer is typically a single neuron with a sigmoid activation function, which can be used to compute a classification probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-layer neural networks\n",
    "\n",
    "{numref}`single_layer_nn` shows a single-layer neural network with two ($D=2$) input nodes/units (features), one hidden layer with four ($K=4$) neurons as the hidden nodes/units (latent features), and one output node/unit (target/label). The input layer is the layer that receives the input data. The hidden layer is the layer that computes the weighted input of the output layer, which can be considered as latent features. The output layer is the layer that computes the output of the network from the weighted input provided by the hidden layer.\n",
    "\n",
    "```{figure} https://upload.wikimedia.org/wikipedia/commons/9/99/Neural_network_example.svg\n",
    "---\n",
    "height: 300px\n",
    "name: single_layer_nn\n",
    "---\n",
    "A simple neural network with a single hidden layer. The hidden layer computes activations $a_1, \\cdots, a_K$ ($K=4$ here) that are nonlinear transformations of linear combinations of the input features $x_1, \\cdots, x_D$ ($D=2$ here). The output layer computes the output $\\hat{y}$ from the activations $a_1, \\ldots, a_K$ in a similar way.\n",
    "```\n",
    "\n",
    "As illustrated in {numref}`one_neuron`, the $k$th neuron in the hidden layer computes the weighted input $z_k$ from the input data $\\mathbf{x}$ using the weights $\\mathbf{w}_k$ and bias $b_k$, and applies the activation function $g(z)$ to the weighted input $z_k$ to compute the output of the neuron as follows:\n",
    "\n",
    "$$a_k=h_k(z_k)=g\\left(w_{k0}+\\sum_{d=1}^Dw_{kd}x_d\\right).$$ \n",
    "\n",
    "The output of the hidden layer is the vector of the outputs of the $K$ neurons in the hidden layer. The output of the network is the output of the output layer, which is computed from the output of the hidden layer, $a_1, \\cdots, a_K$, in a similar way as the output of a hidden neuron is computed from the input features.\n",
    "\n",
    "### System transparency\n",
    "\n",
    "This simple neural network derives four new features from the original two features by computing four differently weighted sums of the original two features and then squashing the weighted sums with an activation function for the hidden layer. It then uses these four new features to compute the output of the network by computing their weighted sum and then squashing the weighted sum with another activation function for the output layer. As mentioned in the overview, neural networks are semi-transparent systems where we can see the transformation of the input to the output, but it is difficult or too complicated to invert the transformation to obtain the input from the output.\n",
    "\n",
    "```{admonition} System transparency\n",
    ":class: important\n",
    "\n",
    "- For any data point $ \\mathbf{x} $, we can transform it through the hidden layer to obtain four latent features $ [a_1 \\;\\; a_2 \\;\\; a_3 \\;\\; a_4]^{\\top} $, and then transform these latent features through the output layer to obtain the output $\\hat{y}$. \n",
    "\n",
    "- Due the the presence of the hidden layer, it is difficult and complicated to invert the transformation from the input to the output for even single-layer neural networks. \n",
    "```\n",
    "\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "The activation function $g(z)$ is a function that maps the weighted input $z_k$ to the output of the neuron. It is typically a **_nonlinear_** function. Early neural networks often use the sigmoid function or the hyperbolic tangent function as the activation function. In modern neural networks, the ReLU function is often used as the activation function:\n",
    "\n",
    "$$g(z)=\\mathtt{ReLU}(z)=\\begin{cases}z & \\text{if } z>0\\\\0 & \\text{otherwise.}\\end{cases}$$\n",
    "\n",
    "The ReLU function is not differentiable at $z=0$, but it is computationally more efficient than the sigmoid function and the hyperbolic tangent function.\n",
    "\n",
    "There are many other activation functions, such as the Gaussian Error Linear Unit (GELU). You can refer to the [Table of activation functions](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions) in Wikipedia for a list of activation functions. The following figure shows the ReLU and GELU functions.\n",
    "\n",
    "```{figure} https://upload.wikimedia.org/wikipedia/commons/4/42/ReLU_and_GELU.svg\n",
    "---\n",
    "height: 300px\n",
    "name: activation_functions\n",
    "---\n",
    "Activation functions: ReLU (left) and GELU (right).\n",
    "```\n",
    "\n",
    "The _nonlinearity_ of the activation function is important for neural networks. Without the nonlinearity, i.e., if all the activation functions of a neural network are linear, this neural network will be equivalent to a simple linear model, no matter how many hidden layers it has (since composition of linear functions is linear). The nonlinearity allows the neural network to learn more complex functions. On the other hand, the nonlinearity also makes the neural network more difficult to train (needs more data), more prone to overfitting, and more difficult to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with as a neural network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "min 3 max 5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
