{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis\n",
    "<!-- Dimension reduction -->\n",
    "\n",
    "This section introduces [principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis). PCA is a popular method for reducing the dimensionality of data, often represented as a data matrix $\\mathbf{X} \\in \\mathbb{R}^{D \\times N}$, where $N$ is the number of observations and $D$ is the number of variables (features). When faced with a large set of potentially correlated variables, PCA allows us to summarise this set with a smaller number of representative variables that collectively explain most of the variability in the original set. \n",
    "\n",
    "<!-- Thus, PCA is also a popular method for visualising high-dimensional data, and for imputing missing values in data matrices. -->\n",
    "<!-- The first principal component direction of the data is that along which the observations vary the most. -->\n",
    "\n",
    "<!-- PCA refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. -->\n",
    "PCA is an unsupervised method, since it involves only a set of $D$ features $x_1, \\ldots, x_D$ and no associated response $y$. PCA is a standard [feature extraction](https://en.wikipedia.org/wiki/Feature_extraction) or [feautre/representation learning](https://en.wikipedia.org/wiki/Feature_learning) that is often used as a pre-processing step to reduce the feature dimensionality for supervised learning problems. PCA also serves as a standard tool for [data visualisation](https://en.wikipedia.org/wiki/Data_and_information_visualization), for visualising either observations or features. It can also be used as a factorisation model for data imputation, i.e. for filling in missing values in a data matrix.\n",
    "\n",
    "Watch the 5-minute video below for a visual explanation of principal component analysis.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/HMOI_lkzW08?start=10\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining main ideas behind principal component analysis by StatQuest](https://www.youtube.com/embed/HMOI_lkzW08?start=10), embedded according to [YouTube's Terms of Service](https://www.youtube.com/static?gl=CA&template=terms).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for dimensionality reduction\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Suppose that we wish to visualise observations with measurements on a set of $ D $ features, $x_1, x_2, \\ldots, x_D $, as part of an [exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis). We could do this by examining two-dimensional scatterplots of the data, each of which contains the $N$ observations’ measurements on two of the features. However, there are $ D(D-1)/2 $ such scatterplots, and it is difficult to visualise more than a few of them at a time even for a $D$ of small value (e.g. if $D=10$, then $ D(D-1)/2 = 45 $). If $ D $ is large, then it will certainly not be possible to look at all of them; moreover, most likely none of them will be informative since they each contain just a small fraction of the total information present in the dataset. \n",
    "\n",
    "Clearly, a better method is needed to visualise the $N$ observations when $D$ is large. In particular, we would like to find a lower-dimensional representation of the data that captures as much of the information as possible. For instance, if we can obtain a two-dimensional representation of the data that captures most of the information, then we can plot the observations in this lower-dimensional space to gain a visual understanding of the data.\n",
    "\n",
    "PCA provides a tool to do just this. It finds a lower-dimensional representation of a dataset that contains as much as possible of the variation. The idea is that each of the $N$ observations lives in $D$-dimensional space, but not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. \n",
    "\n",
    "### Principal components (PCs)\n",
    "\n",
    "<!-- We now explain the manner in which these dimensions, named as _principal components_, are found. -->\n",
    "Each of the dimensions found by PCA is called a principal component (PC), and it is a linear combination of the $D$ features. \n",
    "\n",
    "The first PC of a set of features $x_1, \\ldots, x_D$ is the normalised linear combination of the features\n",
    "\n",
    "$$\n",
    "y_1 = \\phi_{1,1}x_1 + \\phi_{2,1}x_2 + \\cdots + \\phi_{D,1}x_D\n",
    "$$\n",
    "\n",
    "that has the largest variance. By normalised, we mean that $ \\sum_{d=1}^D \\phi_{d,1}^2 = 1 $ for the first PC $m=1$ and $ \\sum_{d=1}^D \\phi_{d,m}^2 = 1$ for the $m\\text{th}$ PC in general. We refer to the elements $\\phi_{1, 1}, \\ldots, \\phi_{D, 1} $ as the  _loadings_ of the first _principal loading component_. Together, the loadings make up the first _principal component loading vector_, $\\boldsymbol{\\phi}_1 = [\\phi_{1,1}, \\ldots, \\phi_{D,1}]^\\top$. We constrain the loadings so that their sum of squares is equal to one, i.e. normalised, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance, giving a trivial solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the first PC\n",
    "\n",
    "Given a $D \\times N$ dataset as a data matrix $\\mathbf{X}$, how do we compute the first principal component? Since we are only interested in variance, we assume that each of the features in $ \\mathbf{X} $ has been centred to have mean zero (that is, the column means of $ \\mathbf{X} $ are zero). We then look for the linear combination of the sample feature values of the form\n",
    "\n",
    "$$\n",
    "y_{n,1} = \\phi_{1,1}x_{n,1} + \\phi_{2,1}x_{n,2} + \\cdots + \\phi_{D,1}x_{n,D}\n",
    "$$\n",
    "\n",
    "that has the largest sample variance, subject to the constraint that $\\sum_{d=1}^D \\phi_{d,1}^2 = 1$. In other words, the first principal component loading vector $\\boldsymbol{\\phi}_1$ solves the optimisation problem\n",
    "\n",
    "```{math}\n",
    ":label: eq:1stpc\n",
    "\\begin{equation}\n",
    "\\arg\\max_{\\boldsymbol{\\phi}_1} \\left\\{\\frac{1}{N} \\sum_{n=1}^N \\left(\\sum_{d=1}^D \\phi_{d,1} x_{n,d} \\right)^2\\right\\} \\quad \\text{subject to} \\quad \\sum_{d=1}^D \\phi_{d,1}^2 = 1\n",
    "\\end{equation}\n",
    "```\n",
    "\n",
    "where the objective can also be written as $\\frac{1}{N} \\sum_{n=1}^N y_{n,1}^2 $. Equation {eq}`eq:1stpc` maximises the sample variance of the $N$ values of $ y_{n,1} $. We refer $ y_{1,1}, \\ldots, y_{N,1} $ as the first _principal component scores_. Equation {eq}`eq:1stpc` can be solved via a standard technique in linear algebra, [_eigendecomposition_](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) or [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition), as the first eigenvector of the covariance matrix $\\mathbf{X}\\mathbf{X}^\\top$, the one corresponding to the largest eigenvalue, where this eigenvalue is the variance captured in $ \\{y_{n,1}\\}$. The mathematical details of which are beyond the scope of this course. \n",
    "\n",
    "### Interpreting the first PC\n",
    "\n",
    "There is a nice geometric interpretation for the first principal component. The loading vector $ \\boldsymbol{\\phi}_1 $ with elements $ \\phi_{1,1}, \\ldots, \\phi_{D, 1} $ defines a direction in feature space along which the data vary the most. If we project the $N$ data points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ onto this direction, the projected values are the principal component scores $ y_{1, 1} , \\ldots , y_{N, 1}$ themselves. \n",
    "\n",
    "### Computing the second PC and beyond\n",
    "\n",
    "After the first principal component $ y_{n,1} $ of the features has been determined, we can find the second principal component $ y_{n,2} $. The second principal component is the linear combination of $ x_1 , \\ldots, x_D $ that has maximal variance out of all linear combinations that are uncorrelated with $ y_1 $. The second principal component scores $ y_{1,2}, y_{2,2}, \\ldots , y_{N,2}$ take the form\n",
    "\n",
    "$$\n",
    "y_{n,2} = \\phi_{1,2}x_{n,1} + \\phi_{2,2}x_{n,2} + \\cdots + \\phi_{D,2}x_{n,D},\n",
    "$$\n",
    "\n",
    "where $ \\boldsymbol{\\phi}_2 = [\\phi_{1,2}, \\ldots, \\phi_{D,2}]^\\top$ is the second principal component loading vector. It turns out that constraining $ \\{y_{n,2}\\} $ to be uncorrelated with $ \\{y_{n,1}\\} $ is equivalent to constraining the direction $ \\boldsymbol{\\phi}_2 $ to be orthogonal (perpendicular) to the direction $ \\boldsymbol{\\phi}_1 $. Thus, by (beautiful) linear algebra, $ \\boldsymbol{\\phi}_2 $ is the second eigenvector of the covariance matrix $\\mathbf{X}\\mathbf{X}^\\top$, the one corresponding to the second largest eigenvalue that equals to the variance captured in $ \\{y_{n,2}\\}$. \n",
    "\n",
    "The following principal component loading vectors $ \\boldsymbol{\\phi}_3 $, $ \\boldsymbol{\\phi}_4 $, and so forth are the third, fourth, and so forth eigenvectors of the covariance matrix $\\mathbf{X}\\mathbf{X}^\\top$, the ones corresponding to the third, fourth, and so forth largest eigenvalues. The corresponding principal component scores are $ \\{y_{n,3}\\} $, $ \\{y_{n,4}\\} $, and so forth.\n",
    "\n",
    "### Visualising the principal components\n",
    "\n",
    "Once we have computed the principal components, we can plot them against each other in order to produce lower-dimensional views of the data. For instance, we can plot the score vector $ \\{y_{n,1}\\} $ against $ \\{y_{n,2}\\} $, $ \\{y_{n,1}\\} $ against $ \\{y_{n,3}\\} $, $ \\{y_{n,2}\\} $ against $ \\{y_{n,3}\\} $, and so forth. Geometrically, this amounts to projecting the original data down onto the subspace spanned by $ \\boldsymbol{\\phi}_1 $, $ \\boldsymbol{\\phi}_2 $, and $ \\boldsymbol{\\phi}_3 $, and plotting the projected points.\n",
    "\n",
    "```{admonition} Interactive exploration\n",
    ":class: tip\n",
    "You can explore PCA-based visualisation interactively in the [Embedding Projector by TensorFlow](https://projector.tensorflow.org/), noting that the default visualisation algorithm is PCA and you can choose different principal components (up to three) to visualise.\n",
    "```\n",
    "\n",
    "### PCA as the best low-rank approximation\n",
    "\n",
    "The first $M$ principal component score vectors and the first $M$ principal component loading vectors provide the best $M$-dimensional (i.e. rank-$M$) approximation (in terms of Euclidean distance) to the $n\\text{th}$ observation $\\mathbf{x}_{n}$. This representation can be written in terms of the $d\\text{th}$ feature of $\\mathbf{x}_{n}$ as\n",
    "\n",
    "$$\n",
    "x_{n, d} \\approx \\sum_{m=1}^M \\phi_{d, m} y_{n, m}.\n",
    "$$\n",
    "\n",
    "When $M$ is sufficiently large, the $M$ principal component score vectors and $M$ principal component loading vectors can give a good approximation to the data. When $M = \\min(N − 1, D)$, then the representation is exact: $x_{n, d} = \\sum_{m=1}^M \\phi_{d, m} y_{n, m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing crimes in the US with PCA\n",
    "\n",
    "We illustrate the use of PCA on the [USArrests dataset](https://github.com/pykale/transparentML/blob/main/data/USArrests.csv). For each of the 50 states in the United States, the dataset contains the number of arrests per 100,000 residents for each of three crimes: `Assault`, `Murder`, and `Rape`. We also record `UrbanPop` (the percent of the population in each state living in urban areas). The principal component score vectors have length $N = 50$, and the principal component loading vectors have length $D = 4$. PCA was performed after standardizing each variable (feature) to have mean zero and standard deviation one.\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and print the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/USArrests.csv\"\n",
    "\n",
    "USArrests = pd.read_csv(data_url, header=0, index_col=0)\n",
    "USArrests.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the mean and standard deviation of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(USArrests.mean())\n",
    "print(USArrests.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise the data by subtracting the mean and dividing by the standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(\n",
    "    scaler.fit_transform(USArrests), index=USArrests.index, columns=USArrests.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform PCA on the standardised data and print (all) the four principal component loading vectors.\n",
    "<!-- \"\"\" \n",
    "Depends on the version of python/module, you may see a flipped loading vector in signs. \n",
    "This is normal because the orientation of the principal components is not deterministic. \n",
    "\"\"\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loading vectors (i.e. these are the projection of the data onto the principal components)\n",
    "pca_loadings = pd.DataFrame(\n",
    "    PCA().fit(X).components_.T,\n",
    "    index=USArrests.columns,\n",
    "    columns=[\"V1\", \"V2\", \"V3\", \"V4\"],\n",
    ")\n",
    "print(pca_loadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these principal component loading vectors to compute the principal component scores, i.e. transform the data to the principal component space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "df_plot = pd.DataFrame(\n",
    "    pca.fit_transform(X), columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\"], index=X.index\n",
    ")\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first two principal component scores against each other for all 50 states. Also visualise the four principal component loading vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(9, 7))\n",
    "\n",
    "ax1.set_xlim(-3.5, 3.5)\n",
    "ax1.set_ylim(-3.5, 3.5)\n",
    "\n",
    "# plot Principal Components 1 and 2\n",
    "for i in df_plot.index:\n",
    "    ax1.annotate(i, (df_plot.PC1.loc[i], -df_plot.PC2.loc[i]), ha=\"center\")\n",
    "\n",
    "# plot reference lines\n",
    "ax1.hlines(0, -3.5, 3.5, linestyles=\"dotted\", colors=\"grey\")\n",
    "ax1.vlines(0, -3.5, 3.5, linestyles=\"dotted\", colors=\"grey\")\n",
    "\n",
    "ax1.set_xlabel(\"First Principal Component\")\n",
    "ax1.set_ylabel(\"Second Principal Component\")\n",
    "\n",
    "# plot Principal Component loading vectors, using a second y-axis.\n",
    "ax2 = ax1.twinx().twiny()\n",
    "\n",
    "ax2.set_ylim(-1, 1)\n",
    "ax2.set_xlim(-1, 1)\n",
    "ax2.tick_params(axis=\"y\", colors=\"orange\")\n",
    "ax2.set_xlabel(\"Principal Component loading vectors\", color=\"orange\")\n",
    "\n",
    "# plot labels for vectors. Variable 'a' is a small offset parameter to separate arrow tip and text.\n",
    "a = 1.07\n",
    "for i in pca_loadings[[\"V1\", \"V2\"]].index:\n",
    "    ax2.annotate(\n",
    "        i, (pca_loadings.V1.loc[i] * a, -pca_loadings.V2.loc[i] * a), color=\"orange\"\n",
    "    )\n",
    "\n",
    "# plot vectors\n",
    "ax2.arrow(0, 0, pca_loadings.V1[0], -pca_loadings.V2[0])\n",
    "ax2.arrow(0, 0, pca_loadings.V1[1], -pca_loadings.V2[1])\n",
    "ax2.arrow(0, 0, pca_loadings.V1[2], -pca_loadings.V2[2])\n",
    "ax2.arrow(0, 0, pca_loadings.V1[3], -pca_loadings.V2[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we can see that the first loading vector places approximately equal weight on `Assault`, `Murder`, and `Rape`, but with much less weight on `UrbanPop`. Hence this component roughly corresponds to a measure of overall rates of serious crimes. The second loading vector places most of its weight on `UrbanPop` and much less weight on the other three features. Hence, this component roughly corresponds to the level of urbanisation of the state. Overall, we see that the crime-related variables/features ( `Murder`, `Assault`, and `Rape`) are located close to each other, and that the `UrbanPop` variable/feature is far from the other three. This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the `UrbanPop` variable/feature is less correlated with the other three.\n",
    "\n",
    "We can examine differences between the states via the two principal component score vectors shown in the figure above. Our discussion of the loading vectors suggests that states with large positive scores on the first component, such as California, Nevada and Florida, have high crime rates, while states like North Dakota, with negative scores on the first component, have low crime rates. California also has a high score on the second component, indicating a high level of urbanisation, while the opposite is true for states like Mississippi. States close to zero on both components, such as Indiana, have approximately average levels of both crime and urbanisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The proportion of variance explained\n",
    "\n",
    "We can now ask a natural question: how much of the information in a given dataset is lost by projecting the observations onto the first $M$ principal components? That is, how much of the variance in the data is not contained in the first $M$ principal components? More generally, we are interested in knowing the proportion of variance explained (PVE) by each principal component. The total variance present in a dataset (assuming that the variables/features have been centred to have mean zero) is defined as\n",
    "\n",
    "$$\n",
    "\\sum_{d=1}^D \\frac{1}{N} \\sum_{n=1}^N x_{n,d}^2.\n",
    "$$\n",
    "\n",
    "and the variance explained by the $m\\textrm{th}$ principal component is\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{n=1}^N y_{n,m}^2 = \\frac{1}{N}\\sum_{n=1}^N \\left(\\sum_{d=1}^D \\phi_{d,m} x_{n,d}\\right)^2.\n",
    "$$\n",
    "\n",
    "Therefore, the PVE of the $m\\textrm{th}$ principal component is given by\n",
    "\n",
    "```{math}\n",
    ":label: eq:pve\n",
    "\\begin{equation}\n",
    "\\frac{\\sum_{n=1}^N y_{n,m}^2}{\\sum_{d=1}^D\\sum_{n=1}^N  x_{d,n}^2} = \\frac{\\sum_{n=1}^N \\left(\\sum_{d=1}^D \\phi_{d,m} x_{n,d}\\right)^2}{\\sum_{d=1}^D\\sum_{n=1}^N  x_{n,d}^2}.\n",
    "\\end{equation}\n",
    "```\n",
    "\n",
    "The PVE of each principal component is a positive quantity. In order to compute the cumulative PVE of the first $M$ principal components, we can simply sum Equation {eq}`eq:pve` over each of the first $M$ PVEs. In total, there are (at most) $ \\min (N − 1, D) $ principal components, and their PVEs sum to one. \n",
    "\n",
    "In `scikit-learn`, the proportion of explained variance is available in the `PCA` object to help select the number of PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Interpreting the proportion of variance explained\n",
    ":class: tip\n",
    "From the above, in the `USArrests` data, the first principal component explains 62.0 % of the variance in the data, and the next principal component explains 24.7 % of the variance. Together, the first two principal components explain almost 87 % of the variance in the data, and the last two principal components explain only 13 % of the variance.\n",
    "```\n",
    "\n",
    "Let us plot the PVE of each principal component, as well as the cumulative PVE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "plt.plot(\n",
    "    [1, 2, 3, 4], pca.explained_variance_ratio_, \"-o\", label=\"Individual component\"\n",
    ")\n",
    "plt.plot(\n",
    "    [1, 2, 3, 4], np.cumsum(pca.explained_variance_ratio_), \"-s\", label=\"Cumulative\"\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Proportion of Variance Explained\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.xlim(0.75, 4.25)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, if we want to preserve at least 80% of variance of the data, we need to select 2 PCs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on toy data and system transparency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we adapt the example [\"Principal Component Regression vs Partial Least Squares Regression\"](https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html#sphx-glr-auto-examples-cross-decomposition-plot-pcr-vs-pls-py) from `scikit-learn` to explain the system transparency of PCA. \n",
    "\n",
    "We generate synthetic data, perform PCA, and then plot the first two principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "n_samples = 500\n",
    "cov = [[3, 3], [3, 4]]\n",
    "X = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 8))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.3, label=\"samples\")\n",
    "\n",
    "for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n",
    "    # comp = comp * var  # scale component by its variance explanation power\n",
    "    plt.arrow(\n",
    "        x=0,\n",
    "        y=0,\n",
    "        dx=comp[0],\n",
    "        dy=comp[1],\n",
    "        color=f\"C{i + 1}\",\n",
    "        width=0.1,\n",
    "        label=f\"Principal component {i+1}\",\n",
    "        head_width=0.3,\n",
    "    )\n",
    "\n",
    "plt.arrow(\n",
    "    x=0,\n",
    "    y=0,\n",
    "    dx=X[1, 0] * 0.85,\n",
    "    dy=X[1, 1] * 0.85,\n",
    "    color=\"k\",\n",
    "    width=0.1,\n",
    "    head_width=0.3,\n",
    "    label=\"Input Data Sample\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X[1, 0],\n",
    "    X[1, 1],\n",
    "    alpha=0.8,\n",
    "    facecolors=\"none\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"r\",\n",
    "    linewidths=3,\n",
    ")\n",
    "\n",
    "plt.gca().set(\n",
    "    aspect=\"equal\",\n",
    "    title=\"2-dimensional dataset with principal components\",\n",
    "    xlabel=\"first feature\",\n",
    "    ylabel=\"second feature\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine the numerical values of a particular data point (circled in red), its principal component scores, the mean of the training data, the principal component loading vectors, and their explained variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The numbers display below round off to 2 decimal places.\")\n",
    "print(\"Data point: \", np.around(X[1, :], 2))\n",
    "print(\n",
    "    \"Scores of the data point: \", np.around(pca.transform(X[1, :].reshape(1, -1))[0], 2)\n",
    ")\n",
    "print(\"Mean of the training data: \", np.around(pca.mean_, 2))\n",
    "print(\"PCA loadings: \\n\", np.around(pca.components_, 2))\n",
    "print(\"Explained variance: \", np.around(pca.explained_variance_, 2))\n",
    "print(\"Explained variance ratio: \", np.around(pca.explained_variance_ratio_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} System transparency\n",
    ":class: important\n",
    "\n",
    "- For data point $ [-2.78 \\;\\; -0.93]^{\\top} $, the score vector of PCA can be obtained by\n",
    "\n",
    "$$ \\underbrace{\\left[\\begin{matrix} -0.64 & -0.77 \\\\ 0.77 & -0.64 \\end{matrix}\\right]}_{\\text{Loading}}\\left(\\underbrace{\\left[\\begin{matrix} -2.78 \\\\ -0.93 \\end{matrix}\\right]}_{\\text{Input Data}} - \\underbrace{\\left[\\begin{matrix}0.12 \\\\ 0.12 \\end{matrix}\\right]}_{\\text{Mean}}\\right) = \\underbrace{\\left[\\begin{matrix} 2.67 \\\\ -1.54 \\end{matrix}\\right].}_{\\text{Scores}} $$\n",
    "\n",
    "- The above process can be used inversely to obtain the input data from the scores by times the transpose (i.e. the inverse due to orthonormality) of the loading matrix ($\\left[\\begin{matrix} -0.64 & -0.77   \\\\ 0.77 &  -0.64  \\end{matrix}\\right]^{\\top}$) on both sides and move the mean to the other side:\n",
    "\n",
    "$$ \n",
    "\\underbrace{\\left[\\begin{matrix} -0.64 & -0.77   \\\\ 0.77 &  -0.64  \\end{matrix}\\right]^{\\top}}_{\\text{Loading}} \\underbrace{\\left[\\begin{matrix} 2.67 \\\\ -1.54 \\end{matrix}\\right]}_{\\text{Scores}} + \\underbrace{\\left[\\begin{matrix}0.12 \\\\ 0.12 \\end{matrix}\\right]}_{\\text{Mean}} = \\underbrace{\\left[\\begin{matrix} -2.78 \\\\ -0.93 \\end{matrix}\\right].}_{\\text{Reconstructed Input}}\n",
    "$$\n",
    "\n",
    "The reconstruction can also be interpreted from the geometric perspective. As displayed in the figure above, the input data point $ [-2.78 \\;\\; -0.93]^{\\top} $ (the black vector) can be viewed as the linear combination of the two loading vectors (PC 1 and PC 2 in orange and green, respectively). The scores of the input data point are the coefficients of the linear combination. The reconstructed input data point is the linear combination of the scores and the loading vectors:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\left[\\begin{matrix} -0.64 \\\\ -0.77 \\end{matrix}\\right]}_{\\text{PC 1}} \\times \\underbrace{2.67}_{\\text{PC1 Score}} + \\underbrace{\\left[\\begin{matrix} 0.77 \\\\ -0.64 \\end{matrix}\\right]}_{\\text{PC 2}} \\times \\underbrace{(-1.54)}_{\\text{PC 2 Score}} + \\underbrace{\\left[\\begin{matrix}0.12 \\\\ 0.12 \\end{matrix}\\right]}_{\\text{Mean}} = \\underbrace{\\left[\\begin{matrix} -2.78 \\\\ -0.93 \\end{matrix}\\right].}_{\\text{Reconstructed Input}}\n",
    "$$\n",
    "\n",
    "Thus, given any point in the output PCA space, we can reconstruct the input data point by the above process. \n",
    "```\n",
    "\n",
    "Note: numbers are limited to two decimal places for the sake of clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. Load the **mice protein dataset** from **sklearn.datasets** using **fetch_openml** where the dataset contains **gene expressions** of **mice brains** and show the first 5 rows of the data. Hint: See [here](https://scikit-learn.org/stable/datasets/loading_other_datasets.html#:~:text=gene%20expressions%20in%20mice%20brains) for further help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"miceprotein\")\n",
    "df = mnist.data.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Using StandardScaler, standardise the loaded mice protein dataset from **Exercise 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Now, run **PCA** from **sklearn** explaining at least $95\\%$ variance on the **standardised mice protein dataset** and display the **top ten eigenvalues**. Hint: Each eigenvalue represents the captured/explained variance in the direction of the respective eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X)\n",
    "\n",
    "print(\n",
    "    \"The first 10 explained variance/eigenvalues are: \\n\", pca.explained_variance_[:10]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Using the fitted PCA model in **Exercise 3**, find out the **explained variance ratio** for the top 10 principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The first 10 explained variance ratio are: \\n\", pca.explained_variance_ratio_[:10]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** After observing the **explained variance ratio** from **Exercise 4** carefully, find out how many **principal components** should we take to preserve $70\\%$ of variance of the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**The first 5 principal components.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.** Finally, using the top 10 principal components, plot the **PVE** of each principal component and the **cumulative PVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_[:10], \"-o\", label=\"Individual component\")\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_[:10]), \"-s\", label=\"Cumulative\")\n",
    "\n",
    "plt.ylabel(\"Proportion of Variance Explained\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
