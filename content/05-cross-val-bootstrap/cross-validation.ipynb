{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "[Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is a resampling method that is commonly used to evaluate the performance of a machine learning model and subsequently select the appropriate level of flexibility of the given model or the best model from a set of candidate models. The process of evaluating a modelâ€™s performance is known as _model assessment_, whereas the process of selecting the proper level of flexibility for a model is known as [model selection](https://en.wikipedia.org/wiki/Model_selection).\n",
    "\n",
    "One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.\n",
    "\n",
    "For example, a machine learning model may be evaluated by splitting the available data into a training set and a test set. The model is then trained on the training set, and its performance is measured on the test set. However, the evaluation procedure can have a high variance, depending on exactly which observations are included in the training set and which observations are included in the test set. Cross-validation remedies this problem by averaging the results over several complementary test sets and training sets. The method is called cross-validation because it involves training and testing the method on different subsets of the original data, where the subsets are different from each other in that the training sets are disjoint from the test sets. The method is also called _rotation estimation_ because it can be thought of as a form of rotation among the available data.\n",
    " \n",
    "Watch the 6-minute video below for a visual explanation of cross-validation:\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/fSytzGwwBVw?start=15\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining Cross Validation by StatQuest](https://www.youtube.com/embed/fSytzGwwBVw?start=15), embedded according to [YouTube's Terms of Service](https://www.youtube.com/static?gl=CA&template=terms).\n",
    "```\n",
    "\n",
    "## Import libraries and load data\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    LeaveOneOut,\n",
    "    KFold,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will study cross-validation on the [Auto dataset](https://github.com/pykale/transparentML/blob/main/data/Auto.csv) (click to explore). Load this dataset and inspect its structure (columns). Note here we remove entries with missing values using the [`dropna()` function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) from the `pandas` library, which is a typical [data cleaning](https://en.wikipedia.org/wiki/Data_cleansing) step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_url = \"https://github.com/pykale/transparentML/raw/main/data/Auto.csv\"\n",
    "\n",
    "auto_df = pd.read_csv(auto_url, na_values=\"?\").dropna()\n",
    "auto_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, it is a good practice to inspect the first few rows before proceeding to any analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set approach\n",
    "\n",
    "<!-- Using [Polynomial](http://scikit-learn.org/dev/modules/preprocessing.html#generating-polynomial-features) feature generation in scikit-learn -->\n",
    "The validation set approach is a strategy to estimate the _test error_ associated with fitting a particular model on a set of observations. It involves randomly dividing the available set of observations into two parts, a _training set_ and a _validation set_ or _hold-out set_. The model is fit on the training set, and the fitted model is used to predict the responses for the \"unseen\" (hold-out) observations in the validation set. The resulting validation set error rate, e.g. mean squared error (MSE) for a quantitative response, provides an _estimate_ of the test error rate.\n",
    "\n",
    "In the following example, we study linear regression on the [Auto dataset](https://github.com/pykale/transparentML/blob/main/data/Auto.csv). In [Linear regression](https://pykale.github.io/transparentML/02-linear-reg/extension-limitation.html#nonlinear-relationships), we discovered a [nonlinear relationship](https://pykale.github.io/transparentML/02-linear-reg/extension-limitation.html#nonlinear-relationships) between `mpg` and `horsepower`. Compared to using only a linear term, a model that using `horsepower` and `horsepower`$^2$ gives better results in predicts `mpg`. Can we tell what kind of nonlinear relationship between `mpg` and `horsepower` will give the best prediction performance?\n",
    "\n",
    "We will use the validation set approach to estimate the test errors associated with different degrees (1 to 10) of polynomial expansion capturing different nonlinear relationships. We can randomly split the 392 observations of the `Auto` dataset into two sets, a training set containing 196 of the data points for model training, and a validation set containing the remaining 196 observations for evaluating by MSE. We will use the [`train_test_split()` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from the `sklearn.model_selection` library to randomly split the data into two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prop = 0.5\n",
    "p_order = np.arange(1, 11)\n",
    "r_state = np.arange(0, 10)\n",
    "\n",
    "X, Y = np.meshgrid(p_order, r_state, indexing=\"ij\")\n",
    "Z = np.zeros((p_order.size, r_state.size))\n",
    "\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Generate 10 random splits of the dataset\n",
    "for (i, j), v in np.ndenumerate(Z):\n",
    "    poly = PolynomialFeatures(int(X[i, j]))\n",
    "    X_poly = poly.fit_transform(auto_df.horsepower.values.reshape(-1, 1))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_poly, auto_df.mpg.ravel(), test_size=t_prop, random_state=Y[i, j]\n",
    "    )\n",
    "\n",
    "    regr.fit(X_train, y_train)\n",
    "    pred = regr.predict(X_test)\n",
    "    Z[i, j] = mean_squared_error(y_test, pred)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Left plot (first split)\n",
    "ax1.plot(X.T[0], Z.T[0], \"-o\")\n",
    "ax1.set_title(\"Random split of the dataset\")\n",
    "\n",
    "# Right plot (all splits)\n",
    "ax2.plot(X, Z)\n",
    "ax2.set_title(\"10 random splits of the dataset\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_ylabel(\"Mean Squared Error\")\n",
    "    ax.set_ylim(15, 30)\n",
    "    ax.set_xlabel(\"Degree of Polynomial\")\n",
    "    ax.set_xlim(0.5, 10.5)\n",
    "    ax.set_xticks(range(2, 11, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left-hand panel of the figure above shows the results on one possible split of the data into a training set and a validation set using the polynomial regression model of different degrees. The training set is used to fit the model, and the validation set is used to evaluate the model fit. The validation set error rate is an estimate of the test error rate. The validation set error is lowest when the model is a function of `horsepower`$^6$ in this case. Is this really/always the case?\n",
    "\n",
    "If we repeat the process of randomly splitting, we will get a somewhat different estimate for the test MSE. Ten different validation set MSE curves are shown in the right-hand panel of the figure above.\n",
    "<!-- As an illustration, the right-hand panel of Figure 5.2 displays ten different validation set MSE curves from the Auto dataset, produced using ten different random splits of the observations into training and validation sets.  -->\n",
    "We can observe:\n",
    "- model with a quadratic term has a dramatically smaller validation set MSE than the model with only a linear term\n",
    "- not much benefit in including cubic or higher-order polynomial terms in the model\n",
    "- each of the ten curves results in a different test MSE estimate for each of the ten regression models considered \n",
    "- there is no consensus among the curves as to which model results in the smallest validation set MSE. \n",
    "\n",
    "Based on the variability among these curves, all that we can conclude with any confidence is that the linear fit is not adequate for this data. The validation set approach is conceptually simple and is easy to implement. But it has two drawbacks:\n",
    "\n",
    "1. As is shown in the right-hand panel of the Figure above, the validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\n",
    "2. In the validation approach, only a subset of the observationsâ€”those that are included in the training set rather than in the validation setâ€”are used to fit the model. Since machine learning models tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to _overestimate_ the test error rate for the model fit on the entire dataset.\n",
    "   \n",
    "Cross-validation below is a refinement of the validation set approach that addresses these two issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out cross-validation\n",
    "\n",
    "<!-- Leave-one-out cross-validation (LOOCV) is closely related to the validation set approach of Section 5.1.1, but it attempts to address that methodâ€™s drawbacks. -->\n",
    "\n",
    "[Leave-one-out cross-validation (LOOCV)](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation) is closely related to the validation set approach. It also splits the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation $(\\mathbf{x}_i , y_i)$, where $ i = 1, 2, \\cdots, N $, is used for the validation set, and the remaining observations $\\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\cdots, (\\mathbf{x}_{i-1}, y_{i-1}), (\\mathbf{x}_{i+1}, y_{i+1}), \\cdots, (\\mathbf{x}_N , y_N)\\}$ make up the training set. The machine learning model is fit on the $ N âˆ’ 1 $ training observations, and a prediction $\\hat{y}_i$ is made for the excluded observation, using its value $\\mathbf{x}_i$. \n",
    "Since $(\\mathbf{x}_i , y_i)$ was not used in the fitting process, the squared test error $ \\epsilon_i = (y_i âˆ’ \\hat{y}_i)^2 $ provides an approximately unbiased estimate. \n",
    "<!-- But even though MSE 1 is unbiased for the test error, it is a poor estimate because it is highly variable, since it is based upon a single observation (x 1 , y 1 ). -->\n",
    "\n",
    "We can repeat the procedure by iterating $i$ from $1$ to $N$ to produce $N$ squared errors, $ \\epsilon_i, \\dots, \\epsilon_N $. The LOOCV estimate for the test MSE is the average of these $N$ test error estimates:\n",
    "\n",
    "$$\\textrm{MSE}_{LOOCV} = \\frac{1}{N}\\sum_{i=1}^N \\epsilon_i = \\frac{1}{N}\\sum_{i=1}^N (y_i âˆ’ \\hat{y}_i)^2 $$\n",
    "\n",
    "\n",
    "Compared to the validation set approach, LOOCV has the following advantages:\n",
    "- LOOCV has far less bias. In LOOCV, we repeatedly fit the machine learning model using training sets that contain $N âˆ’ 1$ observations, almost as many as are in the entire dataset. This is in contrast to the validation set approach, in which the training set is typically around half the size of the original dataset. Consequently, the LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does. \n",
    "- In contrast to the validation set approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will _always yield the same results_: there is _no randomness_ in the training/validation set splits.\n",
    "\n",
    "\n",
    "## $K$-fold cross-validation\n",
    "\n",
    "An alternative to LOOCV is $K$-fold CV. This approach involves randomly dividing the set of observations into $K$ groups, or folds, of approximately equal size. Then repeating the following procedure by iterating $k$ from $1$ to $K$:\n",
    "\n",
    "1. Treating $k\\text{th}$ group of observations (fold) as a validation set,\n",
    "2. Fitting model on the remaining $K âˆ’ 1$ folds,\n",
    "3. Computing the mean squared error, $\\textrm{MSE}_k$\n",
    "\n",
    "This process results in $K$ estimates of the test error, $\\textrm{MSE}_1, \\textrm{MSE}_2, \\dots, \\textrm{MSE}_K$ . The $K$-fold CV estimate is computed by averaging these values, \n",
    "\n",
    "LOOCV can be viewed as a special case of $K$-fold CV in which $K = N$. In practice, one typically performs $K$-fold CV using $K = 5$ or $K = 10$. The most obvious advantage is computational. LOOCV requires fitting the machine learning model $N$ times. This has the potential to be computationally expensive so may be feasible only for computationally efficient models. But cross-validation is a very general approach that can be applied to almost any machine learning model. Some machine learning models have computationally intensive fitting procedures, and so performing LOOCV may pose computational problems, especially if $N$ is extremely large. In contrast, performing 10-fold CV requires fitting the learning procedure only ten times, which may be much more feasible.\n",
    "\n",
    "Let us perform LOOCV on the [Auto dataset](https://github.com/pykale/transparentML/blob/main/data/Auto.csv). We will use the `sklearn.model_selection` module to perform the cross-validation. The [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function performs cross-validation for a given machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_order = np.arange(1, 11)\n",
    "r_state = np.arange(0, 10)\n",
    "\n",
    "# LeaveOneOut CV\n",
    "regr = LinearRegression()\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(auto_df)\n",
    "scores = list()\n",
    "\n",
    "for i in p_order:\n",
    "    poly = PolynomialFeatures(i)\n",
    "    X_poly = poly.fit_transform(auto_df.horsepower.values.reshape(-1, 1))\n",
    "    score = cross_val_score(\n",
    "        regr, X_poly, auto_df.mpg, cv=loo, scoring=\"neg_mean_squared_error\"\n",
    "    ).mean()\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, perform a 10-fold cross-validation (CV) using the same `cross_val_score` function from the `sklearn.model_selection` module, but this time using the `cv=10` argument instead of `cv=loo` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $10$-fold CV\n",
    "folds = 10\n",
    "elements = len(auto_df.index)\n",
    "\n",
    "X, Y = np.meshgrid(p_order, r_state, indexing=\"ij\")\n",
    "Z = np.zeros((p_order.size, r_state.size))\n",
    "\n",
    "regr = LinearRegression()\n",
    "\n",
    "for (i, j), v in np.ndenumerate(Z):\n",
    "    poly = PolynomialFeatures(X[i, j])\n",
    "    X_poly = poly.fit_transform(auto_df.horsepower.values.reshape(-1, 1))\n",
    "    kf_10 = KFold(n_splits=folds, random_state=Y[i, j], shuffle=True)\n",
    "    Z[i, j] = cross_val_score(\n",
    "        regr, X_poly, auto_df.mpg, cv=kf_10, scoring=\"neg_mean_squared_error\"\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the results of the LOOCV and 10-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Note: cross_val_score() method return negative values for the scores.\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "\n",
    "# Left plot\n",
    "ax1.plot(p_order, np.array(scores) * -1, \"-o\")\n",
    "ax1.set_title(\"LOOCV\")\n",
    "\n",
    "# Right plot\n",
    "ax2.plot(X, Z * -1)\n",
    "ax2.set_title(\"10-fold CV\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_ylabel(\"Mean Squared Error\")\n",
    "    ax.set_ylim(15, 30)\n",
    "    ax.set_xlabel(\"Degree of Polynomial\")\n",
    "    ax.set_xlim(0.5, 10.5)\n",
    "    ax.set_xticks(range(2, 11, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of LOOCV and 10-fold CV are shown in the left and right panels, respectively. As we can see from the figure, there is some variability in the 10-fold CV estimates as a result of the variability in how the observations are divided into ten folds. But this variability is typically much lower than the variability in the test error estimates that results from the validation set approach.\n",
    "\n",
    "The objective of performing cross-validation can be different:\n",
    "- For model evaluation, our goal might be to determine how well a given machine learning model can be expected to perform on independent data; in this case, the actual estimate of the test MSE is of interest. \n",
    "- For model selection, we are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of machine learning models, or on a single model using different levels of flexibility (typically different hyper-parameters), in order to identify the model that results in the lowest test error. For this purpose, the location of the minimum point in the estimated test MSE curve is important, but the actual value of the estimated test MSE is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore further by learning from the [cross-validation strategies in scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. Explain how **$K$-fold cross-validation** is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**In $K$-fold cross validation, the original set of observations are randomly divided into $K$ equal sized folds (groups). Of the $K$ folds, a single fold is retained as the validation data for testing the model, and the remaining $K âˆ’ 1$ folds are used as training data. The cross-validation process is then repeated $K$ times, with each of the $K$ folds used exactly once as the validation data. The $K$ results can then be averaged to produce a single estimation.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. In **Chapter 3**, we used logistic regression on the [Default](https://github.com/pykale/transparentML/blob/main/data/Default.csv) dataset to predict the probability of someone defaulting on a loan, by looking at their income and balance. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis and convert the categories to numerical values.\n",
    "\n",
    "**a**. Fit a logistic regression model that uses **income** and **balance** to predict **default** and uses the **validation set approach** to estimate the **test error** of this model. In order to do this, you must perform the following steps:\n",
    "\n",
    "**i**. Split the sample set into a training set and a validation set.\n",
    "\n",
    "**ii**. Fit a multiple logistic regression model using only the training observations.\n",
    "\n",
    "**iii**. Obtain a prediction of default status for each individual in the validation set.\n",
    "\n",
    "**iv**. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.\n",
    "**Hint**: See section [5.1.2](https://pykale.github.io/transparentML/05-cross-val-bootstrap/cross-validation.html#validation-set-approach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "default_df = pd.read_csv(\n",
    "    \"https://github.com/pykale/transparentML/raw/main/data/Default.csv\"\n",
    ")\n",
    "\n",
    "# Check for missing\n",
    "assert default_df.isna().sum().sum() == 0\n",
    "\n",
    "# converting categories\n",
    "default_df[\"default\"] = default_df.default.factorize()[0]\n",
    "default_df[\"student\"] = default_df.student.factorize()[0]\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "X = default_df[[\"income\", \"balance\"]].values\n",
    "Y = default_df[\"default\"]\n",
    "\n",
    "# train test spliting 50:50\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y.ravel(), test_size=0.5)\n",
    "\n",
    "# Logistic regression\n",
    "logit = LogisticRegression()\n",
    "model_logit = logit.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model_logit.predict(X_test)\n",
    "\n",
    "total_error_rate_pct = mean_squared_error(y_test, y_pred) * 100\n",
    "print(\"total_error_rate: {}%\".format(total_error_rate_pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Repeat the process in **Exercise 2(a)** three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained. **Hint**: Try using different **random seed** values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "for s in range(1, 4):\n",
    "    print(\"Random seed = {}\".format(s))\n",
    "    np.random.seed(s)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y.ravel(), test_size=0.5)\n",
    "\n",
    "    # Logistic regression\n",
    "    logit = LogisticRegression()\n",
    "    model_logit = logit.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model_logit.predict(X_test)\n",
    "\n",
    "    total_error_rate_pct = mean_squared_error(y_test, y_pred) * 100\n",
    "    print(\"total_error_rate: {}%\".format(total_error_rate_pct))\n",
    "\n",
    "# For different seed value we gets different error rate as our train_test_split is random. We can use random state parameter inside the train_test)split also to control the randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. Again, consider a logistic regression model that predicts the probability of default using **income**, **balance**, and a **dummy variable** for student. Estimate the test error for this model using the **validation set approach**. Comment on whether or not including a dummy variable for student leads to a **reduction** in the **test error rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "X = default_df[[\"income\", \"balance\", \"student\"]].values\n",
    "Y = default_df[\"default\"]\n",
    "\n",
    "for s in range(1, 4):\n",
    "    print(\"Random seed = {}\".format(s))\n",
    "    np.random.seed(s)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y.ravel(), test_size=0.5)\n",
    "\n",
    "    # Logistic regression\n",
    "    logit = LogisticRegression()\n",
    "    model_logit = logit.fit(X_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = model_logit.predict(X_test)\n",
    "    total_error_rate_pct = mean_squared_error(y_test, y_pred) * 100\n",
    "    print(\"total_error_rate: {}%\".format(total_error_rate_pct))\n",
    "\n",
    "\n",
    "# Including a dummy variable for student didn't leads to a reduction in the test error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d**. Fit a logistic regression model that uses **income** and **balance** to predict **default** by performing **leave-one-out** cross-validation and **$5$-fold** cross-validation to estimate the test error. For simplicity, use the **first $1000$ samples** from the dataset to perform this experiment. **Hint**: See section [5.1.3](https://pykale.github.io/transparentML/05-cross-val-bootstrap/cross-validation.html#leave-one-out-cross-validation) and [5.1.4](https://pykale.github.io/transparentML/05-cross-val-bootstrap/cross-validation.html#k-fold-cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (\n",
    "    LeaveOneOut,\n",
    "    KFold,\n",
    "    cross_val_score,\n",
    ")\n",
    "\n",
    "df = default_df[:1000]\n",
    "\n",
    "X = df[[\"income\", \"balance\"]].values\n",
    "Y = df[\"default\"]\n",
    "\n",
    "regr = LogisticRegression()\n",
    "# LeaveOneOut CV\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(default_df)\n",
    "\n",
    "loocv_score = cross_val_score(\n",
    "    regr, X, Y, cv=loo, scoring=\"neg_mean_squared_error\"\n",
    ").mean()\n",
    "\n",
    "# 5-fold CV\n",
    "folds = 5\n",
    "kf_5 = KFold(n_splits=folds, shuffle=True)\n",
    "kf_score = cross_val_score(regr, X, Y, cv=kf_5, scoring=\"neg_mean_squared_error\").mean()\n",
    "print(\"total_error_rate for loocv: {}%\".format(-1 * loocv_score * 100))\n",
    "print(\"total_error_rate for 5-Fold: {}%\".format(-1 * kf_score * 100))\n",
    "\n",
    "# Test error for loocv is lower then the 5-fold cross validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
