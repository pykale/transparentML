{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions/limitations of linear regression\n",
    "\n",
    "```{admonition} Read then Launch \n",
    "This content is best viewed in html because jupyter notebook cannot display some content (e.g. figures, equations) properly. You should finish reading this page first and then launch it as an interactive notebook in Google Colab (faster, Google account needed) or Binder by clicking the rocket symbol (<i class=\"fas fa-rocket\"></i>) at the top.\n",
    "```\n",
    "\n",
    "This section studies how to extend linear regression to more complicated types of variables and relationships, and the limitations of linear regression. We will use three datasets to illustrate these concepts:\n",
    "\n",
    "1. The [Advertising dataset](https://github.com/pykale/transparentML/blob/main/data/Advertising.csv) that has been used in the previous sections. \n",
    "2. The [Credit dataset](https://github.com/pykale/transparentML/blob/main/data/Credit.csv) contains information about credit card holders. The goal is to predict the credit `Limit` for each card holder. \n",
    "3. The [Auto dataset](https://github.com/pykale/transparentML/blob/main/data/Auto.csv) dataset contains information about the characteristics of cars. The goal is to predict the `mpg` (miles per gallon) for a car based on its features like `horsepower`, `weight` and `acceleration`. This dataset has also been used in the tutorial of {doc}`Prerequisites <../00-prereq/overview>`.\n",
    "\n",
    "You are encouraged to click the links to the datasets and explore the variables and understand their relationships.\n",
    "\n",
    "## Import libraries and load data\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [Advertising dataset](https://github.com/pykale/transparentML/blob/main/data/Advertising.csv) dataset that we are already familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Advertising.csv\"\n",
    "advertising_df = pd.read_csv(data_url, header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [Credit dataset](https://github.com/pykale/transparentML/blob/main/data/Credit.csv) dataset, convert the `Student2` category to numbers ('0' and '1'), and inspect the first three rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_url = \"https://github.com/pykale/transparentML/raw/main/data/Credit.csv\"\n",
    "\n",
    "credit_df = pd.read_csv(credit_url)\n",
    "credit_df[\"Student2\"] = credit_df.Student.map({\"No\": 0, \"Yes\": 1})\n",
    "credit_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [Auto dataset](https://github.com/pykale/transparentML/blob/main/data/Auto.csv) datasets, ignore rows with missing values, and inspect the columns, counts and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_url = \"https://github.com/pykale/transparentML/raw/main/data/Auto.csv\"\n",
    "\n",
    "auto_df = pd.read_csv(auto_url, na_values=\"?\").dropna()\n",
    "auto_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative predictors\n",
    "\n",
    "In the previous sections, the predictor variables are *quantitative*. Here, in the `Credit` dataset, the response is `Balance` (average credit card debt for each individual) and there are six quantitative predictors: `Age` , `Cards` (number of credit cards), `Education` (years of education), `Income` (in thousands of dollars), `Limit` (credit limit), and `Rating` (credit rating). The following code displays the pairwise relationships between these variables (Figure 3.6 in the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    credit_df[[\"Balance\", \"Age\", \"Cards\", \"Education\", \"Income\", \"Limit\", \"Rating\"]]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in practice, predictor variables are not always quantitative. For example, in the `Credit` dataset, it also contains four _qualitative_ (categorical) variables: `Student` (`Yes` or `No`), `Own` (`Yes` or `No`), `Married` (`Yes` or `No`), and `Region` (`South`, `East`, or `West`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictors with only two levels**\n",
    "\n",
    "If a qualitative predictor has only two levels (possible values), then we can treat it as a quantitative variable, known as an indicator or _dummy_ variable. For example, the `Student` variable has two levels: `Yes` and `No`. We can convert these levels to numbers, such as `1` and `0`, and then treat the `Student` variable as a quantitative variable. This is called *coding* the qualitative variable, which is automatically done by the `statsmodels` library. \n",
    "\n",
    "Run the following code to get the least squares coefficient estimates associated with the regression of `Balance` onto `Own` in the `Credit` dataset (Table 3.7 in the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Balance ~ Own\", credit_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large $p$ value above shows that there is no relationship between `Balance` and `Own`. \n",
    "\n",
    "**Predictors with more than two levels**\n",
    "\n",
    "When a qualitative predictor has more than two levels, we need to create additional dummy variables. For example, the `Region` variable has three levels: `South`, `East`, and `West`. We can create **two** dummy variables, `South`, and `West`, and then use these variables in the regression model. There will be always _one fewer dummy variable than the number of levels_ in the qualitative variable. This is because the regression model can determine the value of the missing dummy variable. For example, if `South` is `0` and `West` is `0`, then the individual must be from the `East` region. Again, such _coding_ is automatically done by the `statsmodels` library.\n",
    "\n",
    "Run the following code to get the least squares coefficient estimates associated with the regression of `Balance` onto `Region` in the `Credit` dataset (Table 3.8 in the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Balance ~ Region\", credit_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large $p$-values above show that there is no relationship between `Balance` and `Region`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions of linear regression\n",
    "\n",
    "### Interaction between variables\n",
    "\n",
    "In the previous analysis of `Advertising` dataset, the predictor variables are assumed to be independent. However, this model may be incorrect. For example, `radio` advertising can increase the effectiveness of `TV` advertising, which is known as *interaction* effect in statistics. Consider the standard multiple linear regression with two variables\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon.\n",
    "$$\n",
    "\n",
    "This model can be extended to include an interaction term between $x_1$ and $x_2$:\n",
    "\n",
    "\\begin{align}\n",
    "y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon \\\\\n",
    "&= \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon \\\\\n",
    "&= \\beta_0 + \\tilde{\\beta}_1 x_1 + \\beta_2 x_2 + \\epsilon,\n",
    "\\end{align}\n",
    "\n",
    "where $\\tilde{\\beta}_1 = \\beta_1 + \\beta_3 x_2$ is the *effective* coefficient on $x_1$, which can be viewed as _another linear regression_ that regresses $\\tilde{\\beta}_1$ onto $x_2$. Then the association between $y$ and $x_1$ is no longer constant and depends on the value of $x_2$. Similarly, the association between $y$ and $x_2$ is no longer constant and depends on the value of $x_1$.\n",
    "\n",
    "Run the following code for the `Advertising` data to get the least squares coefficient estimates associated with the regression of `Sales` onto `TV` and `Radio`, with an interaction term, `TV` $\\times$ `Radio` (Table 3.9 of the text book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ TV + Radio + TV*Radio\", advertising_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small $p$-value for the interaction term suggests that the true relationship is not additive and there is strong evidence of the association between `Sales` and the interaction between `TV` and `Radio`. Without including the interaction term, our understanding of the relationship between `Sales` and `TV` and `Radio` is not complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interaction between qualitative and quantitative variables\n",
    "\n",
    "The concept of interactions also applies to qualitative variables. In fact, sometimes an interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation. For example, run the following code to predict `Balance` using the `Income` (quantitative) and `Student` (qualitative) variables, and compare the differences between including and excluding the interaction term between `Income` and `Student`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1 = ols(\"Balance ~ Income + Student2\", credit_df).fit()\n",
    "est2 = ols(\"Balance ~ Income + Income*Student2\", credit_df).fit()\n",
    "\n",
    "print(\"Regression 1 - without interaction term\")\n",
    "print(est1.params)\n",
    "print(\"\\nRegression 2 - with interaction term\")\n",
    "print(est2.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the interaction term between `Income` and `Student` affects the relationship between `Balance` and `Student` more than the relationship between `Balance` and `Income`. To have a closer look, let us visualise the relationship between `Balance` and `Income` for students and non-students separately (Figure 3.7 of the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income (x-axis)\n",
    "regr1 = est1.params\n",
    "regr2 = est2.params\n",
    "\n",
    "income = np.linspace(0, 150)\n",
    "\n",
    "# Balance without interaction term (y-axis)\n",
    "student1 = np.linspace(\n",
    "    regr1[\"Intercept\"] + regr1[\"Student2\"],\n",
    "    regr1[\"Intercept\"] + regr1[\"Student2\"] + 150 * regr1[\"Income\"],\n",
    ")\n",
    "non_student1 = np.linspace(\n",
    "    regr1[\"Intercept\"], regr1[\"Intercept\"] + 150 * regr1[\"Income\"]\n",
    ")\n",
    "\n",
    "# Balance with interaction term (y-axis)\n",
    "student2 = np.linspace(\n",
    "    regr2[\"Intercept\"] + regr2[\"Student2\"],\n",
    "    regr2[\"Intercept\"]\n",
    "    + regr2[\"Student2\"]\n",
    "    + 150 * (regr2[\"Income\"] + regr2[\"Income:Student2\"]),\n",
    ")\n",
    "non_student2 = np.linspace(\n",
    "    regr2[\"Intercept\"], regr2[\"Intercept\"] + 150 * regr2[\"Income\"]\n",
    ")\n",
    "\n",
    "# Create plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1.plot(income, student1, \"r-\", income, non_student1, \"k-\")\n",
    "ax2.plot(income, student2, \"r--\", income, non_student2, \"k--\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.legend([\"student\", \"non-student\"], loc=2)\n",
    "    ax.set_xlabel(\"Income\")\n",
    "    ax.set_ylabel(\"Balance\")\n",
    "    ax.set_ylim(ymax=1550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left figure without the interaction term between `Income` and `Student`, we see two (solid) parallel lines, one for students and one for non-students. The two lines have different intercepts but the same slope, indicating that a one-unit increase in `Income` will have the same effect on `Balance` for students and non-students, which is counterintuitive. \n",
    "\n",
    "In the right figure with the interaction term between `Income` and `Student`, the two (dashed) lines are no longer parallel. We see that the slope of the line for students is flatter than that for non-students, indicating that a one-unit increase in `Income` will have a much smaller effect on credit card `Balance` for students than for non-students. This is consistent with our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear relationships\n",
    "\n",
    "In some cases, the true relationship between the response and the predictors may be nonlinear. Here we present a very simple way to directly extend the linear model to accommodate nonlinear relationships, using [polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression).\n",
    "\n",
    "Run the following code to observe the relationship between `mpg` (gas mileage in miles per gallon) and `horsepower`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    auto_df.horsepower, auto_df.mpg, facecolors=\"None\", edgecolors=\"k\", alpha=0.5\n",
    ")\n",
    "plt.xlabel(\"horsepower\")\n",
    "plt.ylabel(\"mpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows a strong relationship between `mpg` and `horsepower`, but it is clearly not linear. We can use a polynomial regression to fit a nonlinear model to the data. We can fit a polynomial regression of degree 2 (quadratic) to the data using the following regression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{mpg} = \\beta_0 + \\beta_1 \\times \\textrm{horsepower} + \\beta_2 \\times \\textrm{horsepower}^2 + \\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "Run the following code to display the relationship between `mpg` (gas mileage in miles per gallon) and $\\textrm{horsepower}^2$ as well as $\\textrm{horsepower}^5$ (Figure 3.8 of the textbook) for cars in the `Auto` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Seaborn's regplot() you can easily plot higher order polynomials.\n",
    "plt.scatter(\n",
    "    auto_df.horsepower, auto_df.mpg, facecolors=\"None\", edgecolors=\"k\", alpha=0.5\n",
    ")\n",
    "\n",
    "sns.regplot(\n",
    "    x=auto_df.horsepower,\n",
    "    y=auto_df.mpg,\n",
    "    ci=None,\n",
    "    label=\"Linear\",\n",
    "    scatter=False,\n",
    "    color=\"orange\",\n",
    ")\n",
    "sns.regplot(\n",
    "    x=auto_df.horsepower,\n",
    "    y=auto_df.mpg,\n",
    "    ci=None,\n",
    "    label=\"Degree 2\",\n",
    "    order=2,\n",
    "    scatter=False,\n",
    "    color=\"lightblue\",\n",
    ")\n",
    "sns.regplot(\n",
    "    x=auto_df.horsepower,\n",
    "    y=auto_df.mpg,\n",
    "    ci=None,\n",
    "    label=\"Degree 5\",\n",
    "    order=5,\n",
    "    scatter=False,\n",
    "    color=\"g\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.ylim(5, 55)\n",
    "plt.xlim(40, 240);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure seems to suggest that a quadratic relationship between `mpg` and `horsepower` is more appropriate than a linear relationship.\n",
    "\n",
    "In fact, this is still a linear model with $x_1 = \\textrm{horsepower}$ and $x_2 = \\textrm{horsepower}^2$. Run the following code to create a new predictor `horsepower2` and inspect the [Auto dataset](https://github.com/pykale/transparentML/blob/main/data/Auto.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df[\"horsepower2\"] = auto_df.horsepower**2\n",
    "auto_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following code to fit a linear regression model with `mpg` as the response and `horsepower` and `horsepower2` as the predictors (Table 3.10 of the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"mpg ~ horsepower + horsepower2\", auto_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of linear regression\n",
    "\n",
    "Every model will have some limitations, and linear regression is no exception. Here we briefly study two of the limitations of linear regression to show the possible complexities of real-world data. More limitations can be found in the textbook.\n",
    "\n",
    "#### Nonlinearity of the Data\n",
    "  \n",
    "The linear regression model assumes that there is a \"straight-line\" relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are questionable. In addition, the prediction accuracy of the model can be significantly reduced.\n",
    "\n",
    "Run the following code to compare the differences in accuracy between a linear regression model and a quadratic regression model for regressing `mpg` onto `horsepower`.\n",
    "\n",
    "Firstly, fitting a linear regression model and a quadratic regression model for regressing `mpg` onto `horsepower` and compute the residuals for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "\n",
    "# Linear fit\n",
    "X = auto_df.horsepower.values.reshape(-1, 1)\n",
    "y = auto_df.mpg\n",
    "regr.fit(X, y)\n",
    "\n",
    "auto_df[\"pred1\"] = regr.predict(X)\n",
    "auto_df[\"resid1\"] = auto_df.mpg - auto_df.pred1\n",
    "\n",
    "# Quadratic fit\n",
    "X2 = auto_df[[\"horsepower\", \"horsepower2\"]].values\n",
    "regr.fit(X2, y)\n",
    "\n",
    "auto_df[\"pred2\"] = regr.predict(X2)\n",
    "auto_df[\"resid2\"] = auto_df.mpg - auto_df.pred2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots of the residuals for each model. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left plot\n",
    "sns.regplot(\n",
    "    x=auto_df.pred1,\n",
    "    y=auto_df.resid1,\n",
    "    lowess=True,\n",
    "    ax=ax1,\n",
    "    line_kws={\"color\": \"r\", \"lw\": 1},\n",
    "    scatter_kws={\"facecolors\": \"None\", \"edgecolors\": \"k\", \"alpha\": 0.5},\n",
    ")\n",
    "ax1.hlines(\n",
    "    0,\n",
    "    xmin=ax1.xaxis.get_data_interval()[0],\n",
    "    xmax=ax1.xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dotted\",\n",
    ")\n",
    "ax1.set_title(\"Residual Plot for Linear Fit\")\n",
    "\n",
    "# Right plot\n",
    "sns.regplot(\n",
    "    x=auto_df.pred2,\n",
    "    y=auto_df.resid2,\n",
    "    lowess=True,\n",
    "    line_kws={\"color\": \"r\", \"lw\": 1},\n",
    "    ax=ax2,\n",
    "    scatter_kws={\"facecolors\": \"None\", \"edgecolors\": \"k\", \"alpha\": 0.5},\n",
    ")\n",
    "ax2.hlines(\n",
    "    0,\n",
    "    xmin=ax2.xaxis.get_data_interval()[0],\n",
    "    xmax=ax2.xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dotted\",\n",
    ")\n",
    "ax2.set_title(\"Residual Plot for Quadratic Fit\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(\"Fitted values\")\n",
    "    ax.set_ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{Admonition} How to interpret the plots?\n",
    ":class: tip, dropdown\n",
    "- The left panel displays a residual plot from the linear regression of `mpg` onto `horsepower` on the `Auto` dataset. The red line is a smooth fit to the residuals for identifying any trends in the residuals. Here, the residuals exhibit a clear U-shape, which provides a strong indication of nonlinearity in the data.\n",
    "- The right panel displays the residual plot that results from the model contains a quadratic term. There appears to be little pattern in the residuals, suggesting that the quadratic term improves the fit to the data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collinearity\n",
    "  \n",
    "Collinearity refers to the situation in which two or more predictor variables are closely related to one another. \n",
    "  \n",
    " Run the following code to illustrate the problem of collinearity in the `Credit` dataset (Figure 3.14 of the textbook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left plot\n",
    "ax1.scatter(credit_df.Limit, credit_df.Age, facecolor=\"None\", edgecolor=\"r\")\n",
    "ax1.set_ylabel(\"Age\")\n",
    "\n",
    "# Right plot\n",
    "ax2.scatter(credit_df.Limit, credit_df.Rating, facecolor=\"None\", edgecolor=\"r\")\n",
    "ax2.set_ylabel(\"Rating\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(\"Limit\")\n",
    "    ax.set_xticks([2000, 4000, 6000, 8000, 12000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left panel, the two predictors `limit` and `age` appear to have no obvious relationship. In contrast, in the right panel, the predictors `limit` and `rating` are very highly correlated with each other, and we say that they are *collinear*. In this context, `limit` and `rating` tend to increase or decrease together, it can be difficult to determine how each one separately is associated with the response `balance`.\n",
    "\n",
    "Run the following code to learn the difficulties that can result from collinearity in the context of the `Credit` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = credit_df.Balance\n",
    "\n",
    "# Regression for left plot\n",
    "X = credit_df[[\"Age\", \"Limit\"]].values\n",
    "regr1 = LinearRegression()\n",
    "regr1.fit(scale(X.astype(\"float\"), with_std=False), y)\n",
    "print(\"Age/Limit\\n\", regr1.intercept_)\n",
    "print(regr1.coef_)\n",
    "\n",
    "# Regression for right plot\n",
    "X2 = credit_df[[\"Rating\", \"Limit\"]].values\n",
    "regr2 = LinearRegression()\n",
    "regr2.fit(scale(X2.astype(\"float\"), with_std=False), y)\n",
    "print(\"\\nRating/Limit\\n\", regr2.intercept_)\n",
    "print(regr2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create grid coordinates for plotting and then calculate RSS based on grid of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid coordinates\n",
    "beta_age = np.linspace(regr1.coef_[0] - 3, regr1.coef_[0] + 3, 100)\n",
    "beta_limit = np.linspace(regr1.coef_[1] - 0.02, regr1.coef_[1] + 0.02, 100)\n",
    "\n",
    "beta_rating = np.linspace(regr2.coef_[0] - 3, regr2.coef_[0] + 3, 100)\n",
    "beta_limit2 = np.linspace(regr2.coef_[1] - 0.2, regr2.coef_[1] + 0.2, 100)\n",
    "\n",
    "X1, Y1 = np.meshgrid(beta_limit, beta_age, indexing=\"xy\")\n",
    "X2, Y2 = np.meshgrid(beta_limit2, beta_rating, indexing=\"xy\")\n",
    "Z1 = np.zeros((beta_age.size, beta_limit.size))\n",
    "Z2 = np.zeros((beta_rating.size, beta_limit2.size))\n",
    "\n",
    "limit_scaled = scale(credit_df.Limit.astype(\"float\"), with_std=False)\n",
    "age_scaled = scale(credit_df.Age.astype(\"float\"), with_std=False)\n",
    "rating_scaled = scale(credit_df.Rating.astype(\"float\"), with_std=False)\n",
    "\n",
    "# calculate RSS\n",
    "for (i, j), v in np.ndenumerate(Z1):\n",
    "    Z1[i, j] = (\n",
    "        (y - (regr1.intercept_ + X1[i, j] * limit_scaled + Y1[i, j] * age_scaled)) ** 2\n",
    "    ).sum() / 1000000\n",
    "\n",
    "for (i, j), v in np.ndenumerate(Z2):\n",
    "    Z2[i, j] = (\n",
    "        (y - (regr2.intercept_ + X2[i, j] * limit_scaled + Y2[i, j] * rating_scaled))\n",
    "        ** 2\n",
    "    ).sum() / 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the contours of the RSS with respect to the coefficients of the two linear regression models in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import contour\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.suptitle(\"RSS - Regression coefficients\", fontsize=20)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "min_RSS = r\"$\\beta_0$, $\\beta_1$ for minimized RSS\"\n",
    "\n",
    "# Left plot\n",
    "contour1 = ax1.contour(X1, Y1, Z1, cmap=plt.cm.Set1, levels=[21.25, 21.5, 21.8])\n",
    "ax1.scatter(regr1.coef_[1], regr1.coef_[0], c=\"r\", label=min_RSS)\n",
    "ax1.clabel(contour1, inline=True, fontsize=10, fmt=\"%1.1f\")\n",
    "ax1.set_ylabel(r\"$\\beta_{Age}$\", fontsize=17)\n",
    "\n",
    "# Right plot\n",
    "contour2 = ax2.contour(X2, Y2, Z2, cmap=plt.cm.Set1, levels=[21.5, 21.8])\n",
    "ax2.scatter(regr2.coef_[1], regr2.coef_[0], c=\"r\", label=min_RSS)\n",
    "ax2.clabel(contour2, inline=True, fontsize=10, fmt=\"%1.1f\")\n",
    "ax2.set_ylabel(r\"$\\beta_{Rating}$\", fontsize=17)\n",
    "ax2.set_xticks([-0.1, 0, 0.1, 0.2])\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r\"$\\beta_{Limit}$\", fontsize=17)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left: A contour plot of RSS for the regression of `balance` onto `age` and `limit`. The minimum value is well defined. \n",
    "\n",
    "Right: A contour plot of RSS for the regression of `balance` onto `rating` and `limit`. Because of the collinearity, there are many pairs ($\\beta_{\\text{Limit}}$ , $\\beta_{\\text{Rating}}$) with a similar value for RSS.\n",
    "\n",
    "Collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\\hat{\\beta}_j$ to grow. In the presence of collinearity, we may fail to reject $H_0$ : $\\hat{\\beta}_j = 0$. This means that the power of the hypothesis test—the probability of correctly detecting a non-zero coefficient is reduced by collinearity.\n",
    "\n",
    "Run the following code to compare the coefficient estimates obtained from two separate multiple regression models using `statsmodels`, where the first model is a regression of `balance` on `age` and `limit`, and the second model is a regression of `balance` on `rating` and `limit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_age_limit = ols(\"Balance ~ Age + Limit\", credit_df).fit()\n",
    "est_age_limit.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_rating_limit = ols(\"Balance ~ Rating + Limit\", credit_df).fit()\n",
    "est_rating_limit.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above generates Table 3.11 of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} How to interpret the results? \n",
    ":class: tip, dropdown\n",
    "In the first regression, both `age` and `limit` are highly significant with very small $p$-values. In the second, the collinearity between `limit` and `rating` has caused the standard error for the `limit` coefficient estimate to increase by a factor of 12 and the $p$-value to increase to 0.701. In other words, the importance of the `limit` variable has been masked due to the presence of collinearity.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. All the following exercises involve the use of the **[Boston](https://github.com/pykale/transparentML/blob/main/data/Boston.csv)** dataset. We will now try to predict per capita crime rate using the other variables in this dataset. In other words, per capita crime rate is the response, and the other variables are the predictors.\n",
    "\n",
    "For **each predictor**, fit a **simple linear regression** model to predict the response. Don't forget to handle any **NULL values**. **Hint**: See section [2.2.2](https://pykale.github.io/transparentML/02-linear-reg/multi-linear-regression.html#import-libraries-and-load-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "import pandas as pd\n",
    "\n",
    "boston_url = \"https://github.com/pykale/transparentML/raw/main/data/Boston.csv\"\n",
    "\n",
    "boston_df = pd.read_csv(boston_url).dropna()\n",
    "\n",
    "\n",
    "models_a = [\n",
    "    ols(formula=\"crim ~ {}\".format(f), data=boston_df).fit()\n",
    "    for f in boston_df.columns[1:]\n",
    "]\n",
    "\n",
    "for model in models_a:\n",
    "    print(model.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. In which of the models is there a **statistically significant** association between the predictor and the response? Create some plots to back up your assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"p < 0.05\\n\")\n",
    "\n",
    "for model in models_a:\n",
    "    if model.pvalues[1] < 0.05:\n",
    "        print(model.params[1:].index[0])\n",
    "\n",
    "print(\"\\np > 0.05\\n\")\n",
    "\n",
    "for model in models_a:\n",
    "    if model.pvalues[1] > 0.05:\n",
    "        print(model.params[1:].index[0])\n",
    "\n",
    "#  zn, indu,s nox, rm, ag, dis, rad, tax, ptratio, black, lstat, medv  are higly significant with very small p values.\n",
    "\n",
    "\n",
    "def plot_grid(df, response, cols):\n",
    "    variables = df.columns.drop(response)\n",
    "    for i in range(0, len(variables), cols):\n",
    "        g = sns.pairplot(\n",
    "            df, y_vars=[response], x_vars=variables[i : i + cols], height=4\n",
    "        )\n",
    "        g.map(sns.regplot)\n",
    "    return\n",
    "\n",
    "\n",
    "plot_grid(boston_df, \"crim\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**. Fit a **multiple regression** model to predict the response using all of the predictors. **Hint**: See section [2.3.3](https://pykale.github.io/transparentML/02-linear-reg/extension-limitation.html#extensions-of-linear-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "response = \"crim\"\n",
    "predictors = boston_df.columns.drop(response)\n",
    "f = \"{} ~ {}\".format(response, \"+\".join(predictors))\n",
    "\n",
    "model_b = ols(formula=f, data=boston_df).fit()\n",
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4**. Which features seem to be **statistically significant** in this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"p < 0.05\\n\")\n",
    "model_b.pvalues[model_b.pvalues < 0.05]\n",
    "\n",
    "# Only the resultent features seem to be statistically significant in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5**. How do your results from **Exercise 1** compare to your results from **Exercise 3**? Create a plot displaying the univariate regression coefficients from **Exercise 1** on the $x$-axis, and the multiple regression coefficients from **Exercise 3** on the $y$-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the $x$-axis, and its coefficient estimate in the multiple linear regression model is shown on the $y$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Get coefficients\n",
    "univariate_params = pd.concat([m.params[1:] for m in models_a])\n",
    "multivariate_params = model_b.params[1:]\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Univariate_coef\": univariate_params,\n",
    "        \"Multivariate_coef\": multivariate_params,\n",
    "    }\n",
    ")\n",
    "print(df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "ax = sns.scatterplot(x=\"Univariate_coef\", y=\"Multivariate_coef\", data=df)\n",
    "\n",
    "# Multivariate regression found 5 of 13 predictors to be significnat where univariate regression found 12 of 13 significant. Multivariate regression seems to find significanlty less predictors to be significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6**. Is there evidence of **nonlinear association** between any of the predictors and the response? To answer this question, for each predictor $X$, fit a model of the form\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 \\beta_3 X^3 + \\epsilon \n",
    "$$\n",
    "\n",
    "**Hint**: See section [2.3.3.2](https://pykale.github.io/transparentML/02-linear-reg/extension-limitation.html#nonlinear-relationships)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "models_d = [\n",
    "    ols(\n",
    "        formula=\"crim ~ {0} + np.power({0}, 2) + np.power({0}, 3)\".format(f),\n",
    "        data=boston_df,\n",
    "    ).fit()\n",
    "    for f in boston_df.columns[1:]\n",
    "]\n",
    "\n",
    "for model in models_d:\n",
    "    print(model.summary().tables[1])\n",
    "\n",
    "\n",
    "print(\"\\nFeatures with p < 0.05\\n\")\n",
    "\n",
    "sig = pd.concat([model.pvalues[model.pvalues < 0.05] for model in models_d])\n",
    "\n",
    "print(pd.DataFrame({\"P>|t|\": sig.drop(\"Intercept\")}))\n",
    "\n",
    "# There is evidence of a nonlinear association between the repsonse and INDUS, NOX, AGE, DIS, PTRATIO, medv\n",
    "# There is evidence of a linear association between the response and ZN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7**. Compare the differences between a linear regression model and a quadratic regression model for regressing **ptratio** onto **crim**.  Does the quadratic term improves the fit to the data? **Hint**: See section [2.3.3.3](https://pykale.github.io/transparentML/02-linear-reg/extension-limitation.html#limitations-of-linear-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Linear fit\n",
    "X = boston_df.ptratio.values.reshape(-1, 1)\n",
    "y = boston_df.crim\n",
    "regr.fit(X, y)\n",
    "\n",
    "boston_df[\"pred1\"] = regr.predict(X)\n",
    "boston_df[\"resid1\"] = boston_df.crim - boston_df.pred1\n",
    "\n",
    "\n",
    "boston_df[\"ptratio2\"] = boston_df.ptratio**2\n",
    "# Quadratic fit\n",
    "X2 = boston_df[[\"ptratio\", \"ptratio2\"]].values\n",
    "regr.fit(X2, y)\n",
    "\n",
    "boston_df[\"pred2\"] = regr.predict(X2)\n",
    "boston_df[\"resid2\"] = boston_df.crim - boston_df.pred2\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left plot\n",
    "sns.regplot(\n",
    "    x=boston_df.pred1,\n",
    "    y=boston_df.resid1,\n",
    "    lowess=True,\n",
    "    ax=ax1,\n",
    "    line_kws={\"color\": \"r\", \"lw\": 1},\n",
    "    scatter_kws={\"facecolors\": \"None\", \"edgecolors\": \"k\", \"alpha\": 0.5},\n",
    ")\n",
    "ax1.hlines(\n",
    "    0,\n",
    "    xmin=ax1.xaxis.get_data_interval()[0],\n",
    "    xmax=ax1.xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dotted\",\n",
    ")\n",
    "ax1.set_title(\"Residual Plot for Linear Fit\")\n",
    "\n",
    "# Right plot\n",
    "sns.regplot(\n",
    "    x=boston_df.pred2,\n",
    "    y=boston_df.resid2,\n",
    "    lowess=True,\n",
    "    line_kws={\"color\": \"r\", \"lw\": 1},\n",
    "    ax=ax2,\n",
    "    scatter_kws={\"facecolors\": \"None\", \"edgecolors\": \"k\", \"alpha\": 0.5},\n",
    ")\n",
    "ax2.hlines(\n",
    "    0,\n",
    "    xmin=ax2.xaxis.get_data_interval()[0],\n",
    "    xmax=ax2.xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dotted\",\n",
    ")\n",
    "ax2.set_title(\"Residual Plot for Quadratic Fit\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(\"Fitted values\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "\n",
    "# The left plot displays a residual plot from the linear regression of ptratio onto crim on the Boston dataset.\n",
    "# The red line is a smooth fit to the residuals for identifying any trends in the residuals.\n",
    "# Here, the residuals exhibit a tendency of U-shape, which provides a strong indication of nonlinearity in the data.\n",
    "\n",
    "\n",
    "# The right panel displays the residual plot that results from the model contains a quadratic term.\n",
    "# There appears to be little pattern in the residuals, suggesting that the quadratic term improves the fit to the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pykale')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
