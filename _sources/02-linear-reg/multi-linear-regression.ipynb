{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression\n",
    "\n",
    "```{admonition} Read then Launch \n",
    "This content is best viewed in html because jupyter notebook cannot display some content (e.g. figures, equations) properly. You should finish reading this page first and then launch it as an interactive notebook in Google Colab (faster, Google account needed) or Binder by clicking the rocket symbol (<i class=\"fas fa-rocket\"></i>) at the top.\n",
    "```\n",
    "\n",
    "We have examined the relationship between `sale` and `TV` of the `Advertising` dataset for simple linear regression. There are two more predictor variables `radio` and `newspaper` in the dataset. How can we account for the effect of these two variables in the model? \n",
    "\n",
    "Watch the 5-minute video below for a visual explanation of multiple linear regression.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/EkAQAi3a4js?start=21\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> \n",
    "\n",
    "[Explaining Multiple Linear Regression by StatQuest](https://www.youtube.com/embed/EkAQAi3a4js?start=21), embedded according to [YouTube's Terms of Service](https://www.youtube.com/static?gl=CA&template=terms).\n",
    "```\n",
    "\n",
    "Then study the following sections to learn more about multiple linear regression with examples in the textbook.\n",
    "\n",
    "## Install libraries\n",
    "\n",
    "<!-- The following `matplotlib` upgrade is necessary when using Google Colab (as of 19/11/2022) for this notebook. You may comment it if you are running the code locally on your own computer: -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colab, please uncomment the following code to update the version of `matplotlib`. Once complete, click the `RESTART RUNTIME` button to restart the runtime in order to use newly installed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q mpl_toolkits statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and load data\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**\n",
    "\n",
    "Load the [Advertising dataset](https://github.com/pykale/transparentML/blob/main/data/Advertising.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Advertising.csv\"\n",
    "advertising_df = pd.read_csv(data_url, header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accommodate multiple predictor variables, one option is to run simple linear regression separately for each predictor variable. \n",
    "\n",
    "The following code runs a simple linear regression model of `radio`, and `newspaper` onto `sales` using `statsmodels`, respectively (Table 3.3 in the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ Radio\", advertising_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ Newspaper\", advertising_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, fitting a separate simple linear regression model for each predictor is problematic: 1) It is unclear how to make a single prediction given the three advertising media budgets; 2) each separate linear regression model ignores  the effect of the other two predictors, which can lead to misleading estimates. \n",
    "\n",
    "A better approach is to use _multiple linear regression_. Multiple linear regression is an extension of simple linear regression. It allows us to predict a quantitative response using more than one predictor variable. The equation for a multiple linear regression model with $D$ predictor variables is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_D x_D + \\epsilon,\n",
    "\\end{equation}\n",
    "\n",
    "where $y$ is the response, $x_1, x_2, ..., x_D$ are the $D$ predictors, $D$ is the total number of predictor variables (features), and $\\epsilon$ is the error term. The $\\beta$s are called the _regression coefficients_, where $\\beta_0$ is the bias (intercept), and $\\beta_1, \\beta_2, ..., \\beta_D$ are the weights (slopes). Considering $N$ samples, the equation can be written in matrix form as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{y}$ is an $N \\times 1$ vector of responses, $\\mathbf{X}$ is an $N \\times (D+1)$ matrix of predictors, $\\boldsymbol{\\beta}$ is a $(D+1) \\times 1$ vector of regression coefficients, and $\\boldsymbol{\\epsilon}$ is an $N \\times 1$ vector of errors. The predictor (feature) matrix $\\mathbf{X}$ contains a column of 1s to account for the intercept. The vector $\\boldsymbol{\\beta}$ contains the intercept in the first position and the slopes for the remaining $D$ predictors. The vector $\\boldsymbol{\\epsilon}$ contains the error terms for each observation. \n",
    "\n",
    "Using the `Advertising` dataset as an example, we can fit a multiple linear regression model to the three predictor variables `TV`, `radio`, and `newspaper` to predict `sales` as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{Radio} + \\beta_3 \\times \\text{Newspaper} + \\epsilon.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the regression coefficients\n",
    "\n",
    "Similar to simple linear regression, we can estimate the regression coefficients using least squares. The least squares estimates for the regression coefficients are given by:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "\\text{RSS} = & \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\\\\n",
    "= & \\sum_{i=1}^N (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - ... - \\hat{\\beta}_D x_{iD})^2,\n",
    "\\end{aligned}\n",
    "\\end{align}\n",
    "\n",
    "where $y_i$ is the $i$th response, $\\hat{y}_i$ is the $i$th predicted response, $\\hat{\\beta}_0$ is the intercept, $\\hat{\\beta}_1$ is the slope for $x_{i1}$, $\\hat{\\beta}_2$ is the slope for $x_{i2}$, and so on. $\\hat{\\beta}$s are the least squares estimates for the regression coefficients, which can be obtained in matrix form as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}.\n",
    "\\end{equation}\n",
    "\n",
    "The following code run a multiple linear regression model to regress `TV`, `radio`, and `newspaper` onto `sales` using `statsmodels`, and display the learnt coefficients (Table 3.4 in the textbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ TV + Radio + Newspaper\", advertising_df).fit()\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "\n",
    "We have interpreted the results of simple linear regression in the [previous section](https://pykale.github.io/transparentML/02-linear-reg/simple-linear-regression.html#example-explanation-of-system-transparency). The interpretation of the results of multiple linear regression is similar. The only difference is that there are now three coefficients to interpret.\n",
    "\n",
    "```{admonition} Challenge\n",
    ":class: important\n",
    "Interpret the results of the multiple linear regression model above based on the [previous section](https://pykale.github.io/transparentML/02-linear-reg/simple-linear-regression.html#example-explanation-of-system-transparency) and then click the following to compare the provided interpretation with yours.\n",
    "```\n",
    "\n",
    "```{admonition} How to interpret the results? \n",
    ":class: tip, dropdown\n",
    "We interpret these results above as follows: \n",
    "- For a **given** (i.e. fixed) amount of `TV` and `newspaper `advertising budgets, spending an additional \\$1,000 on `radio` advertising is associated with approximately 189 units of additional `sales` (_recall the units of the variables_). \n",
    "- Comparing these coefficients to the estimates in simple linear regression, we notice that the multiple regression coefficient estimates for `TV` and `radio` are pretty similar to the simple linear regression coefficient estimates. However, while the `newspaper` regression coefficient estimate in simple linear regression was significantly non-zero, the coefficient estimate for `newspaper` in the multiple regression model is close to zero, and the corresponding $p$-value is no longer significant, with a value around 0.86. \n",
    "- This illustrates that the simple and multiple regression coefficients can be quite different. This difference stems from the fact that in the simple regression case, the slope term represents the average increase in product sales associated with a \\$1,000 increase in newspaper advertising, ignoring other predictors such as `TV` and `radio`. By contrast, in the multiple regression setting, the coefficient for `newspaper` represents the average increase in product `sales` associated with increasing `newspaper` spending by \\$1,000 while holding `TV` and `radio` **fixed**.\n",
    "```\n",
    "\n",
    "Why the relationship between `sales` and `newspaper` are opposite in the simple linear regression and multiple linear regression? Use following code displays the correlation matrix of the `Advertising` dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertising_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} How to interpret the results?\n",
    ":class: tip, dropdown\n",
    "The [correlation](https://en.wikipedia.org/wiki/Correlation) between `radio` and `newspaper` is 0.35, which is much higher than the other pair-wise correlations among the three medias. This indicates that in those markets spending more on `newspaper` (`radio`) advertising, there is a tendency to spend more on `radio` (`newspaper`) advertising as well.\n",
    "\n",
    "Now suppose that the multiple regression is correct and `newspaper` advertising is not associated with sales, but `radio` advertising is associated with `sales`. Then in markets where we spend more on `radio`, our sales will tend to be higher. As our correlation matrix shows, we also tend to spend more on `newspaper` advertising in those same markets. Hence, in a simple linear regression which only examines sales versus `newspaper`, we will observe that higher values of `newspaper` tend to be associated with higher values of `sales`, even though `newspaper` advertising is not directly associated with `sales`. So `newspaper` advertising is a surrogate for `radio` advertising; `newspaper` gets “credit” for the association between `radio` on `sales`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important questions in multiple linear regression\n",
    "\n",
    "### Is at least one of the predictors $x_1, x_2, ..., x_D$ useful in predicting the response?\n",
    "\n",
    "We can answer this question by testing the [null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis) that all the regression coefficients are zero, i.e.\n",
    "\n",
    "$$\n",
    "H_0: \\beta_1 = \\beta_2 = ... = \\beta_D = 0.\n",
    "$$\n",
    "\n",
    "versus the [alternative hypothesis](https://en.wikipedia.org/wiki/Alternative_hypothesis) that at least one of the regression coefficients is non-zero, i.e.:\n",
    "\n",
    "$$\n",
    "H_1: \\text{at least one of the regression coefficients is non-zero}. \n",
    "$$ \n",
    "\n",
    "This hypothesis test is performed by computing the [$F$-statistic](https://en.wikipedia.org/wiki/F-test), which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "F = \\frac{(\\text{TSS} - \\text{RSS})/D}{\\text{RSS}/(N-D-1)},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{TSS} = \\sum(y_i - \\bar{y})^2$ and $\\text{RSS} = \\sum(y_i - \\hat{y}_i)^2$. Remember, $y_i$ is the $i$th target, $\\hat{y}_i$ is the $i$th prediction and $\\bar{y}$ is the sample mean. When there is no relationship between the response and predictors, one would expect the $F$-statistic to take on a value close to 1. On the other hand, if $H_1$ a is true, we can expect $F$ to be greater than 1. \n",
    "\n",
    "The $F$-statistic for the multiple linear regression model obtained by regressing `sales` onto `radio`, `TV`, and `newspaper` is 570.3 (displayed in the first regression results in [a previous section](https://pykale.github.io/transparentML/02-linear-reg/multi-linear-regression.html#estimating-the-regression-coefficients)). Since this is far greater than 1, it provides compelling evidence against the null hypothesis $H_0$. In other words, the large $F$-statistic suggests that at least one of the advertising media must be related to `sales`.\n",
    "\n",
    "Watch the following 9-minute video excerpt to learn more about the $F$-statistic.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/7ArmBVF2dCs?start=969&end=1525\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> \n",
    "\n",
    "[Calculating a $p$-value for $R^2$ by StatQuest](https://www.youtube.com/embed/7ArmBVF2dCs?start=969&end=1525), embedded according to [YouTube's Terms of Service](https://www.youtube.com/static?gl=CA&template=terms).\n",
    "```\n",
    "\n",
    "<!-- then $E\\{\\text{TSS} - \\text{RSS}/p\\} > \\sigma^2$, where $\\sigma$ is the standard deviation of the error term, so we expect $F$ to be greater than 1. -->\n",
    "\n",
    "<!-- The numerator of the F-statistic is the ratio of the total sum of squares to the residual sum of squares, and the denominator is the ratio of the residual sum of squares to the degrees of freedom. The F-statistic has an $F$ distribution with $p$ and $n-p-1$ degrees of freedom. The null hypothesis is that all the regression coefficients are zero, and the alternative hypothesis is that at least one of the regression coefficients is non-zero.  -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do all the predictors help to explain $y$, or is only a subset of the predictors useful?\n",
    "\n",
    "It is possible that the response is only associated with a subset of the predictors. The task of determining which subset of predictors are (most) associated with the response is referred to as *variable selection* (or *feature selection*). This will be discussed in more detail in {doc}`Feature Selection and Regularisation <../06-ftr-select-regularise/overview>`.\n",
    "\n",
    "<!-- in order to fit a single model involving only those predictors,  -->\n",
    "\n",
    "There are three common approaches to variable selection:\n",
    "\n",
    "- Forward selection. We begin with a model containing no predictors, and then consider adding predictors one at a time until all predictors have been considered. The best single predictor is added to the model, and the process is repeated until all predictors have been added to the model. This is a greedy algorithm, and it is not guaranteed to find the best model containing a subset of the predictors. \n",
    "- Backward selection. We begin with a model containing all predictors, and then consider removing predictors one at a time until no predictors remain. The worst single predictor is removed from the model, and the process is repeated until no predictors remain in the model. This is also a greedy algorithm, and it is not guaranteed to find the best model containing a subset of the predictors.\n",
    "- Mixed selection. We begin with some initial model containing a subset of the predictors. We then consider adding or removing each predictor individually, and retain the best model that results. This is also a greedy algorithm, and it is not guaranteed to find the best model containing a subset of the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well does the model fit the data?\n",
    "\n",
    "Similar to simple linear regression. We can answer this question by computing the $R^2$ and RSE statistics. The $R^2$ for multiple linear regression is defined in the same way as in simple linear regression as:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}.\n",
    "\\end{equation}\n",
    "\n",
    "The $R^2$ statistic provides an indication of the proportion of the variance in the response that is predictable from the predictors. In this case, the $R^2$ statistic is 0.897, which indicates that 89.7% of the variance in `sales` is predictable from `TV`, `radio`, and `newspaper`.\n",
    "\n",
    "The RSE for multiple linear regression is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{N-D-1}} = \\sqrt{\\frac{\\sum(y_i - \\hat{y}_i)^2}{N-D-1}}.\n",
    "\\end{equation}\n",
    "\n",
    "You can verify that when $D=1$, the RSE for multiple linear regression is the same as the RSE for {doc}`simple linear regression <simple-linear-regression>`.\n",
    "\n",
    "Run the following code to fit and then evaluate a multiple linear regression model using `scikit-learn`:\n",
    "\n",
    "Firstly, fit a linear regression to `sales` using `TV` and `radio` as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "\n",
    "X = advertising_df[[\"Radio\", \"TV\"]].values\n",
    "y = advertising_df.Sales\n",
    "\n",
    "regr.fit(X, y)\n",
    "print(regr.coef_)\n",
    "print(regr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the min/max values of Radio & TV to set up the grid (range) for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertising_df[[\"Radio\", \"TV\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a coordinate grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio = np.arange(0, 50)\n",
    "tv = np.arange(0, 300)\n",
    "\n",
    "beta_1, beta_2 = np.meshgrid(radio, tv, indexing=\"xy\")\n",
    "Z = np.zeros((tv.size, radio.size))\n",
    "\n",
    "for (i, j), v in np.ndenumerate(Z):\n",
    "    Z[i, j] = (\n",
    "        regr.intercept_ + beta_1[i, j] * regr.coef_[0] + beta_2[i, j] * regr.coef_[1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3D plot of `sales` vs `TV` and `radio`.\n",
    "\n",
    "<!-- [ToDo] If possible, change the following to stem plot, such as in the [stem3d_demo](https://matplotlib.org/stable/gallery/mplot3d/stem3d_demo.html) to better visualisation as the textbook. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.suptitle(\"Regression: Sales ~ Radio + TV Advertising\", fontsize=20)\n",
    "\n",
    "ax = axes3d.Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "\n",
    "ax.plot_surface(beta_1, beta_2, Z, rstride=10, cstride=5, alpha=0.4)\n",
    "ax.scatter3D(advertising_df.Radio, advertising_df.TV, advertising_df.Sales, c=\"r\")\n",
    "\n",
    "ax.set_xlabel(\"Radio\")\n",
    "ax.set_xlim(0, 50)\n",
    "ax.set_ylabel(\"TV\")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_zlabel(\"Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a set of predictor values, what response value should we predict, and how accurate is our prediction? \n",
    "\n",
    "<!-- **[Advanced, add more content of prediction interval/or remove this sub-section later]** -->\n",
    "\n",
    "Once we have fit a multiple linear regression model, we can use the model to make predictions of the response for a given set of predictor values. \n",
    "\n",
    "\\begin{equation}\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_D x_D.\n",
    "\\end{equation}\n",
    "\n",
    "However, we must be careful when making predictions, because the observed values of the predictors may not have been part of the data used to fit the model. In this case, the prediction may not be very accurate and using a confidence interval can help quantify the uncertainty associated with the prediction. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. All the following exercises involve the use of the **[Carseats](https://github.com/pykale/transparentML/blob/main/data/Carseats.csv)** dataset. Fit a multiple regression model to predict **Sales** using **Price**, **Income**, and **CompPrice**. **Hint**: See section [2.2.5](https://pykale.github.io/transparentML/02-linear-reg/multi-linear-regression.html#important-questions-in-multiple-linear-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load data\n",
    "carseats_df = pd.read_csv(\n",
    "    \"https://github.com/pykale/transparentML/raw/main/data/Carseats.csv\"\n",
    ")\n",
    "\n",
    "regr = LinearRegression()\n",
    "\n",
    "X = carseats_df[[\"Price\", \"Income\", \"CompPrice\"]].values\n",
    "y = carseats_df.Sales\n",
    "\n",
    "regr.fit(X, y)\n",
    "print(\"Regression model slop/coeffcient (weight):\", regr.coef_)\n",
    "print(\"Regression model intercept (bias):\", regr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i**. How well does the model fit the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_pred = regr.predict(X)\n",
    "print(\"R2 score:\", r2_score(y, y_pred))\n",
    "\n",
    "# The model didn't fit the data well as the R2 score is still way below 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii**. Does multiple linear regression help the model fit the data better than simpler linear regression (see simple linear regression [exercises](https://pykale.github.io/transparentML/02-linear-reg/simple-linear-regression.html#exercise))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**Yes. The** $R^2$ **score has increased in multiple linear regression, and the MSE has decreased, which means the model performed better than simpler linear regression in terms of fitting the data.**\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii**. Write out the model in equation form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "$\\hat{y} = 4.95 + (-0.08719674 * Price) + (0.01525145 * Income) + (0.09278583 * CompPrice)$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. Find the correlation between **Sales, Price, Income, and CompPrice** and interpret the result. **Hint**: See section [2.2.4](https://pykale.github.io/transparentML/02-linear-reg/multi-linear-regression.html#interpreting-the-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "carseats_df[[\"Sales\", \"Price\", \"Income\", \"CompPrice\"]].corr()\n",
    "\n",
    "# The correlation between Price and CompPrice is 0.58, which is much higher than the other pair-wise correlations. This indicates that by increasing the component price(Price) of a caraseat, there is a tendency for the overall price(CompPrice) to increase as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**. Perform a multiple linear regression using **statsmodels** library with **Sales** as the response and **Price**, **Income**, and **CompPrice** as predictors. Use the **summary()** function to print the results. Compare the regression coefficients/weights with **Exercise 1**. **Hint**: See section [2.2.3](https://pykale.github.io/transparentML/02-linear-reg/multi-linear-regression.html#estimating-the-regression-coefficients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "\n",
    "f = \"Sales ~ Price + Income + CompPrice\"\n",
    "y, X = patsy.dmatrices(f, carseats_df, return_type=\"dataframe\")\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# The regression coeffcients are same as scikit learn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4**. From the model fitted in **Exercise 3**, for which predictors can you reject the null hypothesis $H_0 : \\beta_j = 0$ ? **Hint**: See section [2.2.5](https://pykale.github.io/transparentML/02-linear-reg/multi-linear-regression.html#important-questions-in-multiple-linear-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# The following predictors have p-values < 0.05 which suggests we can reject the null hypothesis\n",
    "model.pvalues[model.pvalues < 0.05].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i**. Which predictors appear to have a **statistically significant** relationship with the response?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**All predictors appear to have a statistically significant relationship to the response.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii**. What does the coefficient for the income variable suggest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**The coefficient for the income variable suggests that there is a positive relationship between income and sales, where the response variable increases as income increases.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5**. Using the fitted model from **Exercise 3**, obtain **$95\\%$** confidence intervals for the coefficients. **Hint**: See section [2.1.9](https://pykale.github.io/transparentML/02-linear-reg/simple-linear-regression.html#confidence-intervals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Extract 95% confidence intervals\n",
    "conf_inter_95 = model.conf_int(alpha=0.05)\n",
    "conf_inter_95.rename(\n",
    "    index=str,\n",
    "    columns={\n",
    "        0: \"min.\",\n",
    "        1: \"max.\",\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
