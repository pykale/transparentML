{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression model for classification?\n",
    "\n",
    "## What is classification?\n",
    "\n",
    "Just as in the simple regression setting, in the simple (univariate) classification setting, we have a set of training observations $ (x_1, y_1), \\dots, (x_N, y_N) $, where each $ y_n $ is a qualitative (categorical) response.\n",
    "\n",
    "In this chapter, we will illustrate the concept of classification using the simulated [Default dataset](https://github.com/pykale/transparentML/blob/main/data/Default.csv) (click to explore). We are interested in predicting whether an individual will default on his or her credit card payment, which takes on the value `Yes` if the customer defaults on their credit card payment and `No` if they do not, on the basis of annual income and monthly credit card balance.\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [Default dataset](https://github.com/pykale/transparentML/blob/main/data/Default.csv) as a pandas dataframe, convert categories (`default` and `student`) to numerical values (0 and 1), and inspect the first three rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Default.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# Note: factorize() returns two objects: a label array and an array with the unique values. We are only interested in the first object.\n",
    "df[\"default2\"] = df.default.factorize()[0]\n",
    "df[\"student2\"] = df.student.factorize()[0]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good habit to inspect the data in a bit more detail before fitting a model. Let us visualise 10,000 individuals' annual `income`, monthly credit card `balance`, and their relationships with the `default` status using [box plot](https://en.wikipedia.org/wiki/Box_plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "gs = GridSpec(1, 4)\n",
    "ax1 = plt.subplot(gs[0, :-2])\n",
    "ax2 = plt.subplot(gs[0, -2])\n",
    "ax3 = plt.subplot(gs[0, -1])\n",
    "\n",
    "# Take a fraction of the samples where target value (default) is 'no'\n",
    "df_no = df[df.default2 == 0].sample(frac=0.15)\n",
    "# Take all samples  where target value is 'yes'\n",
    "df_yes = df[df.default2 == 1]\n",
    "#\n",
    "df_ = pd.concat((df_no, df_yes))\n",
    "\n",
    "ax1.scatter(\n",
    "    df_[df_.default == \"Yes\"].balance,\n",
    "    df_[df_.default == \"Yes\"].income,\n",
    "    s=40,\n",
    "    c=\"orange\",\n",
    "    marker=\"+\",\n",
    "    linewidths=1,\n",
    ")\n",
    "ax1.scatter(\n",
    "    df_[df_.default == \"No\"].balance,\n",
    "    df_[df_.default == \"No\"].income,\n",
    "    s=40,\n",
    "    marker=\"o\",\n",
    "    linewidths=1,\n",
    "    edgecolors=\"lightblue\",\n",
    "    facecolors=\"white\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "#\n",
    "ax1.set_ylim(ymin=0)\n",
    "ax1.set_ylabel(\"Income\")\n",
    "ax1.set_xlim(xmin=-100)\n",
    "ax1.set_xlabel(\"Balance\")\n",
    "\n",
    "c_palette = {\"No\": \"lightblue\", \"Yes\": \"orange\"}\n",
    "sns.boxplot(data=df, y=\"balance\", x=\"default\", orient=\"v\", ax=ax2, palette=c_palette)\n",
    "sns.boxplot(data=df, y=\"income\", x=\"default\", orient=\"v\", ax=ax3, palette=c_palette)\n",
    "gs.tight_layout(plt.gcf())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left-hand panel, the individuals who defaulted in a given month are shown in orange, and those who did not in blue. (The overall default rate is about 3\\%, so we have plotted only a fraction of the individuals who did not default.) In the centre and right-hand panels, two pairs of boxplots are shown. The first shows the distribution of balance split by the binary default variable; the second is a similar plot for income. \n",
    "\n",
    "```{admonition} What can you observe from the visualisation above?\n",
    ":class: tip, dropdown\n",
    "From the left-hand panel, it appears that individuals who defaulted tended to have higher credit card balances than those who did not. \n",
    "\n",
    "From the centre and right-hand panels, it appears that individuals who defaulted tended to have higher balance than those who did not.\n",
    "\n",
    "<!-- In this chapter, we learn how to build a model to predict default ($y$) for any given value of balance ($x_1$) and income ($x_2$). Since $y$ is not quantitative, the simple linear regression model of Chapter 3 is not a good choice: we will elaborate on this in the next section.  -->\n",
    "\n",
    "```\n",
    "It is worth noting that this example displays a very pronounced relationship between the predictor `balance` and the response `default`. In most real applications, the relationship between the predictor and the response will not be nearly so strong. To illustrate the classification procedures discussed, we use this example in which the relationship between the predictor and the response is somewhat exaggerated.\n",
    "\n",
    "Can we use linear regression to predict the probability of default? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we use linear regression for classification?\n",
    "\n",
    "<!-- There are two main reasons not to perform classification using a regression method: \n",
    "\n",
    "- A regression method cannot accommodate a qualitative response with more than two classes; \n",
    "- A regression method will not provide meaningful estimates of $\\mathbb{P}(y|x)$ -->\n",
    "\n",
    "### How about converting the response to a quantitative variable?\n",
    "\n",
    "Since we can convert categorical variables (class labels) to numerical values, can we treat the classification problem as a regression problem and use linear regression to predict the class label?\n",
    "\n",
    "Suppose there are three medical conditions of patients in an emergency room: `stroke`, `drug overdose`, and `epileptic seizure`. We could consider encoding these values as a quantitative response variable, $y$, as follows:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases} 1 & \\text{if stroke}, \\\\ 2 & \\text{if drug overdose}, \\\\ 3 & \\text{if epileptic seizure}. \\end{cases}\n",
    "$$\n",
    "\n",
    "Using this coding, least squares could be used to fit a linear regression model to predict $y$. However, this coding implies that `drug overdose` is a condition in between `stroke` and `epileptic seizure`, and insisting that the difference between stroke and drug overdose is the same as the difference between drug overdose and epileptic seizure . In practice, there is no particular reason that this needs to be the case. For instance, one could choose an equally reasonable coding,\n",
    "\n",
    "$$\n",
    "y = \\begin{cases} 1 & \\text{if epileptic seizure}, \\\\ 2 & \\text{if stroke}, \\\\ 3 & \\text{if drug overdose}, \\end{cases}\n",
    "$$\n",
    "\n",
    "which would imply a totally different relationship among the three conditions. Each of these codings would produce fundamentally different linear models that would ultimately lead to different sets of predictions on test observations.\n",
    "\n",
    "If the response variable’s values did take on a natural ordering, such as _mild, moderate, and severe_, and we felt the gap between mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable. Even if so, unfortunately, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.\n",
    "\n",
    "### Will the binary case work?\n",
    "\n",
    "For a binary (two level) qualitative response, the situation is better. For example, perhaps there are only two possibilities for the patient’s medical condition: `stroke` and `drug overdose`. We could then code the response as\n",
    "follows:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases} 1 & \\text{if stroke}, \\\\ 0 & \\text{if drug overdose}. \\end{cases}\n",
    "$$\n",
    "\n",
    "We could then fit a linear regression to this binary response, and predict `drug overdose` if $ y > 0.5 $ and `stroke` otherwise. In the binary case, it is not hard to show that even if we flip the above coding, linear regression will produce the same final predictions.\n",
    "\n",
    "Now let us compare the difference between the linear regression model and the logistic regression model for a binary classification task on the [Default dataset](https://github.com/pykale/transparentML/blob/main/data/Default.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.balance.values.reshape(-1, 1)\n",
    "y = df.default2\n",
    "\n",
    "# Create array of test data. Calculate the classification probability and predicted classification.\n",
    "X_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1, 1)\n",
    "\n",
    "clf = LogisticRegression(solver=\"newton-cg\")\n",
    "clf.fit(X_train, y)\n",
    "prob = clf.predict_proba(X_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Left plot\n",
    "sns.regplot(\n",
    "    x=df.balance,\n",
    "    y=df.default2,\n",
    "    order=1,\n",
    "    ci=None,\n",
    "    scatter_kws={\"color\": \"orange\"},\n",
    "    line_kws={\"color\": \"lightblue\", \"lw\": 2},\n",
    "    ax=ax1,\n",
    ")\n",
    "# Right plot\n",
    "ax2.scatter(X_train, y, color=\"orange\")\n",
    "ax2.plot(X_test, prob[:, 1], color=\"lightblue\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.hlines(\n",
    "        1,\n",
    "        xmin=ax.xaxis.get_data_interval()[0],\n",
    "        xmax=ax.xaxis.get_data_interval()[1],\n",
    "        linestyles=\"dashed\",\n",
    "        lw=1,\n",
    "    )\n",
    "    ax.hlines(\n",
    "        0,\n",
    "        xmin=ax.xaxis.get_data_interval()[0],\n",
    "        xmax=ax.xaxis.get_data_interval()[1],\n",
    "        linestyles=\"dashed\",\n",
    "        lw=1,\n",
    "    )\n",
    "    ax.set_ylabel(\"Probability of default\")\n",
    "    ax.set_xlabel(\"Balance\")\n",
    "    ax.set_yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "    ax.set_xlim(xmin=-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the output figure above, for a binary response with a 0/1 coding, although the linear regression model can also produce an estimate of $\\mathbb{P}(y|x)$, some estimates might be outside the $[0, 1]$ interval, making them hard to be interpreted as probabilities. Since the predicted outcome of linear regression is not a probability, but a linear interpolation between points, there is no meaningful threshold at which you can distinguish one class from the other. \n",
    "\n",
    "An additional illustration of this issue is available on [Stackoverflow](https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression).\n",
    "\n",
    "### Reasons not to perform classification using a regression method\n",
    "\n",
    "There are two main reasons not to perform classification using a regression method:\n",
    "- **Label coding**: Difficulty in coding the response variable with more than two classes;\n",
    "- **Interpretation**: Difficulty in interpreting the predicted outcome of linear regression as a probability and thus in choosing a threshold to distinguish one class from the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. All the following exercises involve the use of the **[Weekly](https://github.com/pykale/transparentML/blob/main/data/Weekly.csv)** dataset.\n",
    "\n",
    "Load the dataset, convert the **Direction** categories to numerical values, and inspect the first $10$ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Weekly.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "df[\"Direction1\"] = df.Direction.factorize()[0]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. Visualise the relationships between **Volume** and **Today** with the **Direction** using a box plot. **Hint**: See section [3.1.1](https://pykale.github.io/transparentML/03-logistic-reg/regress-to-classify.html#what-is-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "gs = GridSpec(1, 2)\n",
    "ax2 = plt.subplot(gs[0, -2])\n",
    "ax3 = plt.subplot(gs[0, -1])\n",
    "\n",
    "\n",
    "c_palette = {\"Down\": \"lightblue\", \"Up\": \"orange\"}\n",
    "sns.boxplot(data=df, y=\"Volume\", x=\"Direction\", orient=\"v\", ax=ax2, palette=c_palette)\n",
    "sns.boxplot(data=df, y=\"Today\", x=\"Direction\", orient=\"v\", ax=ax3, palette=c_palette)\n",
    "gs.tight_layout(plt.gcf())\n",
    "\n",
    "# From the left-hand panel, it appears that it is very tough to predict the dicrection from the volume\n",
    "# as the mean value of Volume for both's classes of the direction is almost same.\n",
    "# From the right-hand panels, it appears that positive today's value indicate the up direction and negative value indicate down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**. Use a linear regression model for binary classification on the **[Weekly](https://github.com/pykale/transparentML/blob/main/data/Weekly.csv)** dataset and explain why you should not perform classification using a regression method. **Hint**: See section [3.1.2.2](https://pykale.github.io/transparentML/03-logistic-reg/regress-to-classify.html#will-the-binary-case-work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "sns.regplot(\n",
    "    x=df.Today,\n",
    "    y=df.Direction1,\n",
    "    order=1,\n",
    "    ci=None,\n",
    "    scatter_kws={\"color\": \"orange\"},\n",
    "    line_kws={\"color\": \"lightblue\", \"lw\": 2},\n",
    "    ax=ax1,\n",
    ")\n",
    "\n",
    "ax1.hlines(\n",
    "    1,\n",
    "    xmin=ax1.xaxis.get_data_interval()[0],\n",
    "    xmax=ax1.xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "ax1.hlines(\n",
    "    0,\n",
    "    xmin=ax1.xaxis.get_data_interval()[0],\n",
    "    xmax=ax1.xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dashed\",\n",
    "    lw=1,\n",
    ")\n",
    "ax1.set_ylabel(\"Probability of species\")\n",
    "ax1.set_xlabel(\"sepal\")\n",
    "\n",
    "# As shown in the output of this cell, for a binary response with a 0/1 coding, although the linear regression model can also\n",
    "# produce an estimate of P(y|x), some estimates might be outside the [0, 1] interval, making them hard to be interpreted as probabilities.\n",
    "# Since the predicted outcome of linear regression is not a probability, but a linear interpolation between points,\n",
    "# there is no meaningful threshold at which you can distinguish one class from the other.\n",
    "# Thats why you should not perform classification using a regression method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
