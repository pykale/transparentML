{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "Hypothesis testing has been mentioned briefly in Chapter 2 {doc}`Linear regression <../02-linear-reg/overview>`, but it has not been explained in detail. This chapter will explain the basics of hypothesis testing and how it can be used to conduct _inference_.\n",
    "\n",
    "## Inference via hypothesis testing\n",
    "\n",
    "Hypothesis testing is a way to make inferences about a population based on a sample (of the population). Inference is the process of using sample data to make conclusions about a population. Inference is a key part of data science because we often do not have access to the entire population of interest. Instead, we have a sample of the population. Inference allows us to make conclusions about the population based on the sample.\n",
    "\n",
    "A statistical hypothesis test answers simple \"yes-or-no\" questions about data, to decide whether the data at hand sufficiently support a particular hypothesis, for example\n",
    "\n",
    "- Q1. Is the coefficient $\\beta_d$ in a linear regression of $y$ onto $x_1, . . . , x_D$ equal to zero?\n",
    "- Q2. Is there a difference in the mean blood pressure of laboratory mice in the control group and laboratory mice in the treatment group?\n",
    "\n",
    "As a formal procedure for testing a hypothesis, the hypothesis test is based on a null hypothesis and an alternative hypothesis. The null hypothesis is a statement about the population that is assumed to be true. The alternative hypothesis is a statement about the population that is assumed to be false. The null hypothesis is often denoted by $H_0$ and the alternative hypothesis is often denoted by $H_1$.\n",
    "\n",
    "Watch the 14-minute video below for a visual explanation of hypothesis testing in the context of testing drugs for treatment.\n",
    "\n",
    "```{admonition} Video\n",
    "\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/0oc49DyA3hU?start=16\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explaining Hypothesis Testing by StatQuest](https://www.youtube.com/embed/0oc49DyA3hU?start=16), embedded according to [YouTube's Terms of Service](https://www.youtube.com/static?gl=CA&template=terms).\n",
    "\n",
    "```\n",
    "\n",
    "## Four steps of hypothesis testing\n",
    "\n",
    "The four steps of hypothesis testing are:\n",
    "\n",
    "1. Define the null hypothesis and the alternative hypothesis.\n",
    "2. Construct a test statistic that summarizes the strength of evidence against the null hypothesis.\n",
    "3. Compute a $p$-value that quantifies the probability of having obtained a comparable or more extreme value of the test statistic under the null hypothesis.\n",
    "4. Decide whether to reject the null hypothesis based on the $p$-value.\n",
    "\n",
    "### Step 1: Define the null and alternative hypotheses\n",
    "\n",
    "In hypothesis testing, we consider two possibilities: the null hypothesis and the alternative hypothesis. The null hypothesis $H_0$ is the _default_ state of belief about the world. For example, the null hypotheses\n",
    "associated with the two questions Q1 and Q2 above are:\n",
    "\n",
    "- Q1. The coefficient $\\beta_d$ in a linear regression of $y$ onto $x_1, . . . , x_D$ is equal to zero.\n",
    "- Q2. There is no difference between the mean blood pressure of mice in the control and treatment groups.\n",
    "\n",
    "The alternative hypothesis $H_1$ is the _opposite_ of the null hypothesis, representing something different and often more interesting to us, e.g., there is a difference between the mean blood pressure of the mice in the two groups.\n",
    "\n",
    "Typically, we focus on using data to reject $H_0$, if there is sufficient evidence in favor of $H_1$. We\n",
    "can consider rejecting $H_0$ as making a _discovery_ about our data, i.e., we are discovering that $H_0$ does not hold! However, if we fail to reject $H_0$, we are not making a discovery, but we are not necessarily saying that $H_0$ is true. We are simply saying that we do not have sufficient evidence to reject $H_0$, since there can be multiple reasons for failing to reject $H_0$, e.g., the null hypothesis is true, or the sample size is too small, or the test statistic is not sensitive enough to detect a difference between the two groups.\n",
    "\n",
    "### Step 2: Construct a test statistic\n",
    "\n",
    "Next, we use our data to find evidence for or against the null hypothesis. A [test statistic](https://en.wikipedia.org/wiki/Test_statistic), often denoted by $T$, is a function of the data that summarises the strength of evidence against the null hypothesis. Such function is often constructed from the sample mean and the sample standard deviation.\n",
    "\n",
    "For example, denote the blood pressure measurements for the $D_t$ mice in the treatment group as $x^t_1, \\cdots, x^t_{D_t}$ and the blood pressure measurements for the $D_c$ mice in the control group as $x^c_1, \\cdots, x^c_{D_c}$. Denote their means as $\\mu_t$ and $\\mu_c$, respectively. Then, the test statistic for the two-sample $t$-test for testing $H_0: \\mu_t = \\mu_c$ versus $H_1: \\mu_t \\neq \\mu_c$ is\n",
    "\n",
    "$$\n",
    "T = \\frac{\\hat{\\mu}_t - \\hat{\\mu}_c}{\\sqrt{\\frac{s^2_t}{D_t} + \\frac{s^2_c}{D_c}}},\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mu}_t$ and $\\hat{\\mu}_c$ are the sample means of the treatment and control groups, respectively, and $s^2_t$ and $s^2_c$ are the sample variances of the treatment and control groups, respectively.\n",
    "\n",
    "```{admonition} How to interpret this statistic?\n",
    ":class: tip, dropdown\n",
    "This test statistic $T$ is a measure of the difference between the two sample means, scaled by the standard deviation of the two samples. The larger the test statistic $T$, the more evidence there is against the null hypothesis $H_0$ (and in support of the alternative hypothesis $H_1$). The smaller the test statistic $T$, the more evidence there is in favor of the null hypothesis $H_0$ (and against the alternative hypothesis $H_1$).\n",
    "```\n",
    "\n",
    "The one-sample $t$-test for testing $H_0: \\mu = \\mu_0$ versus $H_1: \\mu \\neq \\mu_0$ is\n",
    "\n",
    "$$\n",
    "T = \\frac{\\hat{\\mu} - \\mu_0}{\\frac{s}{\\sqrt{D}}},\n",
    "$$\n",
    "\n",
    "where $\\mu_0$ is the hypothesised (population) mean, $\\hat{\\mu}$ is the sample mean, and $s$ is the sample standard deviation.\n",
    "\n",
    "The one-sample $t$-test compares one sample mean to a null hypothesis value. The two-sample $t$-test compares two sample means to each other, i.e. considering two independent groups. The paired $t$-test compares two sample means to each other, but the two groups are related to each other, i.e. \"paired\", e.g., the same mice before and after treatment. The paired $t$-test simply calculates the difference between paired groups and then performs the one-sample $t$-test on the differences.\n",
    "\n",
    "### Step 3: Compute a $p$-value\n",
    "\n",
    "The test statistic above is typically used to compute the [$p$-value](https://en.wikipedia.org/wiki/P-value), which is the probability of obtaining a test statistic at least as extreme as the one observed, assuming that the null hypothesis is true. The $p$-value is a measure of the strength of evidence against the null hypothesis. The smaller the $p$-value, the more evidence there is against the null hypothesis. The larger the $p$-value, the more evidence there is in favor of the null hypothesis.\n",
    "\n",
    "Watch the 11-minute video below for a visual explanation of $p$-value in the context of testing drugs for treatment.\n",
    "\n",
    "```{admonition} Video\n",
    "\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/vemZtEM63GY?start=11\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "[Explanation and interpretation of $p$-value by StatQuest](https://www.youtube.com/embed/vemZtEM63GY?start=11), embedded according to [YouTube's Terms of Service](https://www.youtube.com/static?gl=CA&template=terms).\n",
    "```\n",
    "\n",
    "```{admonition} How to interpret the $p$-value?\n",
    ":class: tip, dropdown\n",
    "The $p$-value is as the fraction of the time that we would expect to see such an extreme value of the test\n",
    "statistic if we repeated the experiment many many times, provided that the null hypothesis is true. If the $p$-value is small, then we would expect to see such an extreme value of the test statistic only a small fraction of the time, if we repeated the experiment many many times, provided that the null hypothesis is true. In this case, we would have strong evidence against the null hypothesis. If the $p$-value is large, then we would expect to see such an extreme value of the test statistic a large fraction of the time, if we repeated the experiment many many times, provided that the null hypothesis is true. In this case, we would have weak evidence against the null hypothesis.\n",
    "```\n",
    "\n",
    "In Step 2, we pointed out that a large (absolute) value of the test statistic provides evidence against $H_0$. Suppose\n",
    "a data analyst conducts a statistical test, and reports a test statistic of $T$ = 17.3. Does this provide strong evidence against $H_0$? It’s impossible to know without more information: we would need to know what value of the test statistic should be expected, under $H_0$, if the data analyst had repeated the experiment many times. If the data analyst had repeated the experiment many times, and the test statistic was typically around 0, then a value of 17.3 would be very unusual, and we would have strong evidence against $H_0$. However, if the data analyst had repeated the experiment many times, and the test statistic was typically around 10, then a value of 17.3 would be less unusual, and we would have weaker evidence against $H_0$.\n",
    "\n",
    "This is exactly what a $p$-value measures. A $p$-value allows us to transform (or standardise) our test statistic, which is measured on some arbitrary and uninterpretable scale, into a number between 0 and 1 that can be _more easily interpreted_.\n",
    "\n",
    "### Step 4: Decide whether to reject the null hypothesis $H_0$\n",
    "\n",
    "Finally, we decide whether to reject $H_0$ or not (we do not usually talk about \"accepting\" $H_0$: instead, we talk about \"failing to reject\" $H_0$). We reject $H_0$ if the $p$-value is less than a pre-specified significance level $\\alpha$. The significance level $\\alpha$ is a number between 0 and 1 that we choose before we conduct the statistical test. We typically choose $\\alpha$ = 0.05 or $\\alpha$ = 0.01, which is a common convention in the scientific community (with 0.05 being more common). If the $p$-value is less than $\\alpha$, then we reject $H_0$. If the $p$-value is greater than or equal to $\\alpha$, then we _fail to reject_ $H_0$. Furthermore, a data analyst\n",
    "should typically report the $p$-value itself, rather than just whether or not it exceeds a specified threshold value.\n",
    "\n",
    "```{admonition} How to interpret the significance level $\\alpha$?\n",
    ":class: tip, dropdown\n",
    "The significance level $\\alpha$ is the probability of rejecting $H_0$ when it is true. For example, if $\\alpha$ = 0.05, then there is a 5% chance of rejecting $H_0$ when it is true. In other words, if $H_0$ holds, we would\n",
    "expect to see such a small $p$-value no more than 5% of the time. On the other hand, there is a 95% chance of _not_ rejecting $H_0$ when it is true. Nothing is absolute here.\n",
    "```\n",
    "\n",
    "## Type I and Type II errors\n",
    "\n",
    "### Definition\n",
    "\n",
    "In the context of hypothesis testing, we can distinguish between two types of errors: Type I errors and Type II errors. A Type I error occurs when we reject $H_0$ when it is true. A Type II error occurs when we fail to reject $H_0$ when it is false. A Type I error is also known as a _false positive_, and a Type II error is also known as a _false negative_. A summary of the possible scenarios associated with testing the null hypothesis $H_0$ is shown in table below.\n",
    "\n",
    "```{list-table} Possible scenarios associated with testing the null hypothesis $H_0$ (equivalent to Table 13.1 of the textbook). Type I errors are also known as false positives, and Type II errors as false negatives.\n",
    ":header-rows: 1\n",
    ":align: center\n",
    ":widths: \"auto\"\n",
    ":name: table-13-1\n",
    "* -\n",
    "  - Reject $H_0$\n",
    "  - Fail to reject $H_0$\n",
    "* - $H_0$ is true\n",
    "  - Type I error\n",
    "  - Correct decision\n",
    "* - $H_0$ is false\n",
    "  - Correct decision\n",
    "  - Type II error\n",
    "```\n",
    "\n",
    "The table above summarises the possible scenarios associated with testing the null hypothesis $H_0$. The first row of the table shows the two possible outcomes of the statistical test (that we know after performing the test): either we reject $H_0$ or we fail to reject $H_0$. The first column of the table shows the two possible ground-truth values of $H_0$ (that we do not know): either $H_0$ is true or $H_0$ is false. The four cells in the table show the four possible scenarios that can occur when we test $H_0$.\n",
    "\n",
    "If we reject $H_0$ when it is true, then we have made a Type I error. If we fail to reject $H_0$ when it is false, then we have made a Type II error. If we reject $H_0$ when it is false, then we have made a correct decision. If we fail to reject $H_0$ when it is true, then we have made a correct decision too.\n",
    "\n",
    "The _Type I error rate_ is defined as the probability of making a Type I error given that $H_0$ is true, i.e., the probability of incorrectly rejecting $H_0$. The power of the hypothesis test is defined as the probability of NOT making a Type II error given that $H_1$ holds, i.e., the probability of correctly rejecting $H_0$.\n",
    "\n",
    "\n",
    "```{admonition} Connections to classification\n",
    ":class: note\n",
    "The Type I error rate is equivalent to the false positive rate in binary classification, i.e. predicting a positive (non-null) label when the true label is in fact negative (null). The power of the hypothesis test is equivalent to the true positive rate in classification.\n",
    "```\n",
    "\n",
    "### Trade-off between Type I and Type II errors\n",
    "\n",
    "Ideally we would like both the Type I and Type II errors to be small. But there typically is a trade-off: we can make the Type I error small by only rejecting $H_0$ when we have strong evidence against it, but this will increase the Type II error. We can make the Type II error small by rejecting $H_0$ even when we have weak evidence against it, but this will increase the Type I error.\n",
    "\n",
    "The significance level $\\alpha$ is a trade-off between the Type I and Type II errors. If we choose a small value of $\\alpha$, then we will have strong evidence against $H_0$ when we reject it, but this will increase the Type II error. If we choose a large value of $\\alpha$, then we will have weak evidence against $H_0$ when we reject it, but this will increase the Type I error. By only rejecting $H_0$ when the $p$-value is below $\\alpha$, we ensure that the Type I error\n",
    "rate will be less than or equal to $\\alpha$.\n",
    "\n",
    "In practice, we typically view Type I errors, i.e. false positives, as more \"serious\" than Type II errors, because the former involves declaring a scientific finding that is not correct, which is more serious than failing to declare a scientific finding (false negatives). Therefore, when we perform hypothesis testing, we typically require a low Type I error rate — e.g. at most $\\alpha$ = 0.05 — while trying to make the Type II error small (or, equivalently, the power large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing on synthetic data\n",
    "\n",
    "Let us perform some one-sample $t$-tests on synthetic data for this study.\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as st\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed for reproducibility (more on this later in the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create 100 variables, each with 10 observations (samples). We make the first 50 variables to have mean 0.5 (the `offset`) and variance 1, and the last 50 variables to have mean 0 and variance 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(loc=0.0, scale=1.0, size=(10, 100))\n",
    "offset = 0.5\n",
    "X[:, :50] = X[:, :50] + offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform one-sample $t$-tests of the null hypothesis that the population mean (`popmean`) is zero $H_0: \\mu_i = 0$ for all variables $i=1, \\cdots, 100$ using the [`ttest_1samp`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html) function from the `scipy.stats` module. The function returns the $t$-statistic and the $p$-value for each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = st.ttest_1samp(a=X, popmean=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the $t$-statistic and $p$-value for the first and the 60th variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variable 0 t-statistic: \", results.statistic[0])\n",
    "print(\"Variable 0 p-value: \", results.pvalue[0])\n",
    "print(\"Variable 59 t-statistic: \", results.statistic[59])\n",
    "print(\"Variable 59 p-value: \", results.pvalue[59])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $p$-value for the first variable is 0.25, which is greater than the significance level $\\alpha$ = 0.05. Therefore, we fail to reject the null hypothesis $H_0$ for the first variable. We know the true mean of the first variable is 0.5, so the null hypothesis is false. Thus, this is a Type II error.\n",
    "\n",
    "The $p$-value for the 60th variable is 0.99. We also fail to reject the null hypothesis $H_0$. However, we know the true mean of the 60th variable is 0, so the null hypothesis is true. Thus, this is a correct decision.\n",
    "\n",
    "We can plot the $p$-values of the $t$-tests as a histogram to see the distribution of the $p$-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(results.pvalue, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We set the significance level $\\alpha$ to 0.05 to make a decision on whether to reject $H_0$ or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = results.pvalue\n",
    "decisions = []\n",
    "for i in range(len(p_values)):\n",
    "    if p_values[i] < 0.05:\n",
    "        decisions.append(\"Reject H0\")\n",
    "    else:\n",
    "        decisions.append(\"Fail to reject H0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the ground truth to evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = np.repeat([\"Reject H0\", \"Fail to reject H0\"], [50, 50], axis=0)\n",
    "labels = [\"Reject H0\", \"Fail to reject H0\"]\n",
    "cm = confusion_matrix(ground_truth, decisions, labels=labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise this confusion matrix using the [`heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) function from the `seaborn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_labels = [\"H0 is False\", \"H0 is True\"]\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=ground_truth_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite some errors made. We can make the offset larger (from 0.5 to 1) to see the change to the confusion matrix and the total number of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 1\n",
    "decisions = []\n",
    "X[:, :50] = X[:, :50] + offset\n",
    "results = st.ttest_1samp(a=X, popmean=0)\n",
    "p_values = results.pvalue\n",
    "for i in range(len(p_values)):\n",
    "    if p_values[i] < 0.05:\n",
    "        decisions.append(\"Reject H0\")\n",
    "    else:\n",
    "        decisions.append(\"Fail to reject H0\")\n",
    "cm = confusion_matrix(ground_truth, decisions, labels=labels)\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=ground_truth_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this time, there is only one error in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple hypothesis testing\n",
    "\n",
    "Let us consider the more complicated case where we wish to test $M$ null hypotheses, $H_0^1, H_0^2, \\ldots, H_0^M$.\n",
    "\n",
    "### Case study: a \"sure-win\" stockbroker?\n",
    "\n",
    "A stockbroker wishes to drum up new clients by convincing them of his/her trading acumen. S/he tells 1,024 (i.e. $2^{10}$) potential new clients that s/he can correctly predict whether Apple’s stock price will increase or decrease for 10 days running. For such a binary outcome, we have $2^{10}=1024$ possibilities for the course of these 10 days. Therefore, s/he emails each client one of these 1024 possibilities. Although the vast majority of his/her potential clients will find that his/her predictions are no better than chance, one of his/her potential clients will be really impressed to find that his/her predictions were correct for _ALL 10 of the days!_ And so the stockbroker gains a new client (and possible more when this client spreads the news).\n",
    "\n",
    "This is _part of the reason why we receive so many spam emails/calls_. If you make a lot of guesses (\"predictions\"), then you are bound to get some right by chance.\n",
    "\n",
    "### The challenge of multiple hypothesis testing\n",
    "\n",
    "Likewise, if we flip 1,024 fair coins ten times each. Then we would expect (on average) one coin to come up all tails. If one of our coins comes up all tails, then we might therefore conclude that this particular coin is not fair. But it would be incorrect to conclude that the coin is not fair: in fact, the null hypothesis holds, and we just happen to have gotten ten tails in a row by chance.\n",
    "\n",
    "The examples above demonstrate the main challenge of multiple testing: when testing a huge number of null hypotheses, we are bound to get some very small $p$-values by chance. If we make a decision about whether to reject each null hypothesis without accounting for the fact that we have performed a very large number of tests, then we may end up rejecting a great number of true null hypotheses, i.e. making a large number of Type I errors (false positives), which is what we hope to avoid (see the above).\n",
    "\n",
    "```{admonition} Why repeat the experiment?\n",
    ":class: tip, dropdown\n",
    "The reason why we repeat the experiment is to reduce the chance of getting a false positive. If we only perform the experiment once, then we will have a very small chance of getting a false positive.\n",
    "```\n",
    "\n",
    "### Family-wise error rate (FWER)\n",
    "\n",
    "Rejecting a null hypothesis if the $p$-value is below $\\alpha$ controls the probability of falsely rejecting that null hypothesis at level $\\alpha$. However, if we do this for $M$ null hypotheses, then the chance of falsely rejecting _at least one_ of the M null hypotheses is quite a bit higher! This is because the chance of getting a false positive for each null hypothesis is $\\alpha$, but the chance of getting a false positive for _at least one_ of the $M$ null hypotheses is $1-(1-\\alpha)^M$.\n",
    "\n",
    "Let us see this visually. We will plot the chance of getting a false positive for _at least one_ of the $M$ null hypotheses as a function of $\\alpha$ and $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = range(500)\n",
    "fwe1 = list(map(lambda x: 1 - pow(1 - 0.05, x), m))\n",
    "fwe2 = list(map(lambda x: 1 - pow(1 - 0.01, x), m))\n",
    "fwe3 = list(map(lambda x: 1 - pow(1 - 0.001, x), m))\n",
    "\n",
    "plt.plot(m, fwe1, label=\"alpha=0.05\")\n",
    "plt.plot(m, fwe2, label=\"alpha=0.01\")\n",
    "plt.plot(m, fwe3, label=\"alpha=0.001\")\n",
    "plt.xlabel(\"M: Number of tests in log scale\")\n",
    "plt.ylabel(\"Family-wise error rate\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that setting $\\alpha = 0.05$ results in a high FWER even for moderate $M$. With $\\alpha = 0.01$, we can test no more than five null hypotheses before the FWER exceeds 0.05. Only for very small values, such as $\\alpha = 0.001$, we can manage to ensure a small FWER for moderate values of $M$.\n",
    "\n",
    "Of course, the problem with setting $\\alpha$ to such a low value is that we are likely to make a large number of Type II errors: in other words, our power is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this challenge of multiple testing, we hope to test multiple hypotheses while controlling the probability of making at least one Type I error. This can be achieved by controlling the [family-wise error rate (FWER)](https://en.wikipedia.org/wiki/Family-wise_error_rate), which is the probability of making at least one Type I error when testing $M$ null hypotheses, i.e. rejecting at least one null hypothesis when all $M$ null hypotheses are true. The FWER is also known as the _probability of a false discovery_.\n",
    "\n",
    "One solution is to adjust the significance level $\\alpha$ for each null hypothesis. The [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction) is a simple way to adjust the significance level $\\alpha$ for multiple testing. This method divides the significance level $\\alpha$ by the number of null hypotheses $M$ to obtain the _family-wise significance level_ $\\alpha_M$. The Bonferroni correction is conservative, i.e. it ensures that the FWER is at most $\\alpha_M$. However, it also reduces the power of the test, i.e. the chance of rejecting a false null hypothesis.\n",
    "\n",
    "Let us study the family-wise error rate (FWER) and Bonferroni correction on the [Fund dataset](https://github.com/pykale/transparentML/raw/main/data/Fund.csv) (click to explore).\n",
    "\n",
    "Load the data first and display some essential information first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_url = \"https://github.com/pykale/transparentML/raw/main/data/Fund.csv\"\n",
    "\n",
    "fund_df = pd.read_csv(fund_url, na_values=\"?\").dropna()\n",
    "fund_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first few rows of the data, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do a one-sample $t$-test for the first manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = st.ttest_1samp(a=fund_df[\"Manager1\"], popmean=0)\n",
    "print(result.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a one-sample $t$-test for five managers instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = []\n",
    "manager_number = 5\n",
    "\n",
    "for i in range(manager_number):\n",
    "    result = st.ttest_1samp(a=fund_df.iloc[:, i], popmean=0)\n",
    "    p_values.append(result.pvalue)\n",
    "\n",
    "print(p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $p$-values are low for Managers One and Three, and high for the other three managers. However, we cannot simply reject $H_0^1$ and $H_0^3$, since this would fail to account for the multiple testing that we have performed. Instead, we can use the Bonferroni correction to adjust the significance level $\\alpha$ for multiple testing, using the `multipletests` function from the `statsmodels` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, p_values_corrected, alphacSidak, alphacBonf = multipletests(\n",
    "    p_values, method=\"bonferroni\"\n",
    ")\n",
    "print(p_values_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, using the Bonferroni correction, we will reject the null hypothesis only for Manager One while controlling the FWER at 0.05. This information is also available in the variable reject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is reasonable to control the FWER when $M$ takes on a small value, like 5 or 10. However, for $M$ = 100 or 1,000, attempting to control the FWER will make it almost impossible to reject any of the false null hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False discovery rate\n",
    "\n",
    "In practice, when $M$ is large, trying to prevent any false positives (as in FWER control) is simply too stringent and scientifically uninteresting. In this case, we can tolerate a few false positives, in the interest of making more discoveries, i.e. more rejections of the null hypothesis. Thus, we typically control the [false discovery rate (FDR)](https://en.wikipedia.org/wiki/False_discovery_rate), which is the expected proportion of rejected null hypotheses that are false. The FDR is also known as the _expected proportion of false discoveries_. \n",
    "\n",
    "The Benjamini-Hochberg procedure is a popular method for controlling the FDR. This method controls the FDR at level $\\alpha$ by rejecting the null hypothesis for the $k$th smallest $p$-value if $p_k \\leq \\alpha k/M$, where $M$ is the number of null hypotheses. The Benjamini-Hochberg procedure is less conservative than the Bonferroni correction, i.e. it allows for more false positives. However, it also increases the power of the test, i.e. the chance of rejecting a false null hypothesis.\n",
    "\n",
    "The FDR is typically more useful than the FWER when $M$ is large. The use of the FDR also aligns well with the way that data are often collected in contemporary applications, e.g. in [exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis) and [genomics](https://en.wikipedia.org/wiki/Genomics).\n",
    "\n",
    "Let us study the false discovery rate (FDR) on the [Fund dataset](https://github.com/pykale/transparentML/raw/main/data/Fund.csv) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = []\n",
    "manager_number = fund_df.shape[1]\n",
    "\n",
    "for i in range(manager_number):\n",
    "    result = st.ttest_1samp(a=fund_df.iloc[:, i], popmean=0)\n",
    "    p_values.append(result.pvalue)\n",
    "\n",
    "print(p_values[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are far too many managers to consider if we try to control the FWER. Instead, we focus on controlling the FDR: that is, the expected fraction of rejected null hypotheses that are actually false positives. We can use the same `multipletests` function from the `statsmodels` module to control the FDR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject, p_values_corrected, alphacSidak, alphacBonf = multipletests(\n",
    "    p_values, method=\"fdr_bh\"\n",
    ")\n",
    "print(p_values_corrected[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $p$-values output by the Benjamini-Hochberg procedure can be interpreted as the smallest FDR threshold at which we would reject a particular null hypothesis. For instance, a $p$-value of 0.1 indicates that we can reject the corresponding null hypothesis at an FDR of 10% or greater, but that we cannot reject the null hypothesis at an FDR below 10%. \n",
    "\n",
    "We would find that 146 of the 2,000 fund managers have a corrected $p$-value below 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p_values_corrected <= 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use bonferroni method, we will find None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.array(p_values) <= 0.1 / fund_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike $p$-values, the choice of FDR threshold is typically context-dependent (e.g. cost/budget-dependent), or even dataset-dependent, with no standard accepted threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. The **alternative hypothesis** is a statement about the population that is assumed to be true. \n",
    "\n",
    "       a. True\n",
    "\n",
    "       b. False\n",
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**b. False. The null hypothesis is assumed to be true, and we are carrying out the experiment to see if we can reject the null hypothesis in favour of the alternative hypothesis.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**.  The larger the $p$-value, the more evidence there is in favor of the **null hypothesis**.\n",
    "\n",
    "       a. True\n",
    "\n",
    "       b. False\n",
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "   **a. True**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**. A **test statistic** allows us to standardise our $p$-value.\n",
    "\n",
    "       a. True\n",
    "\n",
    "       b. False\n",
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "   **b. False. A $p$-value allows us to transform (or standardise) our test statistic, which is measured on some arbitrary and uninterpretable scale, into a number between 0 and 1 that can be more easily interpreted.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4**. A **Type II** error occurs when we fail to reject $H_0$ when it is false.\n",
    "\n",
    "       a. True\n",
    "\n",
    "       b. False\n",
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "   **a. True**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5**. When we perform hypothesis testing, we typically require a **low Type II error rate** while trying to make the **Type I error small**.\n",
    "\n",
    "       a. True\n",
    "\n",
    "       b. False\n",
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "   **b. False. We require high Type II error to make the Type I error small**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6**. All the following exercises involve the use of the [Weekly](https://github.com/pykale/transparentML/blob/main/data/Weekly.csv) dataset.\n",
    "\n",
    "**a**. Load the dataset and inspect the first few rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Weekly.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Perform a one-sample $t$-tests of the null hypothesis that the population mean (popmean) is $0.05$, $H_0: \\mu_i = 0.5$ for variable **Today** from the **Weekly** dataset using the **ttest_1samp** function from the **scipy.stats** module and find out the $t$-statistic and $p$-value for the variable **Today** and state whether we can **reject** the null hypothesis for this variable.\n",
    "The $t$-test is calculated for the mean of one set of values. The null hypothesis is that the expected mean of a sample of independent observations is equal to the specified population mean, popmean = $0.5$. **Hint**: See section [4.1.4](https://pykale.github.io/transparentML/04-hypo-test-sw-dev/hypothesis-testing.html#hypothesis-testing-on-synthetic-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy import stats as st\n",
    "\n",
    "results = st.ttest_1samp(a=df[\"Today\"], popmean=0.5)\n",
    "print(\"Variable Volume t-statistic: \", results.statistic)\n",
    "print(\"Variable Volume p-value: \", results.pvalue)\n",
    "\n",
    "# The p-value for the Today variable is 1.09e-06, which is smaller than the significance level alpha= 0.05.\n",
    "# Therefore, we can reject the null hypothesis and accept the alternative hypothesis for the Today variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. Now, do another one-sample $t$-test of the null hypothesis that the population mean (popmean) is $0.05$, $H_0: \\mu_i = 0.5$ for all the **lagging indicator** variables **(Lag1, Lag2, Lag3, Lag4, Lag5)** from the **Weekly** dataset and show all the **$p$-values**. **Hint**: See section [4.1.5](https://pykale.github.io/transparentML/04-hypo-test-sw-dev/hypothesis-testing.html#multiple-hypothesis-testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "p_values = []\n",
    "lag_number = 6\n",
    "\n",
    "for i in range(1, lag_number):\n",
    "    result = st.ttest_1samp(a=df[\"Lag\" + str(i)], popmean=0.5)\n",
    "    p_values.append(result.pvalue)\n",
    "print(p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d**. According to the resulting $p$-values from **Exercise 6(c)**, can we reject all the null hypotheses as the $p$-values are low?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "The $p$-values are low for all the lagging indicators. However, we cannot simply reject $H_{0}^{1}$, $H_{0}^{2}$, $H_{0}^{3}$, $H_{0}^{4}$and $H_{0}^{5}$, since this would fail to account for the multiple testing that we have performed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e**. Now, use the **Bonferroni correction** to adjust the significance level $\\alpha$ for multiple testing performed in **Exercise 6(c)** using the **multipletests** function from the **statsmodels** module and state which null hypothesis we can reject. **Hint**: See section [4.1.5.2](https://pykale.github.io/transparentML/04-hypo-test-sw-dev/hypothesis-testing.html#family-wise-error-rate-fwer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the reference solution below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "reject, p_values_corrected, alphacSidak, alphacBonf = multipletests(\n",
    "    p_values, method=\"bonferroni\"\n",
    ")\n",
    "print(p_values_corrected)\n",
    "print(reject)\n",
    "\n",
    "# Therefore, using the Bonferroni correction, we will reject all the null hypothesis while controlling the FWER at 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f**. If we want to test **$100$ null hypotheses**, can we control FWER using Bonferroni correction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare your answer with the solution below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "It is reasonable to control the FWER when $M$ (number of the null hypothesis) takes on a small value, like $5$ or $10$. However, for  $M= 100$ or $1,000$, attempting to control the FWER will make it almost impossible to reject any of the false null hypotheses. In that case, we control the false discovery rate (FDR) using the Benjamini-Hochberg procedure rather than the FWER.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
