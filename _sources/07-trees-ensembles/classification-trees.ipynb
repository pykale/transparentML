{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification trees\n",
    "\n",
    "Classification trees are very similar to regression trees, except that the target variable is categorical. In regression trees, we used the mean of the target variable in each region as the prediction. In classification trees, we use the most common class in each region as the prediction. Besides the most common class, we are also interesting in the proportion of each class in each region. This is because the proportion of each class in each region is a measure of the **purity** of the region. A region is pure if it contains only one class. A region is impure if it contains multiple classes.\n",
    "\n",
    "## Gini index\n",
    "\n",
    "Although classification accuracy is a good measure for classification performance, a popular cost function (splitting criterion) for classification trees is the [**Gini index**](https://en.wikipedia.org/wiki/Gini_coefficient), a measure of the impurity of a region (not just feature regions in machine learning, but also regions in society often heard in news). This is because classification accuracy is not sufficiently sensitive for building classification trees. \n",
    "\n",
    "The Gini index is defined as:\n",
    "\n",
    "$$\n",
    "G = 1 - \\sum_{c=1}^C p_c^2 = \\sum_{c=1}^C p_c (1 - p_c),\n",
    "$$\n",
    "\n",
    "where $p_c$ is the proportion of class $c$ in the region. The Gini index is zero if the region is pure, and is one if the region is impure. The Gini index is a measure of the probability of misclassification. The Gini index is also known as the **Gini impurity**.\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Another popular cost function (splitting criterion) for classification trees is the [**entropy**](https://en.wikipedia.org/wiki/Entropy_(information_theory)), which is defined as:\n",
    "\n",
    "$$\n",
    "H = - \\sum_{c=1}^C p_c \\log_2 p_c,\n",
    "$$\n",
    "\n",
    "where $p_c$ is the proportion of class $c$ in the region. The entropy is zero if the region is pure, and is one if the region is impure. \n",
    "\n",
    "When building classification trees, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, and the split that produces the lowest cost is chosen. The Gini index and the entropy are very similar, and the Gini index is slightly faster to compute.\n",
    "\n",
    "## Classification trees for heart disease diagnosis\n",
    "\n",
    "Get ready by importing the APIs needed from respective libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [Heart dataset](https://github.com/pykale/transparentML/blob/main/data/Heart.csv) (click to explore) to predict whether a patient has heart disease or not. The target variable is `AHD`, which is a binary variable that indicates whether a patient has heart disease or not. \n",
    "\n",
    "Load the dataset and _remove the rows with missing values_. Inspect the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_url = \"https://github.com/pykale/transparentML/raw/main/data/Heart.csv\"\n",
    "\n",
    "heart_df = pd.read_csv(heart_url).dropna()\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`factorize` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html) from the `pandas` library to convert categorical variables to numerical variables. This is because the `sklearn` library only accepts numerical variables. `ChestPain` is a categorical variable that indicates the type of chest pain. `Thal` is a categorical variable that indicates the type of thalassemia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.ChestPain = pd.factorize(heart_df.ChestPain)[0]\n",
    "heart_df.Thal = pd.factorize(heart_df.Thal)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the feature matrix `X` and the target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_df.drop(\"AHD\", axis=1)\n",
    "y = pd.factorize(heart_df.AHD)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a classification tree to the data using the `DecisionTreeClassifier` class from the `sklearn.tree` module. Set the `max_leaf_nodes` parameter to 6. This will limit the number of leaf nodes to 6. Set the `max_features` parameter to 3. This will limit the number of features to consider when looking for the best split to 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_leaf_nodes=6, max_features=3)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of the classification tree on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the classification tree using the `plot_tree` function from the `sklearn.tree` module. Set the `filled` parameter to `True` to colour the nodes in the tree according to the majority class in each region. Set the `feature_names` parameter to the list of feature names. Set the `class_names` parameter to the list of class names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    clf, filled=True, feature_names=X.columns, class_names=[\"No\", \"Yes\"], fontsize=14\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification trees for car seat sales\n",
    "\n",
    "Next, let us study the [Carseats dataset](https://github.com/pykale/transparentML/blob/main/data/Carseats.csv) (click to explore). In this dataset, we want to predict whether a car seat will be `High` or `Low` based on the `Sales` and `Price` of the car seat.\n",
    "\n",
    "Load the dataset. `Sales` is a continuous variable so we recode it as a binary variable `High` by thresholding it at 8 using the `map()` function from the `pandas` library. `High` takes on a value of 'Y' if the Sales variable exceeds 8, 'N' otherwise. We also convert categorical variables to numerical variables using the `factorize` method from the `pandas` library as above. \n",
    "\n",
    "Inspect the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats_url = \"https://github.com/pykale/transparentML/raw/main/data/Carseats.csv\"\n",
    "\n",
    "carseats_df = pd.read_csv(carseats_url)\n",
    "carseats_df[\"High\"] = carseats_df.Sales.map(lambda x: \"Y\" if x > 8 else \"N\")\n",
    "carseats_df.ShelveLoc = pd.factorize(carseats_df.ShelveLoc)[0]\n",
    "carseats_df.Urban = carseats_df.Urban.map(lambda x: 1 if x == \"Yes\" else 0)\n",
    "carseats_df.US = carseats_df.US.map(lambda x: 1 if x == \"Yes\" else 0)\n",
    "carseats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training (200 samples) and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = carseats_df.drop([\"Sales\", \"High\"], axis=1)\n",
    "y2 = carseats_df.High\n",
    "train_size = 200\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X2, y2, train_size=train_size, test_size=X.shape[0] - train_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, let us use 'entropy' as the split criterion at each node to fit a classification tree to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = \"entropy\"\n",
    "max_depth = 6\n",
    "min_sample_leaf = 4\n",
    "clf_gini = DecisionTreeClassifier(\n",
    "    criterion=criteria, max_depth=max_depth, min_samples_leaf=min_sample_leaf\n",
    ")\n",
    "clf_gini.fit(X_train, y_train)\n",
    "print(clf_gini.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the classification tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one attractive feature of a tree is visulization.\n",
    "plt.figure(figsize=(40, 20))  # customize according to the size of your tree\n",
    "plot_tree(\n",
    "    clf_gini,\n",
    "    filled=True,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=[\"No\", \"Yes\"],\n",
    "    fontsize=14,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the confusion matrix to evaluate the model in accuracy for both training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = clf_gini.predict(X_train)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_train, y_pred_train).T,\n",
    "    index=[\"No\", \"Yes\"],\n",
    "    columns=[\"No\", \"Yes\"],\n",
    ")\n",
    "print(cm)\n",
    "print(\"Train Accuracy is \", accuracy_score(y_train, y_pred_train) * 100)\n",
    "\n",
    "\n",
    "y_pred = clf_gini.predict(X_test)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred).T, index=[\"No\", \"Yes\"], columns=[\"No\", \"Yes\"]\n",
    ")\n",
    "print(cm)\n",
    "print(\"Test Accuracy is \", accuracy_score(y_test, y_pred) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy of the fitted model is significantly lower than the training accuracy, an indication of overfitting. You can go back and change the hyperparameters in the training process to reduce the dimension (flexibility) of the parameter space and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute more metrics such as precision, recall, f1-score, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** All the following exercises use the [Caravan](https://github.com/pykale/transparentML/blob/main/data/Caravan.csv).\n",
    "\n",
    "Load the **Caravan** dataset inspect the first five rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "caravan_url = \"https://github.com/pykale/transparentML/raw/main/data/Caravan.csv\"\n",
    "caravan_df = pd.read_csv(caravan_url)\n",
    "caravan_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Lets train a DecisionTreeClassifier on the 70% dataset from Exercise 1 and find out the testing accuracy and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, plot_roc_curve\n",
    "\n",
    "X2 = caravan_df.drop([\"Purchase\"], axis=1)\n",
    "y2 = caravan_df.Purchase\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.20)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_leaf_nodes=6, max_features=3)\n",
    "\n",
    "clf.fit(X_test, y_test)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuarcy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuarcy)\n",
    "\n",
    "\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC: \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Now, train another DecisionTreeClassifier using the ‘entropy’ as the split criterion at each node to fit a classification tree to the training data and find out the testing accuracy and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "X2 = caravan_df.drop([\"Purchase\"], axis=1)\n",
    "y2 = caravan_df.Purchase\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.20)\n",
    "\n",
    "criteria = \"entropy\"\n",
    "max_depth = 6\n",
    "min_sample_leaf = 4\n",
    "clf_gini = DecisionTreeClassifier(\n",
    "    criterion=criteria, max_depth=max_depth, min_samples_leaf=min_sample_leaf\n",
    ")\n",
    "clf_gini.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_gini.predict(X_test)\n",
    "accuarcy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuarcy)\n",
    "\n",
    "\n",
    "y_prob = clf_gini.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC: \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Build the confusion matrix and classification to evaluate the model trained in Exercise 2 in accuracy for test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_gini.predict(X_test)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred).T, index=[\"No\", \"Yes\"], columns=[\"No\", \"Yes\"]\n",
    ")\n",
    "print(\"Confusion matrix\\n\", cm)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
