
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9.1. Principal component analysis &#8212; Transparent ML Intro</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.2. \(K\)-means and hierarchical clustering" href="clustering.html" />
    <link rel="prev" title="9. Principal Component Analysis &amp; Clustering" href="overview.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/transparentml-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Transparent ML Intro</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Overview
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/pykale/transparentML/discussions">
   Discussion forum
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../00-prereq/overview.html">
   Prerequisites
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/linear-algebra-and-notations.html">
     Linear algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/basic-python.html">
     Python basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/numerical-programming.html">
     Numerical programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/graphics.html">
     Graphics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/loading-data.html">
     Loading data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/quiz-sum-ref.html">
     Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Primary
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01-intro/overview.html">
   1. Intro ML &amp; transparency
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/what-is-ml.html">
     1.1. What is ML?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/ml-systems.html">
     1.2. ML systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/ml-process.html">
     1.3. ML process
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/ml-transp.html">
     1.4. ML transparency
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/knn.html">
     1.5. K-NN classifier
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/organisation.html">
     1.6. Organisation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/quiz-sum-ref.html">
     1.7. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-linear-reg/overview.html">
   2. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/simple-linear-regression.html">
     2.1. Simple linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/multi-linear-regression.html">
     2.2. Multiple linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/extension-limitation.html">
     2.3. Extensions &amp; limitations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/quiz-sum-ref.html">
     2.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-logistic-reg/overview.html">
   3. Logistic regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-logistic-reg/regress-to-classify.html">
     3.1. Regress to classify?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-logistic-reg/logistic-regression.html">
     3.2. Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-logistic-reg/quiz-sum-ref.html">
     3.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-hypo-test-sw-dev/overview.html">
   4. Hypothesis test &amp; software dev
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-hypo-test-sw-dev/hypothesis-testing.html">
     4.1. Hypothesis testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-hypo-test-sw-dev/software-development.html">
     4.2. Software development
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-hypo-test-sw-dev/quiz-sum-ref.html">
     4.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-cross-val-bootstrap/overview.html">
   5. Cross validation &amp; bootstrap
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-cross-val-bootstrap/cross-validation.html">
     5.1. Cross-validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-cross-val-bootstrap/bootstrap.html">
     5.2. Bootstrap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-cross-val-bootstrap/quiz-sum-ref.html">
     5.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Secondary
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-ftr-select-regularise/overview.html">
   6. Feature selection/regularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-ftr-select-regularise/feature-select.html">
     6.1. Feature selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-ftr-select-regularise/regularisation.html">
     6.2. Regularisation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-ftr-select-regularise/quiz-sum-ref.html">
     6.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-trees-ensembles/overview.html">
   7. Trees &amp; ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/regression-trees.html">
     7.1. Regression trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/classification-trees.html">
     7.2. Classification trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/ensembles.html">
     7.3. Ensemble learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/quiz-sum-ref.html">
     7.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-glm-svm/overview.html">
   8. GLM &amp; SVM
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/glm.html">
     8.1. Generalised linear models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/support-vec-classifier.html">
     8.2. Support vector classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/svm.html">
     8.3. Support vector machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/quiz-sum-ref.html">
     8.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="overview.html">
   9. PCA &amp; clustering
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.1. Principal comp. analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering.html">
     9.2. Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="quiz-sum-ref.html">
     9.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-deep-cnn-rnn/overview.html">
   10. Neural nets &amp; deep learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-cnn-rnn/multilayer-nn.html">
     10.1. Multilayer neural nets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-cnn-rnn/convolutional-nn.html">
     10.2. Convolutional neural nets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-cnn-rnn/recurrent-nn.html">
     10.3. Recurrent neural nets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-cnn-rnn/quiz-sum-ref.html">
     10.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/system-transp.html">
   System transparency
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/process-transp.html">
   Process transparency
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/09-pca-clustering/pca.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/pykale/transparentML"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/pykale/transparentML/issues/new?title=Issue%20on%20page%20%2F09-pca-clustering/pca.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/pykale/transparentML/edit/main/content/09-pca-clustering/pca.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/pykale/transparentML/main?urlpath=tree/content/09-pca-clustering/pca.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/pykale/transparentML/blob/main/content/09-pca-clustering/pca.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-for-dimensionality-reduction">
   9.1.1. PCA for dimensionality reduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#motivation">
     9.1.1.1. Motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principal-components-pcs">
     9.1.1.2. Principal components (PCs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-the-first-pc">
     9.1.1.3. Computing the first PC
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-the-first-pc">
     9.1.1.4. Interpreting the first PC
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-the-second-pc-and-beyond">
     9.1.1.5. Computing the second PC and beyond
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualising-the-principal-components">
     9.1.1.6. Visualising the principal components
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pca-as-the-best-low-rank-approximation">
     9.1.1.7. PCA as the best low-rank approximation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analysing-crimes-in-the-us-with-pca">
   9.1.2. Analysing crimes in the US with PCA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-proportion-of-variance-explained">
   9.1.3. The proportion of variance explained
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-on-toy-data-and-system-transparency">
   9.1.4. PCA on toy data and system transparency
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   9.1.5. Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Principal component analysis</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-for-dimensionality-reduction">
   9.1.1. PCA for dimensionality reduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#motivation">
     9.1.1.1. Motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principal-components-pcs">
     9.1.1.2. Principal components (PCs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-the-first-pc">
     9.1.1.3. Computing the first PC
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-the-first-pc">
     9.1.1.4. Interpreting the first PC
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-the-second-pc-and-beyond">
     9.1.1.5. Computing the second PC and beyond
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualising-the-principal-components">
     9.1.1.6. Visualising the principal components
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pca-as-the-best-low-rank-approximation">
     9.1.1.7. PCA as the best low-rank approximation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analysing-crimes-in-the-us-with-pca">
   9.1.2. Analysing crimes in the US with PCA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-proportion-of-variance-explained">
   9.1.3. The proportion of variance explained
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-on-toy-data-and-system-transparency">
   9.1.4. PCA on toy data and system transparency
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   9.1.5. Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="principal-component-analysis">
<h1><span class="section-number">9.1. </span>Principal component analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h1>
<!-- Dimension reduction -->
<p>This section introduces <a class="reference external" href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis (PCA)</a>. PCA is a popular method for reducing the dimensionality of data, often represented as a data matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{D \times N}\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of observations and <span class="math notranslate nohighlight">\(D\)</span> is the number of variables (features). When faced with a large set of potentially correlated variables, PCA allows us to summarise this set with a smaller number of representative variables that collectively explain most of the variability in the original set.</p>
<!-- Thus, PCA is also a popular method for visualising high-dimensional data, and for imputing missing values in data matrices. -->
<!-- The first principal component direction of the data is that along which the observations vary the most. -->
<!-- PCA refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. -->
<p>PCA is an unsupervised method, since it involves only a set of <span class="math notranslate nohighlight">\(D\)</span> features <span class="math notranslate nohighlight">\(x_1, \ldots, x_D\)</span> and no associated response <span class="math notranslate nohighlight">\(y\)</span>. PCA is a standard <a class="reference external" href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a> or <a class="reference external" href="https://en.wikipedia.org/wiki/Feature_learning">feautre/representation learning</a> that is often used as a pre-processing step to reduce the feature dimensionality for supervised learning problems. PCA also serves as a standard tool for <a class="reference external" href="https://en.wikipedia.org/wiki/Data_and_information_visualization">data visualisation</a>, for visualising either observations or features. It can also be used as a factorisation model for data imputation, i.e. for filling in missing values in a data matrix.</p>
<p>Watch the 5-minute video below for a visual explanation of principal component analysis.</p>
<div class="admonition-video admonition">
<p class="admonition-title">Video</p>
<iframe width="700" height="394" src="https://www.youtube.com/embed/HMOI_lkzW08?start=10" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p><a class="reference external" href="https://www.youtube.com/embed/HMOI_lkzW08?start=10">Explaining main ideas behind principal component analysis, by StatQuest</a></p>
</div>
<div class="section" id="pca-for-dimensionality-reduction">
<h2><span class="section-number">9.1.1. </span>PCA for dimensionality reduction<a class="headerlink" href="#pca-for-dimensionality-reduction" title="Permalink to this headline">¶</a></h2>
<div class="section" id="motivation">
<h3><span class="section-number">9.1.1.1. </span>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h3>
<p>Suppose that we wish to visualise observations with measurements on a set of <span class="math notranslate nohighlight">\( D \)</span> features, <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_D \)</span>, as part of an <a class="reference external" href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">exploratory data analysis</a>. We could do this by examining two-dimensional scatterplots of the data, each of which contains the <span class="math notranslate nohighlight">\(N\)</span> observations’ measurements on two of the features. However, there are <span class="math notranslate nohighlight">\( D(D-1)/2 \)</span> such scatterplots, and it is difficult to visualise more than a few of them at a time even for a <span class="math notranslate nohighlight">\(D\)</span> of small value (e.g. if <span class="math notranslate nohighlight">\(D=10\)</span>, then <span class="math notranslate nohighlight">\( D(D-1)/2 = 45 \)</span>). If <span class="math notranslate nohighlight">\( D \)</span> is large, then it will certainly not be possible to look at all of them; moreover, most likely none of them will be informative since they each contain just a small fraction of the total information present in the dataset.</p>
<p>Clearly, a better method is needed to visualise the <span class="math notranslate nohighlight">\(N\)</span> observations when <span class="math notranslate nohighlight">\(D\)</span> is large. In particular, we would like to find a lower-dimensional representation of the data that captures as much of the information as possible. For instance, if we can obtain a two-dimensional representation of the data that captures most of the information, then we can plot the observations in this lower-dimensional space to gain a visual understanding of the data.</p>
<p>PCA provides a tool to do just this. It finds a lower-dimensional representation of a dataset that contains as much as possible of the variation. The idea is that each of the <span class="math notranslate nohighlight">\(N\)</span> observations lives in <span class="math notranslate nohighlight">\(D\)</span>-dimensional space, but not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension.</p>
</div>
<div class="section" id="principal-components-pcs">
<h3><span class="section-number">9.1.1.2. </span>Principal components (PCs)<a class="headerlink" href="#principal-components-pcs" title="Permalink to this headline">¶</a></h3>
<!-- We now explain the manner in which these dimensions, named as _principal components_, are found. -->
<p>Each of the dimensions found by PCA is called a principal component (PC), and it is a linear combination of the <span class="math notranslate nohighlight">\(D\)</span> features.</p>
<p>The first PC of a set of features <span class="math notranslate nohighlight">\(x_1, \ldots, x_D\)</span> is the normalised linear combination of the features</p>
<div class="math notranslate nohighlight">
\[
y_1 = \phi_{1,1}x_1 + \phi_{2,1}x_2 + \cdots + \phi_{D,1}x_D
\]</div>
<p>that has the largest variance. By normalised, we mean that <span class="math notranslate nohighlight">\( \sum_{d=1}^D \phi_{d,1}^2 = 1 \)</span> for the first PC <span class="math notranslate nohighlight">\(m=1\)</span> and <span class="math notranslate nohighlight">\( \sum_{d=1}^D \phi_{d,m}^2 = 1\)</span> for the <span class="math notranslate nohighlight">\(m\text{th}\)</span> PC in general. We refer to the elements <span class="math notranslate nohighlight">\(\phi_{1, 1}, \ldots, \phi_{D, 1} \)</span> as the  <em>loadings</em> of the first <em>principal loading component</em>. Together, the loadings make up the first <em>principal component loading vector</em>, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1 = [\phi_{1,1}, \ldots, \phi_{D,1}]^\top\)</span>. We constrain the loadings so that their sum of squares is equal to one, i.e. normalised, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance, giving a trivial solution.</p>
</div>
<div class="section" id="computing-the-first-pc">
<h3><span class="section-number">9.1.1.3. </span>Computing the first PC<a class="headerlink" href="#computing-the-first-pc" title="Permalink to this headline">¶</a></h3>
<p>Given a <span class="math notranslate nohighlight">\(D \times N\)</span> dataset as a data matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, how do we compute the first principal component? Since we are only interested in variance, we assume that each of the features in <span class="math notranslate nohighlight">\( \mathbf{X} \)</span> has been centred to have mean zero (that is, the column means of <span class="math notranslate nohighlight">\( \mathbf{X} \)</span> are zero). We then look for the linear combination of the sample feature values of the form</p>
<div class="math notranslate nohighlight">
\[
y_{n,1} = \phi_{1,1}x_{n,1} + \phi_{2,1}x_{n,2} + \cdots + \phi_{D,1}x_{n,D}
\]</div>
<p>that has the largest sample variance, subject to the constraint that <span class="math notranslate nohighlight">\(\sum_{d=1}^D \phi_{d,1}^2 = 1\)</span>. In other words, the first principal component loading vector <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1\)</span> solves the optimisation problem</p>
<div class="math notranslate nohighlight" id="equation-eq-1stpc">
<span class="eqno">(9.1)<a class="headerlink" href="#equation-eq-1stpc" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\arg\max_{\boldsymbol{\phi}_1} \left\{\frac{1}{N} \sum_{n=1}^N \left(\sum_{d=1}^D \phi_{d,1} x_{n,d} \right)^2\right\} \quad \text{subject to} \quad \sum_{d=1}^D \phi_{d,1}^2 = 1
\end{equation}\]</div>
<p>where the objective can also be written as <span class="math notranslate nohighlight">\(\frac{1}{N} \sum_{n=1}^N y_{n,1}^2 \)</span>. Equation <a class="reference internal" href="#equation-eq-1stpc">(9.1)</a> maximises the sample variance of the <span class="math notranslate nohighlight">\(N\)</span> values of <span class="math notranslate nohighlight">\( y_{n,1} \)</span>. We refer <span class="math notranslate nohighlight">\( y_{1,1}, \ldots, y_{N,1} \)</span> as the first <em>principal component scores</em>. Equation <a class="reference internal" href="#equation-eq-1stpc">(9.1)</a> can be solved via a standard technique in linear algebra, <a class="reference external" href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix"><em>eigendecomposition</em></a> or <a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a>, as the first eigenvector of the covariance matrix <span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{X}^\top\)</span>, the one corresponding to the largest eigenvalue, where this eigenvalue is the variance captured in $ {y_{n,1}}. The mathematical details of which are beyond the scope of this course.</p>
</div>
<div class="section" id="interpreting-the-first-pc">
<h3><span class="section-number">9.1.1.4. </span>Interpreting the first PC<a class="headerlink" href="#interpreting-the-first-pc" title="Permalink to this headline">¶</a></h3>
<p>There is a nice geometric interpretation for the first principal component. The loading vector <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_1 \)</span> with elements <span class="math notranslate nohighlight">\( \phi_{1,1}, \ldots, \phi_{D, 1} \)</span> defines a direction in feature space along which the data vary the most. If we project the <span class="math notranslate nohighlight">\(N\)</span> data points <span class="math notranslate nohighlight">\(\mathbf{x}_1, \ldots, \mathbf{x}_N\)</span> onto this direction, the projected values are the principal component scores <span class="math notranslate nohighlight">\( y_{1, 1} , \ldots , y_{N, 1}\)</span> themselves.</p>
</div>
<div class="section" id="computing-the-second-pc-and-beyond">
<h3><span class="section-number">9.1.1.5. </span>Computing the second PC and beyond<a class="headerlink" href="#computing-the-second-pc-and-beyond" title="Permalink to this headline">¶</a></h3>
<p>After the first principal component <span class="math notranslate nohighlight">\( y_{n,1} \)</span> of the features has been determined, we can find the second principal component <span class="math notranslate nohighlight">\( y_{n,2} \)</span>. The second principal component is the linear combination of <span class="math notranslate nohighlight">\( x_1 , \ldots, x_D \)</span> that has maximal variance out of all linear combinations that are uncorrelated with <span class="math notranslate nohighlight">\( y_1 \)</span>. The second principal component scores <span class="math notranslate nohighlight">\( y_{1,2}, y_{2,2}, \ldots , y_{N,2}\)</span> take the form</p>
<div class="math notranslate nohighlight">
\[
y_{n,2} = \phi_{1,2}x_{n,1} + \phi_{2,2}x_{n,2} + \cdots + \phi_{D,2}x_{n,D},
\]</div>
<p>where <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_2 = [\phi_{1,2}, \ldots, \phi_{D,2}]^\top\)</span> is the second principal component loading vector. It turns out that constraining <span class="math notranslate nohighlight">\( \{y_{n,2}\} \)</span> to be uncorrelated with <span class="math notranslate nohighlight">\( \{y_{n,1}\} \)</span> is equivalent to constraining the direction <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_2 \)</span> to be orthogonal (perpendicular) to the direction <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_1 \)</span>. Thus, by (beautiful) linear algebra, <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_2 \)</span> is the second eigenvector of the covariance matrix <span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{X}^\top\)</span>, the one corresponding to the second largest eigenvalue that equals to the variance captured in $ {y_{n,2}}. The</p>
<p>The following principal component loading vectors <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_3 \)</span>, <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_4 \)</span>, and so forth are the third, fourth, and so forth eigenvectors of the covariance matrix <span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{X}^\top\)</span>, the ones corresponding to the third, fourth, and so forth largest eigenvalues. The corresponding principal component scores are <span class="math notranslate nohighlight">\( \{y_{n,3}\} \)</span>, <span class="math notranslate nohighlight">\( \{y_{n,4}\} \)</span>, and so forth.</p>
</div>
<div class="section" id="visualising-the-principal-components">
<h3><span class="section-number">9.1.1.6. </span>Visualising the principal components<a class="headerlink" href="#visualising-the-principal-components" title="Permalink to this headline">¶</a></h3>
<p>Once we have computed the principal components, we can plot them against each other in order to produce lower-dimensional views of the data. For instance, we can plot the score vector <span class="math notranslate nohighlight">\( \{y_{n,1}\} \)</span> against <span class="math notranslate nohighlight">\( \{y_{n,2}\} \)</span>, <span class="math notranslate nohighlight">\( \{y_{n,1}\} \)</span> against <span class="math notranslate nohighlight">\( \{y_{n,3}\} \)</span>, <span class="math notranslate nohighlight">\( \{y_{n,2}\} \)</span> against <span class="math notranslate nohighlight">\( \{y_{n,3}\} \)</span>, and so forth. Geometrically, this amounts to projecting the original data down onto the subspace spanned by <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_1 \)</span>, <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_2 \)</span>, and <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_3 \)</span>, and plotting the projected points.</p>
<div class="tip admonition">
<p class="admonition-title">Interactive exploration</p>
<p>You can explore PCA-based visualisation interactively in the <a class="reference external" href="https://projector.tensorflow.org/">Embedding Projector by TensorFlow</a>, noting that the default visualisation algorithm is PCA and you can choose different principal components (up to three) to visualise.</p>
</div>
</div>
<div class="section" id="pca-as-the-best-low-rank-approximation">
<h3><span class="section-number">9.1.1.7. </span>PCA as the best low-rank approximation<a class="headerlink" href="#pca-as-the-best-low-rank-approximation" title="Permalink to this headline">¶</a></h3>
<p>The first <span class="math notranslate nohighlight">\(M\)</span> principal component score vectors and the first <span class="math notranslate nohighlight">\(M\)</span> principal component loading vectors provide the best <span class="math notranslate nohighlight">\(M\)</span>-dimensional (i.e. rank-<span class="math notranslate nohighlight">\(M\)</span>) approximation (in terms of Euclidean distance) to the <span class="math notranslate nohighlight">\(n\text{th}\)</span> observation <span class="math notranslate nohighlight">\(\mathbf{x}_{n}\)</span>. This representation can be written in terms of the <span class="math notranslate nohighlight">\(d\text{th}\)</span> feature of <span class="math notranslate nohighlight">\(\mathbf{x}_{n}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
x_{n, d} \approx \sum_{m=1}^M \phi_{d, m} y_{n, m}.
\]</div>
<p>When <span class="math notranslate nohighlight">\(M\)</span> is sufficiently large, the <span class="math notranslate nohighlight">\(M\)</span> principal component score vectors and <span class="math notranslate nohighlight">\(M\)</span> principal component loading vectors can give a good approximation to the data. When <span class="math notranslate nohighlight">\(M = \min(N − 1, D)\)</span>, then the representation is exact: <span class="math notranslate nohighlight">\(x_{n, d} = \sum_{m=1}^M \phi_{d, m} y_{n, m}\)</span>.</p>
</div>
</div>
<div class="section" id="analysing-crimes-in-the-us-with-pca">
<h2><span class="section-number">9.1.2. </span>Analysing crimes in the US with PCA<a class="headerlink" href="#analysing-crimes-in-the-us-with-pca" title="Permalink to this headline">¶</a></h2>
<p>We illustrate the use of PCA on the <a class="reference external" href="https://github.com/pykale/transparentML/blob/main/data/USArrests.csv">USArrests dataset</a>. For each of the 50 states in the United States, the dataset contains the number of arrests per 100,000 residents for each of three crimes: <code class="docutils literal notranslate"><span class="pre">Assault</span></code>, <code class="docutils literal notranslate"><span class="pre">Murder</span></code>, and <code class="docutils literal notranslate"><span class="pre">Rape</span></code>. We also record <code class="docutils literal notranslate"><span class="pre">UrbanPop</span></code> (the percent of the population in each state living in urban areas). The principal component score vectors have length <span class="math notranslate nohighlight">\(N = 50\)</span>, and the principal component loading vectors have length <span class="math notranslate nohighlight">\(D = 4\)</span>. PCA was performed after standardizing each variable (feature) to have mean zero and standard deviation one.</p>
<p>Get ready by importing the APIs needed from respective libraries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">svd</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">scipy.cluster</span> <span class="kn">import</span> <span class="n">hierarchy</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<p>Load the data and print the first five rows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/pykale/transparentML/raw/main/data/USArrests.csv&quot;</span>

<span class="n">USArrests</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_url</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">USArrests</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Murder</th>
      <th>Assault</th>
      <th>UrbanPop</th>
      <th>Rape</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Alabama</th>
      <td>13.2</td>
      <td>236</td>
      <td>58</td>
      <td>21.2</td>
    </tr>
    <tr>
      <th>Alaska</th>
      <td>10.0</td>
      <td>263</td>
      <td>48</td>
      <td>44.5</td>
    </tr>
    <tr>
      <th>Arizona</th>
      <td>8.1</td>
      <td>294</td>
      <td>80</td>
      <td>31.0</td>
    </tr>
    <tr>
      <th>Arkansas</th>
      <td>8.8</td>
      <td>190</td>
      <td>50</td>
      <td>19.5</td>
    </tr>
    <tr>
      <th>California</th>
      <td>9.0</td>
      <td>276</td>
      <td>91</td>
      <td>40.6</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Inspect the mean and standard deviation of each feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">USArrests</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">USArrests</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Murder        7.788
Assault     170.760
UrbanPop     65.540
Rape         21.232
dtype: float64
Murder        18.970465
Assault     6945.165714
UrbanPop     209.518776
Rape          87.729159
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Standardise the data by subtracting the mean and dividing by the standard deviation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">USArrests</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="n">USArrests</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">USArrests</span><span class="o">.</span><span class="n">columns</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Perform PCA on the standardised data and print (all) the four principal component loading vectors.</p>
<!-- """ 
Depends on the version of python/module, you may see a flipped loading vector in signs. 
This is normal because the orientation of the principal components is not deterministic. 
""" --><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The loading vectors (i.e. these are the projection of the data onto the principal components)</span>
<span class="n">pca_loadings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="n">USArrests</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;V1&quot;</span><span class="p">,</span> <span class="s2">&quot;V2&quot;</span><span class="p">,</span> <span class="s2">&quot;V3&quot;</span><span class="p">,</span> <span class="s2">&quot;V4&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pca_loadings</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                V1        V2        V3        V4
Murder    0.535899  0.418181 -0.341233  0.649228
Assault   0.583184  0.187986 -0.268148 -0.743407
UrbanPop  0.278191 -0.872806 -0.378016  0.133878
Rape      0.543432 -0.167319  0.817778  0.089024
</pre></div>
</div>
</div>
</div>
<p>Use these principal component loading vectors to compute the principal component scores, i.e. transform the data to the principal component space.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">df_plot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;PC1&quot;</span><span class="p">,</span> <span class="s2">&quot;PC2&quot;</span><span class="p">,</span> <span class="s2">&quot;PC3&quot;</span><span class="p">,</span> <span class="s2">&quot;PC4&quot;</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">index</span>
<span class="p">)</span>
<span class="n">df_plot</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PC1</th>
      <th>PC2</th>
      <th>PC3</th>
      <th>PC4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Alabama</th>
      <td>0.985566</td>
      <td>1.133392</td>
      <td>-0.444269</td>
      <td>0.156267</td>
    </tr>
    <tr>
      <th>Alaska</th>
      <td>1.950138</td>
      <td>1.073213</td>
      <td>2.040003</td>
      <td>-0.438583</td>
    </tr>
    <tr>
      <th>Arizona</th>
      <td>1.763164</td>
      <td>-0.745957</td>
      <td>0.054781</td>
      <td>-0.834653</td>
    </tr>
    <tr>
      <th>Arkansas</th>
      <td>-0.141420</td>
      <td>1.119797</td>
      <td>0.114574</td>
      <td>-0.182811</td>
    </tr>
    <tr>
      <th>California</th>
      <td>2.523980</td>
      <td>-1.542934</td>
      <td>0.598557</td>
      <td>-0.341996</td>
    </tr>
    <tr>
      <th>Colorado</th>
      <td>1.514563</td>
      <td>-0.987555</td>
      <td>1.095007</td>
      <td>0.001465</td>
    </tr>
    <tr>
      <th>Connecticut</th>
      <td>-1.358647</td>
      <td>-1.088928</td>
      <td>-0.643258</td>
      <td>-0.118469</td>
    </tr>
    <tr>
      <th>Delaware</th>
      <td>0.047709</td>
      <td>-0.325359</td>
      <td>-0.718633</td>
      <td>-0.881978</td>
    </tr>
    <tr>
      <th>Florida</th>
      <td>3.013042</td>
      <td>0.039229</td>
      <td>-0.576829</td>
      <td>-0.096285</td>
    </tr>
    <tr>
      <th>Georgia</th>
      <td>1.639283</td>
      <td>1.278942</td>
      <td>-0.342460</td>
      <td>1.076797</td>
    </tr>
    <tr>
      <th>Hawaii</th>
      <td>-0.912657</td>
      <td>-1.570460</td>
      <td>0.050782</td>
      <td>0.902807</td>
    </tr>
    <tr>
      <th>Idaho</th>
      <td>-1.639800</td>
      <td>0.210973</td>
      <td>0.259801</td>
      <td>-0.499104</td>
    </tr>
    <tr>
      <th>Illinois</th>
      <td>1.378911</td>
      <td>-0.681841</td>
      <td>-0.677496</td>
      <td>-0.122021</td>
    </tr>
    <tr>
      <th>Indiana</th>
      <td>-0.505461</td>
      <td>-0.151563</td>
      <td>0.228055</td>
      <td>0.424666</td>
    </tr>
    <tr>
      <th>Iowa</th>
      <td>-2.253646</td>
      <td>-0.104054</td>
      <td>0.164564</td>
      <td>0.017556</td>
    </tr>
    <tr>
      <th>Kansas</th>
      <td>-0.796881</td>
      <td>-0.270165</td>
      <td>0.025553</td>
      <td>0.206496</td>
    </tr>
    <tr>
      <th>Kentucky</th>
      <td>-0.750859</td>
      <td>0.958440</td>
      <td>-0.028369</td>
      <td>0.670557</td>
    </tr>
    <tr>
      <th>Louisiana</th>
      <td>1.564818</td>
      <td>0.871055</td>
      <td>-0.783480</td>
      <td>0.454728</td>
    </tr>
    <tr>
      <th>Maine</th>
      <td>-2.396829</td>
      <td>0.376392</td>
      <td>-0.065682</td>
      <td>-0.330460</td>
    </tr>
    <tr>
      <th>Maryland</th>
      <td>1.763369</td>
      <td>0.427655</td>
      <td>-0.157250</td>
      <td>-0.559070</td>
    </tr>
    <tr>
      <th>Massachusetts</th>
      <td>-0.486166</td>
      <td>-1.474496</td>
      <td>-0.609497</td>
      <td>-0.179599</td>
    </tr>
    <tr>
      <th>Michigan</th>
      <td>2.108441</td>
      <td>-0.155397</td>
      <td>0.384869</td>
      <td>0.102372</td>
    </tr>
    <tr>
      <th>Minnesota</th>
      <td>-1.692682</td>
      <td>-0.632261</td>
      <td>0.153070</td>
      <td>0.067317</td>
    </tr>
    <tr>
      <th>Mississippi</th>
      <td>0.996494</td>
      <td>2.393796</td>
      <td>-0.740808</td>
      <td>0.215508</td>
    </tr>
    <tr>
      <th>Missouri</th>
      <td>0.696787</td>
      <td>-0.263355</td>
      <td>0.377444</td>
      <td>0.225824</td>
    </tr>
    <tr>
      <th>Montana</th>
      <td>-1.185452</td>
      <td>0.536874</td>
      <td>0.246889</td>
      <td>0.123742</td>
    </tr>
    <tr>
      <th>Nebraska</th>
      <td>-1.265637</td>
      <td>-0.193954</td>
      <td>0.175574</td>
      <td>0.015893</td>
    </tr>
    <tr>
      <th>Nevada</th>
      <td>2.874395</td>
      <td>-0.775600</td>
      <td>1.163380</td>
      <td>0.314515</td>
    </tr>
    <tr>
      <th>New Hampshire</th>
      <td>-2.383915</td>
      <td>-0.018082</td>
      <td>0.036855</td>
      <td>-0.033137</td>
    </tr>
    <tr>
      <th>New Jersey</th>
      <td>0.181566</td>
      <td>-1.449506</td>
      <td>-0.764454</td>
      <td>0.243383</td>
    </tr>
    <tr>
      <th>New Mexico</th>
      <td>1.980024</td>
      <td>0.142849</td>
      <td>0.183692</td>
      <td>-0.339534</td>
    </tr>
    <tr>
      <th>New York</th>
      <td>1.682577</td>
      <td>-0.823184</td>
      <td>-0.643075</td>
      <td>-0.013484</td>
    </tr>
    <tr>
      <th>North Carolina</th>
      <td>1.123379</td>
      <td>2.228003</td>
      <td>-0.863572</td>
      <td>-0.954382</td>
    </tr>
    <tr>
      <th>North Dakota</th>
      <td>-2.992226</td>
      <td>0.599119</td>
      <td>0.301277</td>
      <td>-0.253987</td>
    </tr>
    <tr>
      <th>Ohio</th>
      <td>-0.225965</td>
      <td>-0.742238</td>
      <td>-0.031139</td>
      <td>0.473916</td>
    </tr>
    <tr>
      <th>Oklahoma</th>
      <td>-0.311783</td>
      <td>-0.287854</td>
      <td>-0.015310</td>
      <td>0.010332</td>
    </tr>
    <tr>
      <th>Oregon</th>
      <td>0.059122</td>
      <td>-0.541411</td>
      <td>0.939833</td>
      <td>-0.237781</td>
    </tr>
    <tr>
      <th>Pennsylvania</th>
      <td>-0.888416</td>
      <td>-0.571100</td>
      <td>-0.400629</td>
      <td>0.359061</td>
    </tr>
    <tr>
      <th>Rhode Island</th>
      <td>-0.863772</td>
      <td>-1.491978</td>
      <td>-1.369946</td>
      <td>-0.613569</td>
    </tr>
    <tr>
      <th>South Carolina</th>
      <td>1.320724</td>
      <td>1.933405</td>
      <td>-0.300538</td>
      <td>-0.131467</td>
    </tr>
    <tr>
      <th>South Dakota</th>
      <td>-1.987775</td>
      <td>0.823343</td>
      <td>0.389293</td>
      <td>-0.109572</td>
    </tr>
    <tr>
      <th>Tennessee</th>
      <td>0.999742</td>
      <td>0.860251</td>
      <td>0.188083</td>
      <td>0.652864</td>
    </tr>
    <tr>
      <th>Texas</th>
      <td>1.355138</td>
      <td>-0.412481</td>
      <td>-0.492069</td>
      <td>0.643195</td>
    </tr>
    <tr>
      <th>Utah</th>
      <td>-0.550565</td>
      <td>-1.471505</td>
      <td>0.293728</td>
      <td>-0.082314</td>
    </tr>
    <tr>
      <th>Vermont</th>
      <td>-2.801412</td>
      <td>1.402288</td>
      <td>0.841263</td>
      <td>-0.144890</td>
    </tr>
    <tr>
      <th>Virginia</th>
      <td>-0.096335</td>
      <td>0.199735</td>
      <td>0.011713</td>
      <td>0.211371</td>
    </tr>
    <tr>
      <th>Washington</th>
      <td>-0.216903</td>
      <td>-0.970124</td>
      <td>0.624871</td>
      <td>-0.220848</td>
    </tr>
    <tr>
      <th>West Virginia</th>
      <td>-2.108585</td>
      <td>1.424847</td>
      <td>0.104775</td>
      <td>0.131909</td>
    </tr>
    <tr>
      <th>Wisconsin</th>
      <td>-2.079714</td>
      <td>-0.611269</td>
      <td>-0.138865</td>
      <td>0.184104</td>
    </tr>
    <tr>
      <th>Wyoming</th>
      <td>-0.629427</td>
      <td>0.321013</td>
      <td>-0.240659</td>
      <td>-0.166652</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Plot the first two principal component scores against each other for all 50 states. Also visualise the four principal component loading vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>

<span class="c1"># plot Principal Components 1 and 2</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df_plot</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">df_plot</span><span class="o">.</span><span class="n">PC1</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="o">-</span><span class="n">df_plot</span><span class="o">.</span><span class="n">PC2</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>

<span class="c1"># plot reference lines</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s2">&quot;dotted&quot;</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s2">&quot;dotted&quot;</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;First Principal Component&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Second Principal Component&quot;</span><span class="p">)</span>

<span class="c1"># plot Principal Component loading vectors, using a second y-axis.</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span><span class="o">.</span><span class="n">twiny</span><span class="p">()</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Principal Component loading vectors&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">)</span>

<span class="c1"># plot labels for vectors. Variable &#39;a&#39; is a small offset parameter to separate arrow tip and text.</span>
<span class="n">a</span> <span class="o">=</span> <span class="mf">1.07</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pca_loadings</span><span class="p">[[</span><span class="s2">&quot;V1&quot;</span><span class="p">,</span> <span class="s2">&quot;V2&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span>
        <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">pca_loadings</span><span class="o">.</span><span class="n">V1</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="o">-</span><span class="n">pca_loadings</span><span class="o">.</span><span class="n">V2</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">a</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span>
    <span class="p">)</span>

<span class="c1"># plot vectors</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">pca_loadings</span><span class="o">.</span><span class="n">V1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="n">pca_loadings</span><span class="o">.</span><span class="n">V2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">pca_loadings</span><span class="o">.</span><span class="n">V1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">pca_loadings</span><span class="o">.</span><span class="n">V2</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">pca_loadings</span><span class="o">.</span><span class="n">V1</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="n">pca_loadings</span><span class="o">.</span><span class="n">V2</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">pca_loadings</span><span class="o">.</span><span class="n">V1</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="o">-</span><span class="n">pca_loadings</span><span class="o">.</span><span class="n">V2</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pca_16_0.png" src="../_images/pca_16_0.png" />
</div>
</div>
<p>In the figure above, we can see that the first loading vector places approximately equal weight on <code class="docutils literal notranslate"><span class="pre">Assault</span></code>, <code class="docutils literal notranslate"><span class="pre">Murder</span></code>, and <code class="docutils literal notranslate"><span class="pre">Rape</span></code>, but with much less weight on <code class="docutils literal notranslate"><span class="pre">UrbanPop</span></code>. Hence this component roughly corresponds to a measure of overall rates of serious crimes. The second loading vector places most of its weight on <code class="docutils literal notranslate"><span class="pre">UrbanPop</span></code> and much less weight on the other three features. Hence, this component roughly corresponds to the level of urbanisation of the state. Overall, we see that the crime-related variables/features ( <code class="docutils literal notranslate"><span class="pre">Murder</span></code>, <code class="docutils literal notranslate"><span class="pre">Assault</span></code>, and <code class="docutils literal notranslate"><span class="pre">Rape</span></code>) are located close to each other, and that the <code class="docutils literal notranslate"><span class="pre">UrbanPop</span></code> variable/feature is far from the other three. This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the <code class="docutils literal notranslate"><span class="pre">UrbanPop</span></code> variable/feature is less correlated with the other three.</p>
<p>We can examine differences between the states via the two principal component score vectors shown in the figure above. Our discussion of the loading vectors suggests that states with large positive scores on the first component, such as California, Nevada and Florida, have high crime rates, while states like North Dakota, with negative scores on the first component, have low crime rates. California also has a high score on the second component, indicating a high level of urbanisation, while the opposite is true for states like Mississippi. States close to zero on both components, such as Indiana, have approximately average levels of both crime and urbanisation.</p>
</div>
<div class="section" id="the-proportion-of-variance-explained">
<h2><span class="section-number">9.1.3. </span>The proportion of variance explained<a class="headerlink" href="#the-proportion-of-variance-explained" title="Permalink to this headline">¶</a></h2>
<p>We can now ask a natural question: how much of the information in a given dataset is lost by projecting the observations onto the first <span class="math notranslate nohighlight">\(M\)</span> principal components? That is, how much of the variance in the data is not contained in the first <span class="math notranslate nohighlight">\(M\)</span> principal components? More generally, we are interested in knowing the proportion of variance explained (PVE) by each principal component. The total variance present in a dataset (assuming that the variables/features have been centred to have mean zero) is defined as</p>
<div class="math notranslate nohighlight">
\[
\sum_{d=1}^D \frac{1}{N} \sum_{n=1}^N x_{n,d}^2.
\]</div>
<p>and the variance explained by the <span class="math notranslate nohighlight">\(m\textrm{th}\)</span> principal component is</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{N}\sum_{n=1}^N y_{n,m}^2 = \frac{1}{N}\sum_{n=1}^N \left(\sum_{d=1}^D \phi_{d,m} x_{n,d}\right)^2.
\]</div>
<p>Therefore, the PVE of the <span class="math notranslate nohighlight">\(m\textrm{th}\)</span> principal component is given by</p>
<div class="math notranslate nohighlight" id="equation-eq-pve">
<span class="eqno">(9.2)<a class="headerlink" href="#equation-eq-pve" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{\sum_{n=1}^N y_{n,m}^2}{\sum_{d=1}^D\sum_{n=1}^N  x_{d,n}^2} = \frac{\sum_{n=1}^N \left(\sum_{d=1}^D \phi_{d,m} x_{n,d}\right)^2}{\sum_{d=1}^D\sum_{n=1}^N  x_{n,d}^2}.
\end{equation}\]</div>
<p>The PVE of each principal component is a positive quantity. In order to compute the cumulative PVE of the first <span class="math notranslate nohighlight">\(M\)</span> principal components, we can simply sum Equation <a class="reference internal" href="#equation-eq-pve">(9.2)</a> over each of the first <span class="math notranslate nohighlight">\(M\)</span> PVEs. In total, there are (at most) <span class="math notranslate nohighlight">\( \min (N − 1, D) \)</span> principal components, and their PVEs sum to one.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, the proportion of explained variance is available in the <code class="docutils literal notranslate"><span class="pre">PCA</span></code> object to help select the number of PCs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2.53085875 1.00996444 0.36383998 0.17696948]
[0.62006039 0.24744129 0.0891408  0.04335752]
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Interpreting the proportion of variance explained</p>
<p>From the above, in the <code class="docutils literal notranslate"><span class="pre">USArrests</span></code> data, the first principal component explains 62.0 % of the variance in the data, and the next principal component explains 24.7 % of the variance. Together, the first two principal components explain almost 87 % of the variance in the data, and the last two principal components explain only 13 % of the variance.</p>
</div>
<p>Let us plot the PVE of each principal component, as well as the cumulative PVE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">,</span> <span class="s2">&quot;-o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Individual component&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">),</span> <span class="s2">&quot;-s&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Cumulative&quot;</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Proportion of Variance Explained&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Principal Component&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">4.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pca_21_0.png" src="../_images/pca_21_0.png" />
</div>
</div>
<p>In this case, if we want to preserve at least 80% of variance of the data, we need to select 2 PCs.</p>
</div>
<div class="section" id="pca-on-toy-data-and-system-transparency">
<h2><span class="section-number">9.1.4. </span>PCA on toy data and system transparency<a class="headerlink" href="#pca-on-toy-data-and-system-transparency" title="Permalink to this headline">¶</a></h2>
<p>Here, we adapt the example <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html#sphx-glr-auto-examples-cross-decomposition-plot-pcr-vs-pls-py">“Principal Component Regression vs Partial Least Squares Regression”</a> from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> to explain the system transparency of PCA.</p>
<p>We generate synthetic data, perform PCA, and then plot the first two principal components:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;samples&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">comp</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)):</span>
    <span class="c1"># comp = comp * var  # scale component by its variance explanation power</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">dx</span><span class="o">=</span><span class="n">comp</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">dy</span><span class="o">=</span><span class="n">comp</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Principal component </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">head_width</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dx</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.85</span><span class="p">,</span>
    <span class="n">dy</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.85</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">head_width</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Input Data Sample&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">facecolors</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span>
    <span class="n">linewidths</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;equal&quot;</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;2-dimensional dataset with principal components&quot;</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;first feature&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;second feature&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pca_25_0.png" src="../_images/pca_25_0.png" />
</div>
</div>
<p>Let us examine the numerical values of a particular data point (circled in red), its principal component scores, the mean of the training data, the principal component loading vectors, and their explained variance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The numbers display below round off to 2 decimal places.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data point: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Scores of the data point: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean of the training data: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PCA loadings: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Explained variance: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Explained variance ratio: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The numbers display below round off to 2 decimal places.
Data point:  [-2.78 -0.93]
Scores of the data point:  [ 2.67 -1.54]
Mean of the training data:  [0.12 0.12]
PCA loadings: 
 [[-0.64 -0.77]
 [ 0.77 -0.64]]
Explained variance:  [6.21 0.46]
Explained variance ratio:  [0.93 0.07]
</pre></div>
</div>
</div>
</div>
<div class="important admonition">
<p class="admonition-title">System transparency</p>
<ul class="simple">
<li><p>For data point <span class="math notranslate nohighlight">\( [-2.78 \;\; -0.93]^{\top} \)</span>, the score vector of PCA can be obtained by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} \underbrace{\left[\begin{matrix} -0.64 &amp; -0.77 \\ 0.77 &amp; -0.64 \end{matrix}\right]}_{\text{Loading}}\left(\underbrace{\left[\begin{matrix} -2.78 \\ -0.93 \end{matrix}\right]}_{\text{Input Data}} - \underbrace{\left[\begin{matrix}0.12 \\ 0.12 \end{matrix}\right]}_{\text{Mean}}\right) = \underbrace{\left[\begin{matrix} 2.67 \\ -1.54 \end{matrix}\right].}_{\text{Scores}} \end{split}\]</div>
<ul class="simple">
<li><p>The above process can be used inversely to obtain the input data from the scores by times the transpose (i.e. the inverse due to orthonormality) of the loading matrix (<span class="math notranslate nohighlight">\(\left[\begin{matrix} -0.64 &amp; -0.77   \\ 0.77 &amp;  -0.64  \end{matrix}\right]^{\top}\)</span>) on both sides and move the mean to the other side:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} 
\underbrace{\left[\begin{matrix} -0.64 &amp; -0.77   \\ 0.77 &amp;  -0.64  \end{matrix}\right]^{\top}}_{\text{Loading}} \underbrace{\left[\begin{matrix} 2.67 \\ -1.54 \end{matrix}\right]}_{\text{Scores}} + \underbrace{\left[\begin{matrix}0.12 \\ 0.12 \end{matrix}\right]}_{\text{Mean}} = \underbrace{\left[\begin{matrix} -2.78 \\ -0.93 \end{matrix}\right].}_{\text{Reconstructed Input}}
\end{split}\]</div>
<p>The reconstruction can also be interpreted from the geometric perspective. As displayed in the figure above, the input data point <span class="math notranslate nohighlight">\( [-2.78 \;\; -0.93]^{\top} \)</span> (the black vector) can be viewed as the linear combination of the two loading vectors (PC 1 and PC 2 in orange and green, respectively). The scores of the input data point are the coefficients of the linear combination. The reconstructed input data point is the linear combination of the scores and the loading vectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\underbrace{\left[\begin{matrix} -0.64 \\ -0.77 \end{matrix}\right]}_{\text{PC 1}} \times \underbrace{2.67}_{\text{PC1 Score}} + \underbrace{\left[\begin{matrix} 0.77 \\ -0.64 \end{matrix}\right]}_{\text{PC 2}} \times \underbrace{(-1.54)}_{\text{PC 2 Score}} + \underbrace{\left[\begin{matrix}0.12 \\ 0.12 \end{matrix}\right]}_{\text{Mean}} = \underbrace{\left[\begin{matrix} -2.78 \\ -0.93 \end{matrix}\right].}_{\text{Reconstructed Input}}
\end{split}\]</div>
<p>Thus, given any point in the output PCA space, we can reconstruct the input data point by the above process.</p>
</div>
<p>Note: numbers are limited to two decimal places for the sake of clarity.</p>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">9.1.5. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p><strong>1</strong>. Load the <strong>miceprotein dataset</strong> from <strong>sklearn.datasets</strong> using <strong>fetch_openml</strong> where the dataset contains <strong>gene expressions</strong> of <strong>mice brains</strong>. Hint: See <a class="reference external" href="https://scikit-learn.org/stable/datasets/loading_other_datasets.html#:~:text=gene%20expressions%20in%20mice%20brains">here</a> for further help.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">fetch_openml</span><span class="p">(</span><span class="s2">&quot;miceprotein&quot;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `&#39;liac-arff&#39;` to `&#39;auto&#39;` in 1.4. You can set `parser=&#39;auto&#39;` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml&#39;s API doc for details.
  warn(
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>DYRK1A_N</th>
      <th>ITSN1_N</th>
      <th>BDNF_N</th>
      <th>NR1_N</th>
      <th>NR2A_N</th>
      <th>pAKT_N</th>
      <th>pBRAF_N</th>
      <th>pCAMKII_N</th>
      <th>pCREB_N</th>
      <th>pELK_N</th>
      <th>...</th>
      <th>SHH_N</th>
      <th>BAD_N</th>
      <th>BCL2_N</th>
      <th>pS6_N</th>
      <th>pCFOS_N</th>
      <th>SYP_N</th>
      <th>H3AcK18_N</th>
      <th>EGR1_N</th>
      <th>H3MeK4_N</th>
      <th>CaNA_N</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>75</th>
      <td>0.649781</td>
      <td>0.828696</td>
      <td>0.405862</td>
      <td>2.921435</td>
      <td>5.167979</td>
      <td>0.207174</td>
      <td>0.176640</td>
      <td>3.728084</td>
      <td>0.239283</td>
      <td>1.666579</td>
      <td>...</td>
      <td>0.239752</td>
      <td>0.139052</td>
      <td>0.112926</td>
      <td>0.132001</td>
      <td>0.129363</td>
      <td>0.486912</td>
      <td>0.125152</td>
      <td>0.146865</td>
      <td>0.143517</td>
      <td>1.627181</td>
    </tr>
    <tr>
      <th>76</th>
      <td>0.616481</td>
      <td>0.841974</td>
      <td>0.388584</td>
      <td>2.862575</td>
      <td>5.194163</td>
      <td>0.223433</td>
      <td>0.167725</td>
      <td>3.648240</td>
      <td>0.221030</td>
      <td>1.565150</td>
      <td>...</td>
      <td>0.249031</td>
      <td>0.133787</td>
      <td>0.121607</td>
      <td>0.139008</td>
      <td>0.143084</td>
      <td>0.467833</td>
      <td>0.112857</td>
      <td>0.161132</td>
      <td>0.145719</td>
      <td>1.562096</td>
    </tr>
    <tr>
      <th>77</th>
      <td>0.637424</td>
      <td>0.852882</td>
      <td>0.400561</td>
      <td>2.968155</td>
      <td>5.350820</td>
      <td>0.208790</td>
      <td>0.173261</td>
      <td>3.814545</td>
      <td>0.222300</td>
      <td>1.741732</td>
      <td>...</td>
      <td>0.247956</td>
      <td>0.142324</td>
      <td>0.130261</td>
      <td>0.134804</td>
      <td>0.147673</td>
      <td>0.462501</td>
      <td>0.116433</td>
      <td>0.160594</td>
      <td>0.142879</td>
      <td>1.571868</td>
    </tr>
    <tr>
      <th>78</th>
      <td>0.576815</td>
      <td>0.755390</td>
      <td>0.348346</td>
      <td>2.624901</td>
      <td>4.727509</td>
      <td>0.205892</td>
      <td>0.161192</td>
      <td>3.778530</td>
      <td>0.194153</td>
      <td>1.505475</td>
      <td>...</td>
      <td>0.233225</td>
      <td>0.133637</td>
      <td>0.107321</td>
      <td>0.118982</td>
      <td>0.121290</td>
      <td>0.479110</td>
      <td>0.102831</td>
      <td>0.144238</td>
      <td>0.141681</td>
      <td>1.646608</td>
    </tr>
    <tr>
      <th>79</th>
      <td>0.542545</td>
      <td>0.757917</td>
      <td>0.350051</td>
      <td>2.634509</td>
      <td>4.735602</td>
      <td>0.210526</td>
      <td>0.165671</td>
      <td>3.871971</td>
      <td>0.194297</td>
      <td>1.531613</td>
      <td>...</td>
      <td>0.244469</td>
      <td>0.133358</td>
      <td>0.112851</td>
      <td>0.128635</td>
      <td>0.142617</td>
      <td>0.438354</td>
      <td>0.110614</td>
      <td>0.155667</td>
      <td>0.146408</td>
      <td>1.607631</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 77 columns</p>
</div></div></div>
</div>
<p><strong>2.</strong> Using StandardScaler, standardize the loaded mouse protein dataset from <strong>Exercise 1</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>3.</strong> Now, run <strong>PCA</strong> from <strong>sklearn</strong> on the <strong>standardised mouse protein dataset</strong> to determine the <strong>top ten eigenvalues</strong>. (use <span class="math notranslate nohighlight">\(95\%\)</span> minimum number of principal components).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">for</span> <span class="n">eigenvector</span> <span class="ow">in</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">eigenvector</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">,</span> <span class="n">eigenvector</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>23.04876913576254
13.358730172070146
7.586459786950799
7.161540399168171
3.2824168810598997
3.042339638744958
2.36732540129829
2.2552750062859226
1.830754540428051
1.2787183994614408
</pre></div>
</div>
</div>
</div>
<p><strong>4.</strong> Using the <strong>Exercise 3</strong> fitted PCA object, find out the <strong>portion of explained variance (PVE)</strong> and its <strong>ratio</strong> for the top 10 principal components to select the number of principal components.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[23.09059993 13.38297469  7.60022832  7.17453775  3.28837408  3.04786113
  2.37162182  2.25936806  1.83407714  1.28103912]
[0.29933466 0.17349    0.09852545 0.09300702 0.04262879 0.0395109
 0.03074449 0.02928929 0.02377603 0.01660673]
</pre></div>
</div>
</div>
</div>
<p><strong>5.</strong> After observing the <strong>PVE</strong> from <strong>Exercise 4</strong> carefuly,find out how many <strong>principal components</strong> should we take to preserve <span class="math notranslate nohighlight">\(70\%\)</span> of variance of the data?</p>
<div class="toggle docutils container">
<p><strong>First 5 principal components.</strong></p>
</div>
<p><strong>6.</strong> Finally, plot the <strong>PVE</strong> of each principal component and the <strong>cumulative PVE</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="s2">&quot;-o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Individual component&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[:</span><span class="mi">10</span><span class="p">]),</span> <span class="s2">&quot;-s&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Cumulative&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Proportion of Variance Explained&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Principal Component&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pca_51_0.png" src="../_images/pca_51_0.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./09-pca-clustering"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="overview.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">9. </span>Principal Component Analysis &amp; Clustering</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="clustering.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9.2. </span><span class="math notranslate nohighlight">\(K\)</span>-means and hierarchical clustering</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Haiping Lu and Shuo Zhou<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>