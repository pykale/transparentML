
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9.2. \(K\)-means and hierarchical clustering &#8212; Transparent ML Intro</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="https://raw.githubusercontent.com/pykale/transparentML/main/content/transparentml-logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.3. Quiz and summary" href="quiz-sum-ref.html" />
    <link rel="prev" title="9.1. Principal component analysis" href="pca.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-89ZWPQL897"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-89ZWPQL897');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/transparentml-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Transparent ML Intro</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Overview
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/pykale/transparentML/discussions">
   Discussion forum
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../00-prereq/overview.html">
   Prerequisites
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/linear-algebra-and-notations.html">
     Linear algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/basic-python.html">
     Python basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/numerical-programming.html">
     Numerical programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/graphics.html">
     Graphics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/loading-data.html">
     Loading data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/quiz-sum-ref.html">
     Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Primary
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01-intro/overview.html">
   1. Intro ML &amp; transparency
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/what-is-ml.html">
     1.1. What is ML?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/ml-systems.html">
     1.2. ML systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/ml-process.html">
     1.3. ML process
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/ml-transp.html">
     1.4. ML transparency
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/knn.html">
     1.5. K-NN classifier
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/organisation.html">
     1.6. Organisation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/quiz-sum-ref.html">
     1.7. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-linear-reg/overview.html">
   2. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/simple-linear-regression.html">
     2.1. Simple linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/multi-linear-regression.html">
     2.2. Multiple linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/extension-limitation.html">
     2.3. Extensions &amp; limitations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/quiz-sum-ref.html">
     2.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-logistic-reg/overview.html">
   3. Logistic regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-logistic-reg/regress-to-classify.html">
     3.1. Regress to classify?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-logistic-reg/logistic-regression.html">
     3.2. Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-logistic-reg/quiz-sum-ref.html">
     3.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-hypo-test-sw-dev/overview.html">
   4. Hypothesis test &amp; software dev
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-hypo-test-sw-dev/hypothesis-testing.html">
     4.1. Hypothesis testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-hypo-test-sw-dev/software-development.html">
     4.2. Software development
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-hypo-test-sw-dev/quiz-sum-ref.html">
     4.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-cross-val-bootstrap/overview.html">
   5. Cross validation &amp; bootstrap
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-cross-val-bootstrap/cross-validation.html">
     5.1. Cross-validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-cross-val-bootstrap/bootstrap.html">
     5.2. Bootstrap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-cross-val-bootstrap/quiz-sum-ref.html">
     5.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Secondary
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-ftr-select-regularise/overview.html">
   6. Feature selection/regularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-ftr-select-regularise/feature-select.html">
     6.1. Feature selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-ftr-select-regularise/regularisation.html">
     6.2. Regularisation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-ftr-select-regularise/quiz-sum-ref.html">
     6.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-trees-ensembles/overview.html">
   7. Trees &amp; ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/regression-trees.html">
     7.1. Regression trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/classification-trees.html">
     7.2. Classification trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/ensembles.html">
     7.3. Ensemble learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/quiz-sum-ref.html">
     7.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-glm-svm/overview.html">
   8. GLM &amp; SVM
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/glm.html">
     8.1. Generalised linear models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/support-vec-classifier.html">
     8.2. Support vector classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/svm.html">
     8.3. Support vector machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/quiz-sum-ref.html">
     8.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="overview.html">
   9. PCA &amp; clustering
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="pca.html">
     9.1. Principal comp. analysis
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.2. Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="quiz-sum-ref.html">
     9.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-deep-cnn-rnn/overview.html">
   10. Neural nets &amp; deep learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-cnn-rnn/multilayer-nn.html">
     10.1. Multilayer neural nets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-cnn-rnn/convolutional-nn.html">
     10.2. Convolutional neural nets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-cnn-rnn/recurrent-nn.html">
     10.3. Recurrent neural nets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-cnn-rnn/quiz-sum-ref.html">
     10.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/system-transp.html">
   System transparency
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/process-transp.html">
   Process transparency
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/09-pca-clustering/clustering.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/pykale/transparentML"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/pykale/transparentML/issues/new?title=Issue%20on%20page%20%2F09-pca-clustering/clustering.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/pykale/transparentML/edit/main/content/09-pca-clustering/clustering.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/pykale/transparentML/main?urlpath=tree/content/09-pca-clustering/clustering.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/pykale/transparentML/blob/main/content/09-pca-clustering/clustering.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-means-clustering">
   9.2.1.
   <span class="math notranslate nohighlight">
    \(K\)
   </span>
   -means clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means-objective-function">
     9.2.1.1.
     <span class="math notranslate nohighlight">
      \(K\)
     </span>
     -means objective function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means-algorithm">
     9.2.1.2.
     <span class="math notranslate nohighlight">
      \(K\)
     </span>
     -means algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means-clustering-on-toy-data-with-scikit-learn">
     9.2.1.3.
     <span class="math notranslate nohighlight">
      \(K\)
     </span>
     -means clustering on toy data with
     <code class="docutils literal notranslate">
      <span class="pre">
       scikit-learn
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hierarchical-clustering">
   9.2.2. Hierarchical clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchical-clustering-on-toy-data">
     9.2.2.1. Hierarchical clustering on toy data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-hierarchical-clustering-algorithm">
     9.2.2.2. The hierarchical clustering Algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchical-clustering-to-explore-cancer-cell-line-microarray-data">
     9.2.2.3. Hierarchical clustering to explore cancer cell line microarray data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   9.2.3. Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>K-means and hierarchical clustering</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-means-clustering">
   9.2.1.
   <span class="math notranslate nohighlight">
    \(K\)
   </span>
   -means clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means-objective-function">
     9.2.1.1.
     <span class="math notranslate nohighlight">
      \(K\)
     </span>
     -means objective function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means-algorithm">
     9.2.1.2.
     <span class="math notranslate nohighlight">
      \(K\)
     </span>
     -means algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#k-means-clustering-on-toy-data-with-scikit-learn">
     9.2.1.3.
     <span class="math notranslate nohighlight">
      \(K\)
     </span>
     -means clustering on toy data with
     <code class="docutils literal notranslate">
      <span class="pre">
       scikit-learn
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hierarchical-clustering">
   9.2.2. Hierarchical clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchical-clustering-on-toy-data">
     9.2.2.1. Hierarchical clustering on toy data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-hierarchical-clustering-algorithm">
     9.2.2.2. The hierarchical clustering Algorithm
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchical-clustering-to-explore-cancer-cell-line-microarray-data">
     9.2.2.3. Hierarchical clustering to explore cancer cell line microarray data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   9.2.3. Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="k-means-and-hierarchical-clustering">
<h1><span class="section-number">9.2. </span><span class="math notranslate nohighlight">\(K\)</span>-means and hierarchical clustering<a class="headerlink" href="#k-means-and-hierarchical-clustering" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Cluster_analysis">Clustering</a>, or cluster analysis, refers to methods for finding subgroups, or clusters, in a dataset. When we cluster the observations (data samples) of a dataset, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. Typically, similarity between observations is defined as the Euclidean distance between their features.</p>
 <!-- For instance, suppose that we have a set of $N$ observations, each with $D$ features. -->
<p>Both clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:</p>
<ul class="simple">
<li><p>PCA looks to find a lower-dimensional representation of the observations that explain a good proportion of the variance;</p></li>
<li><p>Clustering looks to find homogeneous subgroups among the observations.</p></li>
</ul>
<p>Recall <a class="reference internal" href="../01-intro/ml-systems.html#mlproblems-table"><span class="std std-numref">Table 1.1</span></a>, PCA and clustering can be viewed as regression and classification problems under the unsupervised learning paradigm, respectively. PCA outputs a set of numerical values (lower-dimensional feature), while clustering outputs a set of labels (cluster indices).</p>
<p>There are many clustering methods. In this section, we focus on two popular clustering approaches: <span class="math notranslate nohighlight">\(K\)</span>-<em>means clustering</em> and <em>hierarchical clustering</em>. In <span class="math notranslate nohighlight">\(K\)</span>-means clustering, we seek to partition the observations into a pre-specified number of clusters. On the other hand, in hierarchical clustering, we do not know in advance how many clusters we want so we produce a tree-like visual representation of the observations, called a <a class="reference external" href="https://en.wikipedia.org/wiki/Dendrogram">dendrogram</a>, allowing us to view at once the clusterings obtained for each possible number of clusters, from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>While we focus on clustering observations on the basis of the features in order to identify subgroups among the observations, we can also cluster features on the basis of the observations in order to discover subgroups among the features, e.g. by simply transposing the data matrix.</p>
<!-- In general, we can cluster observations on the basis of the features in order to identify subgroups among the observations, or we can cluster features on the basis of the observations in order to discover subgroups among the features. In what follows, for simplicity we will discuss clustering observations on the basis of the features, though the converse can be performed by simply transposing the data matrix. --><div class="section" id="k-means-clustering">
<h2><span class="section-number">9.2.1. </span><span class="math notranslate nohighlight">\(K\)</span>-means clustering<a class="headerlink" href="#k-means-clustering" title="Permalink to this headline">¶</a></h2>
<p><span class="math notranslate nohighlight">\(K\)</span>-means clustering is a simple and elegant approach for partitioning a dataset into <span class="math notranslate nohighlight">\(K\)</span> distinct, non-overlapping clusters, each represented by its cluster centre (centroid). To perform <span class="math notranslate nohighlight">\(K\)</span>-means clustering, we must first specify the desired number of clusters <span class="math notranslate nohighlight">\(K\)</span>; then the <span class="math notranslate nohighlight">\(K\)</span>-means algorithm will assign each observation to exactly one of the <span class="math notranslate nohighlight">\(K\)</span> clusters.</p>
<p>Watch the 8-minute video below for a visual explanation of <span class="math notranslate nohighlight">\(K\)</span>-means clustering.</p>
<div class="admonition-video admonition">
<p class="admonition-title">Video</p>
<iframe width="700" height="394" src="https://www.youtube.com/embed/4b5d3muPQmA?start=14" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p><a class="reference external" href="https://www.youtube.com/embed/4b5d3muPQmA?start=14">Explaining <span class="math notranslate nohighlight">\(K\)</span>-Means Clustering by StatQuest</a>, embedded according to <a class="reference external" href="https://www.youtube.com/static?gl=CA&amp;template=terms">YouTube’s Terms of Service</a>.</p>
</div>
<div class="section" id="k-means-objective-function">
<h3><span class="section-number">9.2.1.1. </span><span class="math notranslate nohighlight">\(K\)</span>-means objective function<a class="headerlink" href="#k-means-objective-function" title="Permalink to this headline">¶</a></h3>
<p>The <span class="math notranslate nohighlight">\(K\)</span>-means clustering procedure results from a simple and intuitive mathematical problem. We begin by defining some notation. Let <span class="math notranslate nohighlight">\(C_1, \ldots, C_K\)</span> denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(C_1 \cup C_2 \cup \ldots \cup C_K = {1, \ldots , N} \)</span>. In other words, each observation belongs to at least one of the <span class="math notranslate nohighlight">\(K\)</span> clusters.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_k \cap C_{k^′} = \Phi \text{ for all } k \neq k^′\)</span>. In other words, the clusters are non-overlapping: no observation belongs to more than one cluster.</p></li>
</ol>
<p>For instance, if the <span class="math notranslate nohighlight">\(n\)</span>th observation is in the <span class="math notranslate nohighlight">\(k\text{th}\)</span> cluster, then <span class="math notranslate nohighlight">\(n \in C_k \)</span>. The idea behind <span class="math notranslate nohighlight">\(K\)</span>-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. The within-cluster variation
for cluster <span class="math notranslate nohighlight">\(C_k\)</span> is a measure <span class="math notranslate nohighlight">\(W(C_k)\)</span> of the amount by which the observations within a cluster differ from each other, a concept similar to variation studied in PCA. Hence we want to solve the problem</p>
<div class="math notranslate nohighlight" id="equation-kmeans-problem">
<span class="eqno">(9.3)<a class="headerlink" href="#equation-kmeans-problem" title="Permalink to this equation">¶</a></span>\[\min_{C_1, \ldots, C_K} \sum_{k=1}^K W(C_k).\]</div>
<p>In words, this formula says that we want to partition the observations into <span class="math notranslate nohighlight">\(K\)</span> clusters such that the total within-cluster variation, summed over all <span class="math notranslate nohighlight">\(K\)</span> clusters, is as small as possible.</p>
<p>In order to make solving Equation <a class="reference internal" href="#equation-kmeans-problem">(9.3)</a> actionable we need to define the within-cluster variation. There are many possible ways to define this concept, but by far the most common choice involves <em>squared Euclidean distance</em>. That is, we define</p>
<div class="math notranslate nohighlight" id="equation-within-cluster-var">
<span class="eqno">(9.4)<a class="headerlink" href="#equation-within-cluster-var" title="Permalink to this equation">¶</a></span>\[W(C_k) = \frac{1}{|C_k|} \sum_{i,j \in C_k} \sum_{d=1}^{D} \left( x_{i,d} - x_{j,d} \right)^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(|C_k|\)</span> denotes the number of observations in the <span class="math notranslate nohighlight">\(k\text{th}\)</span>  cluster. In other words, the within-cluster variation for the <span class="math notranslate nohighlight">\(k\)</span>th cluster is the sum of all of the pairwise squared Euclidean distances between the observations in the <span class="math notranslate nohighlight">\(k\text{th}\)</span> cluster, divided by the total number of observations in the <span class="math notranslate nohighlight">\(k\text{th}\)</span>  cluster.</p>
<p>Combining Equations <a class="reference internal" href="#equation-kmeans-problem">(9.3)</a> and <a class="reference internal" href="#equation-within-cluster-var">(9.4)</a>, gives the optimization problem that defines
<span class="math notranslate nohighlight">\(K\)</span>-means clustering,</p>
<div class="math notranslate nohighlight" id="equation-kmeans-obj">
<span class="eqno">(9.5)<a class="headerlink" href="#equation-kmeans-obj" title="Permalink to this equation">¶</a></span>\[\min_{C_1, \ldots, C_K} \left\{\sum_{k=1}^K \frac{1}{|C_k|} \sum_{i,j \in C_k} \sum_{d=1}^{D} \left( x_{i,d} - x_{j,d} \right)^2\right\}.\]</div>
</div>
<div class="section" id="k-means-algorithm">
<h3><span class="section-number">9.2.1.2. </span><span class="math notranslate nohighlight">\(K\)</span>-means algorithm<a class="headerlink" href="#k-means-algorithm" title="Permalink to this headline">¶</a></h3>
<p>Now, we would like to find an algorithm to solve Equation <a class="reference internal" href="#equation-kmeans-obj">(9.5)</a>—that is, a method to partition the observations into <span class="math notranslate nohighlight">\(K\)</span> clusters such that the objective of Equation <a class="reference internal" href="#equation-kmeans-obj">(9.5)</a> is minimised. This is in fact a very difficult problem to solve precisely, since there are almost <span class="math notranslate nohighlight">\(K^N\)</span> ways to partition <span class="math notranslate nohighlight">\(N\)</span> observations into <span class="math notranslate nohighlight">\(K\)</span> clusters. This is a huge number unless <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(N\)</span> are tiny! Fortunately, a very simple algorithm can be shown to provide a local optimum, i.e. a pretty good (but not necessarily the best) solution, to the <span class="math notranslate nohighlight">\(K\)</span>-means optimisation problem <a class="reference internal" href="#equation-kmeans-obj">(9.5)</a>. This approach is laid out in <a class="reference internal" href="#alg:kmeans-algorithm">Algorithm 9.1</a>.</p>
<div class="proof algorithm admonition" id="alg:kmeans-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 9.1 </span> (<span class="math notranslate nohighlight">\(K\)</span>-means Clustering)</p>
<div class="algorithm-content section" id="proof-content">
<ol>
<li><p>Randomly assign a number, from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(K\)</span>, to each of the observations. These serve as initial cluster assignments for the observations.</p></li>
<li><p>Iterate until the cluster assignments stop changing:</p>
<p>(a) For each of the <span class="math notranslate nohighlight">\(K\)</span> clusters, compute the cluster centroid, which is the vector of the <span class="math notranslate nohighlight">\(D\)</span> feature means of the observations in the <span class="math notranslate nohighlight">\(k\text{th}\)</span> cluster.</p>
<p>(b) Assign each observation to the cluster for which the squared Euclidean distance between the observation and the cluster centroid is smallest.</p>
</li>
</ol>
</div>
</div><p><a class="reference internal" href="#alg:kmeans-algorithm">Algorithm 9.1</a> is guaranteed to decrease the value of the objective <a class="reference internal" href="#equation-kmeans-obj">(9.5)</a> at each step. To understand why, the following identity is illuminating:</p>
<div class="math notranslate nohighlight" id="equation-kmeans-obj-identity">
<span class="eqno">(9.6)<a class="headerlink" href="#equation-kmeans-obj-identity" title="Permalink to this equation">¶</a></span>\[\frac{1}{|C_k|} \sum_{i,j \in C_k} \sum_{d=1}^{D} \left( x_{i,d} - x_{j,d} \right)^2 = 2 \sum_{i \in C_k} \sum_{d=1}^{D} \left( x_{i,d} - \bar{x}_{k,d} \right)^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{x}_{k,d}\)</span> is the mean of the <span class="math notranslate nohighlight">\(d\text{th}\)</span> feature for the observations in the <span class="math notranslate nohighlight">\(k\text{th}\)</span> cluster. In Step 2(a) of <a class="reference internal" href="#alg:kmeans-algorithm">Algorithm 9.1</a>, the cluster means for each feature are the constants that minimise the sum-of-squared deviations. In Step 2(b), reallocating the observations can only improve Equation <a class="reference internal" href="#equation-kmeans-obj-identity">(9.6)</a>. This means that as the algorithm is run, the clustering obtained will continually improve until the result no longer changes; the objective of Equation <a class="reference internal" href="#equation-kmeans-obj">(9.5)</a> will never increase. When the result no longer changes, a local optimum has been reached.</p>
<p>Because the <span class="math notranslate nohighlight">\(K\)</span>-means algorithm finds a local rather than a global optimum, the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of <a class="reference internal" href="#alg:kmeans-algorithm">Algorithm 9.1</a>. For this reason, it is important to run the algorithm multiple times from different random initial configurations and to set a random seed for reproducibility. Then one selects the best solution, i.e. that for which the objective in Eq. <a class="reference internal" href="#equation-kmeans-obj">(9.5)</a> is the smallest.</p>
</div>
<div class="section" id="k-means-clustering-on-toy-data-with-scikit-learn">
<h3><span class="section-number">9.2.1.3. </span><span class="math notranslate nohighlight">\(K\)</span>-means clustering on toy data with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code><a class="headerlink" href="#k-means-clustering-on-toy-data-with-scikit-learn" title="Permalink to this headline">¶</a></h3>
<p>Get ready by importing the APIs needed from respective libraries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">svd</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">scipy.cluster</span> <span class="kn">import</span> <span class="n">hierarchy</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<p>Generate some toy data to study the <span class="math notranslate nohighlight">\(K\)</span>-means algorithm. Set a random seed for reproducibility.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">X</span><span class="p">[:</span><span class="mi">25</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">25</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">3</span>
<span class="n">X</span><span class="p">[:</span><span class="mi">25</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">25</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">4</span>
</pre></div>
</div>
</div>
</div>
<p>Fit <span class="math notranslate nohighlight">\(K\)</span>-means models with <span class="math notranslate nohighlight">\(K=2\)</span> and <span class="math notranslate nohighlight">\(3\)</span>, respectively:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">km1</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">km1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">km2</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">km2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KMeans(n_clusters=3, n_init=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">KMeans</label><div class="sk-toggleable__content"><pre>KMeans(n_clusters=3, n_init=20)</pre></div></div></div></div></div></div></div>
</div>
<p>Inspect the first <span class="math notranslate nohighlight">\(K\)</span>-means model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">km1</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">dir</span><span class="p">(</span><span class="n">km1</span><span class="p">))</span>  <span class="c1"># we can use dir to see other saved attributes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1]
[&#39;__abstractmethods__&#39;, &#39;__annotations__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getstate__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setstate__&#39;, &#39;__sizeof__&#39;, &#39;__slots__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_abc_impl&#39;, &#39;_algorithm&#39;, &#39;_check_feature_names&#39;, &#39;_check_mkl_vcomp&#39;, &#39;_check_n_features&#39;, &#39;_check_params_vs_input&#39;, &#39;_check_test_data&#39;, &#39;_estimator_type&#39;, &#39;_get_param_names&#39;, &#39;_get_tags&#39;, &#39;_init_centroids&#39;, &#39;_more_tags&#39;, &#39;_n_features_out&#39;, &#39;_n_init&#39;, &#39;_n_threads&#39;, &#39;_parameter_constraints&#39;, &#39;_repr_html_&#39;, &#39;_repr_html_inner&#39;, &#39;_repr_mimebundle_&#39;, &#39;_sklearn_auto_wrap_output_keys&#39;, &#39;_tol&#39;, &#39;_transform&#39;, &#39;_validate_center_shape&#39;, &#39;_validate_data&#39;, &#39;_validate_params&#39;, &#39;_warn_mkl_vcomp&#39;, &#39;algorithm&#39;, &#39;cluster_centers_&#39;, &#39;copy_x&#39;, &#39;fit&#39;, &#39;fit_predict&#39;, &#39;fit_transform&#39;, &#39;get_feature_names_out&#39;, &#39;get_params&#39;, &#39;inertia_&#39;, &#39;init&#39;, &#39;labels_&#39;, &#39;max_iter&#39;, &#39;n_clusters&#39;, &#39;n_features_in_&#39;, &#39;n_init&#39;, &#39;n_iter_&#39;, &#39;predict&#39;, &#39;random_state&#39;, &#39;score&#39;, &#39;set_output&#39;, &#39;set_params&#39;, &#39;tol&#39;, &#39;transform&#39;, &#39;verbose&#39;]
</pre></div>
</div>
</div>
</div>
<p>We can see the cluster assignments for each observation.</p>
<p>Let us plot a figure that shows the results obtained from performing <span class="math notranslate nohighlight">\(K\)</span>-means clustering on the simulated examples, using two different values of <span class="math notranslate nohighlight">\(K\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">km1</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">prism</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;$K$-means Clustering Results with $K$=2&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">km1</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">km1</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">facecolors</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
    <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span>
    <span class="n">linewidths</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">km2</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">prism</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;$K$-means Clustering Results with $K$=3&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">km2</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">km2</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/clustering_12_0.png" src="../_images/clustering_12_0.png" />
</div>
</div>
<p>Let us examine a particular data point to explain the system transparency of <span class="math notranslate nohighlight">\(K\)</span>-means clustering.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data sample point : </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Centre coordinates for K=2: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">km1</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Distance of sample point from each cluster centre: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">km1</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data sample point : 
 [ 4.04 -5.26]
Centre coordinates for K=2: 
 [[ 3.09 -4.12]
 [-0.06  0.51]]
Distance of sample point from each cluster centre: 
 [1.48 7.08]
</pre></div>
</div>
</div>
</div>
<div class="important admonition">
<p class="admonition-title">System transparency</p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(K=2\)</span>, the centres of the two clusters are <span class="math notranslate nohighlight">\([ 3.09 \;\; -4.12]\)</span> (red cluster) and <span class="math notranslate nohighlight">\([-0.06 \;\;  0.51]\)</span> (green cluster). For the data point <span class="math notranslate nohighlight">\( [4.04 \;\; -5.26]^{\top} \)</span>, its distance to the red cluster centre is <span class="math notranslate nohighlight">\(1.48\)</span> and to the green cluster centre is <span class="math notranslate nohighlight">\(7.08\)</span>. Hence, it is assigned to the red cluster.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(K=2\)</span>, given a particular cluster <span class="math notranslate nohighlight">\(k\)</span>, e.g. the green cluster centred at <span class="math notranslate nohighlight">\([-0.06 \;\;  0.51]\)</span>, any data point <span class="math notranslate nohighlight">\([x_1, x_2]\)</span> belong to this cluster should satisfy the following condition:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\begin{aligned}
\sqrt{(x_1 - (-0.06))^2 + (x_2 - 0.51)^2} &lt; &amp; \sqrt{(x_1-3.09)^2 + (x_2-(-4.12))^2} \\
0.12 x_1 - 1.02x_2 + 0.2637  &lt; &amp; - 6.18 x_1 + 8.14x_2 + 26.5225 \\
6.3 x_1 - 9.16x_2 &lt; &amp; 26.2588
\end{aligned} 
\end{align*}\]</div>
<p>Note: numbers are limited to two decimal places for the sake of clarity.</p>
</div>
</div>
</div>
<div class="section" id="hierarchical-clustering">
<h2><span class="section-number">9.2.2. </span>Hierarchical clustering<a class="headerlink" href="#hierarchical-clustering" title="Permalink to this headline">¶</a></h2>
<p>One potential disadvantage of <span class="math notranslate nohighlight">\(K\)</span>-means clustering is that it requires us to pre-specify the number of clusters <span class="math notranslate nohighlight">\(K\)</span>, which will need to employ a model selection (hyperparameter tuning) method. Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of <span class="math notranslate nohighlight">\(K\)</span>. Hierarchical clustering has an added advantage over <span class="math notranslate nohighlight">\(K\)</span>-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram.</p>
<div class="section" id="hierarchical-clustering-on-toy-data">
<h3><span class="section-number">9.2.2.1. </span>Hierarchical clustering on toy data<a class="headerlink" href="#hierarchical-clustering-on-toy-data" title="Permalink to this headline">¶</a></h3>
<p>Using the same toy data above, let us plot a figure to demonstrate <em>bottom-up</em> or <em>agglomerative</em> clustering. This is the most common type of hierarchical clustering, and refers to the fact that a dendrogram (generally depicted as an upside-down tree) is built starting from the leaves and combining clusters up to the trunk.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>

<span class="k">for</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
    <span class="p">[</span><span class="n">hierarchy</span><span class="o">.</span><span class="n">complete</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">single</span><span class="p">(</span><span class="n">X</span><span class="p">)],</span>
    <span class="p">[</span><span class="s2">&quot;c1&quot;</span><span class="p">,</span> <span class="s2">&quot;c2&quot;</span><span class="p">,</span> <span class="s2">&quot;c3&quot;</span><span class="p">],</span>
    <span class="p">[</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">],</span>
<span class="p">):</span>
    <span class="n">cluster</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">dendrogram</span><span class="p">(</span><span class="n">linkage</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color_threshold</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Complete Linkage&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Average Linkage&quot;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Single Linkage&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/clustering_17_0.png" src="../_images/clustering_17_0.png" />
</div>
</div>
<p>In the top panel of the figure above, each leaf (at the bottom) of the dendrogram represents one of the 50 toy examples generated previously. However, as we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other. On the other hand, observations that fuse later (near the top of the tree) can be quite different. In fact, this statement can be made precise: for any two observations, we can look for the point in the tree where branches containing those two observations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.</p>
</div>
<div class="section" id="the-hierarchical-clustering-algorithm">
<h3><span class="section-number">9.2.2.2. </span>The hierarchical clustering Algorithm<a class="headerlink" href="#the-hierarchical-clustering-algorithm" title="Permalink to this headline">¶</a></h3>
<p>The hierarchical clustering dendrogram is obtained via an extremely simple algorithm. We begin by defining some sort of dissimilarity measure between each pair of observations. Most often, Euclidean distance is used. The algorithm proceeds iteratively. Starting out at the bottom of the dendrogram, each of the <span class="math notranslate nohighlight">\(N\)</span> observations is treated as its own cluster. The two clusters that are most similar to each other are then fused so that there are <span class="math notranslate nohighlight">\(N\)</span> − 1 clusters after this first fusion. Next the two clusters that are most similar to each other are fused again, so that there are <span class="math notranslate nohighlight">\(N\)</span> − 2 clusters after the second fusion. The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete. <a class="reference internal" href="#alg:hierarchical-algorithm">Algorithm 9.2</a> summarises the hierarchical clustering algorithm.</p>
<div class="proof algorithm admonition" id="alg:hierarchical-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 9.2 </span> (Hierarchical clustering)</p>
<div class="algorithm-content section" id="proof-content">
<ol>
<li><p>Begin with <span class="math notranslate nohighlight">\(N\)</span> observations and a measure (such as Euclidean distance) of all the <span class="math notranslate nohighlight">\(N(N − 1)/2\)</span> pairwise dissimilarities. Treat each observation as its own cluster.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(n = N, N − 1, \ldots, 2\)</span>:</p>
<p>(a) Examine all pairwise inter-cluster dissimilarities among the <span class="math notranslate nohighlight">\(n\)</span> clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.</p>
<p>(b) Compute the new pairwise inter-cluster dissimilarities among the <span class="math notranslate nohighlight">\(n − 1\)</span> remaining clusters.</p>
</li>
</ol>
</div>
</div><p>We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations? The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. This extension is achieved by developing the notion of linkage, which defines the dissimilarity between two groups of observations. The four most common types of linkage—<em>complete, average, single,</em> and <em>centroid</em>. <a class="reference internal" href="#linkage-clustering"><span class="std std-numref">Table 9.1</span></a> summarises the four types of linkage.</p>
<p>The figure generated in the code cell above shows the clustering results of average, complete, and single linkage on the toy dataset, where we can observe that average and complete linkage methods tend to yield more balanced clusters (trees).</p>
<table class="colwidths-auto table" id="linkage-clustering">
<caption><span class="caption-number">Table 9.1 </span><span class="caption-text">Types of linkage in hierarchical clustering</span><a class="headerlink" href="#linkage-clustering" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Linkage</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Complete</strong></p></td>
<td><p>Maximal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Single</strong></p></td>
<td><p>Minimal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Average</strong></p></td>
<td><p>Mean inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Centroid</strong></p></td>
<td><p>Dissimilarity between the centroid for cluster A (a mean vector of length <span class="math notranslate nohighlight">\(D\)</span>) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="hierarchical-clustering-to-explore-cancer-cell-line-microarray-data">
<h3><span class="section-number">9.2.2.3. </span>Hierarchical clustering to explore cancer cell line microarray data<a class="headerlink" href="#hierarchical-clustering-to-explore-cancer-cell-line-microarray-data" title="Permalink to this headline">¶</a></h3>
<!-- the Observations of the NCI60 Data -->
<p>We illustrate these hierarchical clustering on the <a class="reference external" href="https://github.com/pykale/transparentML/blob/main/data/NCI60_data.csv">NCI60</a> cancer cell line microarray data, which consists of 6,830 gene expression measurements on 64 cancer cell lines.</p>
<p>Load the data and inspect the first few rows of the data frame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://github.com/pykale/transparentML/raw/main/data/NCI60_data.csv&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://github.com/pykale/transparentML/raw/main/data/NCI60_labs.csv&quot;</span><span class="p">)</span>

<span class="n">y</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>CNS</td>
    </tr>
    <tr>
      <th>1</th>
      <td>CNS</td>
    </tr>
    <tr>
      <th>2</th>
      <td>CNS</td>
    </tr>
    <tr>
      <th>3</th>
      <td>RENAL</td>
    </tr>
    <tr>
      <th>4</th>
      <td>BREAST</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Each cell line is labelled with a cancer type, given in <code class="docutils literal notranslate"><span class="pre">y</span></code>. We do not make use of the cancer types in performing clustering since we are studying unsupervised learning here. After performing clustering, we will check to see the extent to which these cancer types agree with the clustering results.</p>
<p>Inspect the data size.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(64, 6830)
</pre></div>
</div>
</div>
</div>
<p>The data has 64 rows and 6,830 columns.</p>
<p>Standardise the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We now perform hierarchical clustering of the observations using complete, single, and average linkage. Euclidean distance is used as the dissimilarity measure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>

<span class="k">for</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
    <span class="p">[</span><span class="n">hierarchy</span><span class="o">.</span><span class="n">complete</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">),</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">single</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)],</span>
    <span class="p">[</span><span class="s2">&quot;c1&quot;</span><span class="p">,</span> <span class="s2">&quot;c2&quot;</span><span class="p">,</span> <span class="s2">&quot;c3&quot;</span><span class="p">],</span>
    <span class="p">[</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">],</span>
<span class="p">):</span>
    <span class="n">cluster</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">dendrogram</span><span class="p">(</span>
        <span class="n">linkage</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">X_scaled</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
        <span class="n">orientation</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span>
        <span class="n">color_threshold</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Complete Linkage&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Average Linkage&quot;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Single Linkage&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/clustering_27_0.png" src="../_images/clustering_27_0.png" />
</div>
</div>
<p>We can see the above for the <code class="docutils literal notranslate"><span class="pre">NCI60</span></code> cancer cell line microarray data clustered with average, complete, and single linkage, using Euclidean distance as the dissimilarity measure. As earlier, complete and average linkage tend to yield evenly sized clusters whereas single linkage tends to yield extended clusters to which single leaves are fused one by one.</p>
<p>After obtain the dendrogram, we can take a cut at the desired height to obtain a partition of the observations into <span class="math notranslate nohighlight">\(K\)</span> clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">cut4</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">dendrogram</span><span class="p">(</span>
    <span class="n">hierarchy</span><span class="o">.</span><span class="n">complete</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">),</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">X_scaled</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
    <span class="n">orientation</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span>
    <span class="n">color_threshold</span><span class="o">=</span><span class="mi">140</span><span class="p">,</span>
    <span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span>
    <span class="mi">140</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">get_data_interval</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/clustering_29_0.png" src="../_images/clustering_29_0.png" />
</div>
</div>
<p>Here, we take a cut at a height of 140 to obtain four clusters.</p>
</div>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">9.2.3. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<!-- Clustering the Observations of the NCI60 Data --><p><strong>1.</strong> In this exercise, you will generate your own simulated data and perform PCA and <span class="math notranslate nohighlight">\(K\)</span>-means clustering.</p>
<p><strong>(a)</strong> Generate a simulated 50-dimensional dataset consisting of 3 classes, with each class containing <span class="math notranslate nohighlight">\(20\)</span> observations. Your dataset should have a shape of [60, 50], i.e <span class="math notranslate nohighlight">\(60\)</span> observations in total, nad each with <span class="math notranslate nohighlight">\(50\)</span> variables. Think carefully about how you can define your 3 classes. For example, you could use the <code class="docutils literal notranslate"><span class="pre">np.random.normal()</span></code> function to create your 3 classes, changing the mean and standard deviation of each class using the loc &amp; scale parameters!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">## Generate the data</span>
<span class="n">true_cluster_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">true_cluster_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">true_cluster_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>

<span class="c1">## Combine the data into a single array</span>
<span class="n">combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
    <span class="n">true_cluster_1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">true_cluster_2</span><span class="p">,</span> <span class="n">true_cluster_3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>(b)</strong> Perform <strong>PCA</strong> on the <span class="math notranslate nohighlight">\(60\)</span> observations and plot the <strong>first two principal component</strong> score vectors. Use a different colour to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part <strong>(c)</strong>. If not, then return to part <strong>(a)</strong> and modify the simulation so that there is greater separation between the three classes. Do not continue to part <strong>(c)</strong> until the three classes show at least some separation in the first two principal component score vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1">## Break the data into Principal components</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Cluster 1&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="mi">20</span><span class="p">:</span><span class="mi">40</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="mi">20</span><span class="p">:</span><span class="mi">40</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Cluster 2&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">:</span><span class="mi">60</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">:</span><span class="mi">60</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Cluster 3&quot;</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;PC 1&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;PC 2&quot;</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Principal component plot of three distinct clusters&quot;</span><span class="p">)</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/clustering_40_0.png" src="../_images/clustering_40_0.png" />
</div>
</div>
<p><strong>(c)</strong> Perform <span class="math notranslate nohighlight">\(K\)</span>-means clustering of the observations with <span class="math notranslate nohighlight">\(K = 3\)</span>. How well do the clusters that you obtained in <span class="math notranslate nohighlight">\(K\)</span>-means clustering compare to the true class labels?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2022</span><span class="p">)</span>
<span class="c1">## Break the data into Principal components</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 1 predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 2 predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">20</span><span class="p">:</span><span class="mi">40</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 3 predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">:</span><span class="mi">60</span><span class="p">])</span>

<span class="c1"># We see that there is perfect recall of the true clusters. We can not produce a nice plot as in part b) as it is 50 dimensional.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True Cluster 1 predictions:
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
True Cluster 2 predictions:
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
True Cluster 3 predictions:
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
</pre></div>
</div>
</div>
</div>
<p><strong>(d)</strong> Perform <span class="math notranslate nohighlight">\(K\)</span>-means clustering with <span class="math notranslate nohighlight">\(K = 2\)</span>. Describe your results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Break the data into Principal components</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2022</span><span class="p">)</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 1 predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 2 predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">20</span><span class="p">:</span><span class="mi">40</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 3 predictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">:</span><span class="mi">60</span><span class="p">])</span>

<span class="c1"># Here it has clustered the last two clusters into one.</span>
<span class="c1"># This seems intuitive looking at the PCA plot as these two clusters are the most similar on the first two PCs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True Cluster 1 predictions:
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
True Cluster 2 predictions:
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
True Cluster 3 predictions:
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to &#39;auto&#39; in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<p><strong>(d)</strong> Now perform <span class="math notranslate nohighlight">\(K\)</span>-means clustering with <span class="math notranslate nohighlight">\(K = 4\)</span> , and describe your results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Break the data into Principal components</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2022</span><span class="p">)</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 1 prdictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 2 prdictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">20</span><span class="p">:</span><span class="mi">40</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 3 prdictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">:</span><span class="mi">60</span><span class="p">])</span>

<span class="c1"># Setting the number of desired clusters as more than the true number means that at least one will have to split up.</span>
<span class="c1"># In this example, setting n=4 makes the second cluster split into two. This is because it has the most variance compared to the other two clusters.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True Cluster 1 prdictions:
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
True Cluster 2 prdictions:
[2 3 2 2 2 3 3 3 2 2 2 3 3 2 2 3 2 2 3 2]
True Cluster 3 prdictions:
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to &#39;auto&#39; in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<p><strong>(e)</strong> Using the StandardScaler function, perform <span class="math notranslate nohighlight">\(K\)</span>-means clustering with <span class="math notranslate nohighlight">\(K = 3\)</span> on the data after scaling each variable to have a standard deviation of one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Break the data into Principal components</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2022</span><span class="p">)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">(</span><span class="n">with_mean</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 1 prdictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 2 prdictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">20</span><span class="p">:</span><span class="mi">40</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Cluster 3 prdictions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">40</span><span class="p">:</span><span class="mi">60</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True Cluster 1 prdictions:
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
True Cluster 2 prdictions:
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
True Cluster 3 prdictions:
[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
</pre></div>
</div>
</div>
</div>
<p><strong>2.</strong> Consider the <a class="reference external" href="https://github.com/pykale/transparentML/raw/main/data/USArrests.csv">USArrests</a> dataset. We will now perform hierarchical clustering on the states.</p>
<p><strong>(a)</strong> Load the <strong>USArrests</strong> dataset, rename the index of each row to its corresponding state name and finally drop the column of state names from the original data frame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>


<span class="n">USArrests</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/pykale/transparentML/raw/main/data/USArrests.csv&quot;</span>
<span class="p">)</span>

<span class="n">state_name</span> <span class="o">=</span> <span class="n">USArrests</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># abstract a list of state names</span>

<span class="c1"># rename index of each row to its corresponding state name</span>
<span class="n">USArrests</span> <span class="o">=</span> <span class="n">USArrests</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">state_name</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>

<span class="c1"># drop the column of state name from the original data frame</span>
<span class="n">USArrests</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">USArrests</span><span class="o">.</span><span class="n">columns</span><span class="p">[[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">scaled_x</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">USArrests</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>(b)</strong> Using <strong>agglomerative hierarchical clustering</strong> with <strong>complete linkage</strong> and <strong>Euclidean distance</strong>, cluster the <strong>states</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span>

<span class="n">hc_complete1</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">USArrests</span><span class="p">,</span> <span class="s2">&quot;complete&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>(c)</strong> <strong>Cut the dendrogram</strong> at a height that results in <strong>three distinct clusters</strong>. Which states belong to which clusters?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">dendrogram</span><span class="p">,</span> <span class="n">cut_tree</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">res1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cut_tree</span><span class="p">(</span><span class="n">hc_complete1</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="n">state_name</span><span class="p">)</span>
<span class="n">res1</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;states&quot;</span>
<span class="n">res1</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;Clusters&quot;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># dendrogram</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Hierarchical Clustering Dendrogram on original data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;states&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Ecludian distance&quot;</span><span class="p">)</span>
<span class="n">dendrogram</span><span class="p">(</span><span class="n">hc_complete1</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">res1</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">res1</span><span class="p">)</span>

<span class="c1"># Cutting the dendrogram at height 150 results in three distinct clusters.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/clustering_69_0.png" src="../_images/clustering_69_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                Clusters
states                  
Alabama                0
Alaska                 0
Arizona                0
Arkansas               1
California             0
Colorado               1
Connecticut            2
Delaware               0
Florida                0
Georgia                1
Hawaii                 2
Idaho                  2
Illinois               0
Indiana                2
Iowa                   2
Kansas                 2
Kentucky               2
Louisiana              0
Maine                  2
Maryland               0
Massachusetts          1
Michigan               0
Minnesota              2
Mississippi            0
Missouri               1
Montana                2
Nebraska               2
Nevada                 0
New Hampshire          2
New Jersey             1
New Mexico             0
New York               0
North Carolina         0
North Dakota           2
Ohio                   2
Oklahoma               1
Oregon                 1
Pennsylvania           2
Rhode Island           1
South Carolina         0
South Dakota           2
Tennessee              1
Texas                  1
Utah                   2
Vermont                2
Virginia               1
Washington             1
West Virginia          2
Wisconsin              2
Wyoming                1
</pre></div>
</div>
</div>
</div>
<p><strong>(d)</strong> Perform <strong>agglomerative hierarchical clustering</strong> of the states using <strong>complete linkage</strong> and <strong>Euclidean distance</strong> after scaling the variables to have a standard deviation of one. Then, <strong>cut the dendrogram</strong> at a height that results in <strong>six distinct clusters</strong>. Which states belong to which clusters?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write your code below to answer the question</span>
</pre></div>
</div>
</div>
</div>
<p><em>Compare your answer with the reference solution below</em></p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hc_complete2</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">scaled_x</span><span class="p">,</span> <span class="s2">&quot;complete&quot;</span><span class="p">)</span>
<span class="n">res2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cut_tree</span><span class="p">(</span><span class="n">hc_complete2</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="mi">6</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="n">state_name</span><span class="p">)</span>
<span class="n">res2</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;states&quot;</span>
<span class="n">res2</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;ID&quot;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Hierarchical Clustering Dendrogram on scaled data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;states&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Ecludian distance&quot;</span><span class="p">)</span>
<span class="n">dendrogram</span><span class="p">(</span><span class="n">hc_complete2</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">res2</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">2.8</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">res2</span><span class="p">)</span>
<span class="c1"># Cutting the dendrogram at height 2.8 results in six distinct clusters.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/clustering_73_0.png" src="../_images/clustering_73_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                ID
states            
Alabama          0
Alaska           1
Arizona          2
Arkansas         3
California       2
Colorado         2
Connecticut      4
Delaware         4
Florida          2
Georgia          0
Hawaii           4
Idaho            5
Illinois         2
Indiana          4
Iowa             5
Kansas           4
Kentucky         3
Louisiana        0
Maine            5
Maryland         2
Massachusetts    4
Michigan         2
Minnesota        4
Mississippi      0
Missouri         3
Montana          5
Nebraska         5
Nevada           2
New Hampshire    5
New Jersey       4
New Mexico       2
New York         2
North Carolina   0
North Dakota     5
Ohio             4
Oklahoma         4
Oregon           3
Pennsylvania     4
Rhode Island     4
South Carolina   0
South Dakota     5
Tennessee        0
Texas            2
Utah             4
Vermont          5
Virginia         3
Washington       3
West Virginia    5
Wisconsin        4
Wyoming          3
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./09-pca-clustering"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="pca.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">9.1. </span>Principal component analysis</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="quiz-sum-ref.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9.3. </span>Quiz and summary</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Haiping Lu and Shuo Zhou<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>