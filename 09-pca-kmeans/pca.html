
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9.1. Principal components analysis &#8212; Transparent ML Intro</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.2. \(K\)-means clustering" href="kmeans.html" />
    <link rel="prev" title="9. Principal Component Analysis &amp; \(K\)-Means Clustering" href="overview.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/transparentml-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Transparent ML Intro</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Overview
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/pykale/transparentML/discussions">
   Discussion forum
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../00-prereq/overview.html">
   Prerequisites
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/linear-algebra-and-notations.html">
     Linear algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/basic-python.html">
     Python basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/numerical-programming.html">
     Numerical programming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/graphics.html">
     Graphics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/loading-data.html">
     Loading data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../00-prereq/quiz-sum-ref.html">
     Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Primary
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01-intro/overview.html">
   1. Intro ML &amp; Transparency
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/what-is-ml.html">
     1.1. What is ML?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/ml-systems.html">
     1.2. ML systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/ml-process.html">
     1.3. ML process
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/ml-transp.html">
     1.4. ML transparency
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/knn.html">
     1.5. K-NN classifier
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/organisation.html">
     1.6. Organisation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01-intro/quiz-sum-ref.html">
     1.7. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-linear-reg/overview.html">
   2. Linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/simple-linear-regression.html">
     2.1. Simple linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/multi-linear-regression.html">
     2.2. Multiple linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/extension-limitation.html">
     2.3. Extensions &amp; limitations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-linear-reg/quiz-sum-ref.html">
     2.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-logistic-reg/overview.html">
   3. Logistic regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-logistic-reg/regress-to-classify.html">
     3.1. Regress to classify?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-logistic-reg/logistic-regression.html">
     3.2. Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-logistic-reg/quiz-sum-ref.html">
     3.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-hypo-test-sw-dev/overview.html">
   4. Hypothesis test &amp; software dev
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-hypo-test-sw-dev/hypothesis-testing.html">
     4.1. Hypothesis testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-hypo-test-sw-dev/software-development.html">
     4.2. Software development
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-hypo-test-sw-dev/quiz-sum-ref.html">
     4.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-cross-val-bootstrap/overview.html">
   5. Cross validation &amp; bootstrap
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-cross-val-bootstrap/cross-validation.html">
     5.1. Cross-validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-cross-val-bootstrap/bootstrap.html">
     5.2. Bootstrap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-cross-val-bootstrap/quiz-sum-ref.html">
     5.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Secondary
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-ftr-select-regularise/overview.html">
   6. Feature selection/regularisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-ftr-select-regularise/feature-select.html">
     6.1. Feature selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-ftr-select-regularise/regularisation.html">
     6.2. Regularisation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-ftr-select-regularise/quiz-sum-ref.html">
     6.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-trees-ensembles/overview.html">
   7. Trees &amp; ensembles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/regression-trees.html">
     7.1. Regression trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/classification-trees.html">
     7.2. Classification trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/ensembles.html">
     7.3. Ensemble learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-trees-ensembles/quiz-sum-ref.html">
     7.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-glm-svm/overview.html">
   8. GLM &amp; SVM
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/glm.html">
     8.1. Generalised linear models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/support-vec-classifier.html">
     8.2. Support vector classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/svm.html">
     8.3. Support vector machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-glm-svm/quiz-sum-ref.html">
     8.4. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="overview.html">
   9. PCA &amp; K-means
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.1. Principal component analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="kmeans.html">
     9.2. K-means clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="quiz-sum-ref.html">
     9.3. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-deep-cnn-rnn/overview.html">
   10. Convolutional &amp; recurrent NN
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-deep-cnn-rnn/quiz-sum-ref.html">
     10.1. Quiz &amp; summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/system-transp.html">
   System transparency
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/process-transp.html">
   Process transparency
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/09-pca-kmeans/pca.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/pykale/transparentML"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/pykale/transparentML/issues/new?title=Issue%20on%20page%20%2F09-pca-kmeans/pca.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/pykale/transparentML/edit/main/content/09-pca-kmeans/pca.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/pykale/transparentML/main?urlpath=tree/content/09-pca-kmeans/pca.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/pykale/transparentML/blob/main/content/09-pca-kmeans/pca.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-are-principal-components">
   9.1.1. What are principal components?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-on-usarrests-dataset">
   9.1.2. PCA on
   <code class="docutils literal notranslate">
    <span class="pre">
     USArrests
    </span>
   </code>
   dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-proportion-of-variance-explained">
   9.1.3. The proportion of variance explained
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-on-toy-data-and-system-transparency">
   9.1.4. PCA on toy data and system transparency
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   9.1.5. Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Principal components analysis</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-are-principal-components">
   9.1.1. What are principal components?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-on-usarrests-dataset">
   9.1.2. PCA on
   <code class="docutils literal notranslate">
    <span class="pre">
     USArrests
    </span>
   </code>
   dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-proportion-of-variance-explained">
   9.1.3. The proportion of variance explained
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-on-toy-data-and-system-transparency">
   9.1.4. PCA on toy data and system transparency
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   9.1.5. Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="principal-components-analysis">
<h1><span class="section-number">9.1. </span>Principal components analysis<a class="headerlink" href="#principal-components-analysis" title="Permalink to this headline">¶</a></h1>
<p>Principal components analysis (PCA) is a technique for reducing the dimension of an data matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{D \times N}\)</span>.</p>
<!-- The first principal component direction of the data is that along which the observations vary the most. -->
<p>When faced with a large set of correlated variables, principal components allow us to summarise this set with a smaller number of representative variables that collectively explain most of the variability in the original set.</p>
<p>PCA refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. PCA is an unsupervised approach, since
it involves only a set of features <span class="math notranslate nohighlight">\(x_1, \ldots, x_D\)</span> and no associated response <span class="math notranslate nohighlight">\(y\)</span>. Apart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualisation (visualisation of the observations or visualisation of the variables). It can also be used as a tool for data imputation — that is, for filling in missing values in a data matrix.</p>
<p>Watch the 5-minute video below for a visual explanation of principal components analysis.</p>
<div class="admonition-video admonition">
<p class="admonition-title">Video</p>
<iframe width="700" height="394" src="https://www.youtube.com/embed/HMOI_lkzW08?start=10" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p><a class="reference external" href="https://www.youtube.com/embed/HMOI_lkzW08?start=10">Explaining main ideas behine principal components analysis, by StatQuest</a></p>
</div>
<div class="section" id="what-are-principal-components">
<h2><span class="section-number">9.1.1. </span>What are principal components?<a class="headerlink" href="#what-are-principal-components" title="Permalink to this headline">¶</a></h2>
<p>Suppose that we wish to visualise n observations with measurements on a set of <span class="math notranslate nohighlight">\( D \)</span>features, <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_D \)</span>, as part of an exploratory data analysis. We could do this by examining two-dimensional scatterplots of the data, each of which contains the <span class="math notranslate nohighlight">\(N\)</span> observations’ measurements on two of the features. However, there are <span class="math notranslate nohighlight">\( D(D-1)/2 \)</span> such scatterplots, and it is difficult to visualise more than a few of them at a time. If <span class="math notranslate nohighlight">\( D \)</span> is large, then it will certainly not be possible to look at all of them; moreover, most likely none of them will be informative since they each contain just a small fraction of the total information present in the data set. Clearly, a better method is required to visualise the n observations when <span class="math notranslate nohighlight">\( D \)</span>is large. In particular, we would like to find a low-dimensional representation of the data that captures as much of the information as possible. For instance, if we can obtain a two-dimensional representation of the data that captures most of the information, then we can plot the observations in this low-dimensional space.</p>
<p>PCA provides a tool to do just this. It finds a low-dimensional representation of a data set that contains as much as possible of the variation. The idea is that each of the n observations lives in <span class="math notranslate nohighlight">\(D\)</span>-dimensional space, but not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. Each of the dimensions found by PCA is a linear combination of the <span class="math notranslate nohighlight">\( D \)</span> features. We now explain the manner in which these dimensions, or principal components, are found.</p>
<p>The first principal component of a set of features <span class="math notranslate nohighlight">\(x_1, \ldots, x_D\)</span> is the normalised linear combination of the features</p>
<div class="math notranslate nohighlight">
\[
z_1 = \phi_{1,1}x_1 + \phi_{1,2}x_2 + \cdots + \phi_{1,D}x_D
\]</div>
<p>that has the largest variance. By normalised, we mean that <span class="math notranslate nohighlight">\( \sum_{d=1}^D \phi_{d,1}^2 = 1\)</span>. We refer to the elements <span class="math notranslate nohighlight">\(\phi_{n, 1}, \ldots, \phi_{n, D} \)</span> as the loadings of the first principal loading component; together, the loadings make up the principal component loading vector, <span class="math notranslate nohighlight">\(\boldsymbol{\phi} = [\phi_{1,1}, \ldots, \phi_{1,D}]^\top\)</span>. We constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.</p>
<p>Given a <span class="math notranslate nohighlight">\(D \times N\)</span> data set <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, how do we compute the first principal component? Since we are only interested in variance, we assume that each of the variables in <span class="math notranslate nohighlight">\( \mathbf{X} \)</span> has been centred to have mean zero (that is, the column means of <span class="math notranslate nohighlight">\( \mathbf{X} \)</span> are zero). We then look for the linear combination of the sample feature values of the form</p>
<div class="math notranslate nohighlight">
\[
z_{n,1} = \phi_{1,1}x_{n,1} + \phi_{2,1}x_{n,2} + \cdots + \phi_{D,1}x_{n,D}
\]</div>
<p>that has largest sample variance, subject to the constraint that <span class="math notranslate nohighlight">\(\sum_{d=1}^D \phi_{d,1}^2 = 1\)</span>. In other words, the first principal component loading vector solves the optimisation problem</p>
<div class="math notranslate nohighlight" id="equation-eq-1stpc">
<span class="eqno">(9.1)<a class="headerlink" href="#equation-eq-1stpc" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\arg\max_{\boldsymbol{\phi}} \left\{\frac{1}{N} \sum_{n=1}^N \left(\sum_{d=1}^D \phi_{d,1} x_{d,n} \right)^2\right\} \quad \text{subject to} \quad \sum_{d=1}^D \phi_{d,1}^2 = 1
\end{equation}\]</div>
<p>where the objective can also be written as <span class="math notranslate nohighlight">\(\frac{1}{N} \sum_{n=1}^N z_{n,1}^2 \)</span>. Equation <a class="reference internal" href="#equation-eq-1stpc">(9.1)</a> maximises the sample variance of the <span class="math notranslate nohighlight">\( N \)</span> values of <span class="math notranslate nohighlight">\( z_{n,1} \)</span>. We refer <span class="math notranslate nohighlight">\( z_{1,1}, \ldots, z_{N,1} \)</span> as the first principal component scores. Equation <a class="reference internal" href="#equation-eq-1stpc">(9.1)</a> can be solved via a standard technique in linear algebra, <a class="reference external" href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix"><em>eigen decomposition</em></a>, the mathematical details of which are beyond the scope of this course.</p>
<p>There is a nice geometric interpretation for the first principal component. The loading vector <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_1 \)</span> with elements <span class="math notranslate nohighlight">\( \phi_{1,1}, \ldots, \phi_{D, 1} \)</span> defines a direction in feature space along which the data vary the most. If we project the <span class="math notranslate nohighlight">\( N \)</span> data points <span class="math notranslate nohighlight">\(\mathbf{x}_1, \ldots, \mathbf{x}_N\)</span> onto this direction, the projected values are the principal component scores <span class="math notranslate nohighlight">\( z_{1,1} , \ldots , z_{N,1 }\)</span> themselves.</p>
<p>After the first principal component <span class="math notranslate nohighlight">\( z_1 \)</span> of the features has been determined, we can find the second principal component <span class="math notranslate nohighlight">\( z_2 \)</span>. The second principal component is the linear combination of <span class="math notranslate nohighlight">\( x_1 , \ldots, x_D \)</span> that has maximal variance out of all linear combinations that are uncorrelated with <span class="math notranslate nohighlight">\( z_1 \)</span>. The second principal component scores <span class="math notranslate nohighlight">\( z_{1,2}, z_{2,2}, \ldots , z_{N,2}\)</span> take the form</p>
<div class="math notranslate nohighlight">
\[
z_{n,1} = \phi_{1,2}x_{n,1} + \phi_{2,2}x_{n,2} + \cdots + \phi_{D,2}x_{n,D},
\]</div>
<p>where <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_2 \)</span> is the second principal component loading vector, with elements <span class="math notranslate nohighlight">\( \phi_{1,2}, \ldots, \phi_{D, 2} \)</span>. It turns out that constraining <span class="math notranslate nohighlight">\( z_2 \)</span> to be uncorrelated with <span class="math notranslate nohighlight">\( z_1 \)</span> is equivalent to constraining the direction <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_2 \)</span> to be orthogonal (perpendicular) to the direction <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_1 \)</span>.</p>
<p>Once we have computed the principal components, we can plot them against each other in order to produce low-dimensional views of the data.
For instance, we can plot the score vector <span class="math notranslate nohighlight">\( z_1 \)</span> against <span class="math notranslate nohighlight">\( z_2 \)</span>, <span class="math notranslate nohighlight">\( z_1 \)</span> against <span class="math notranslate nohighlight">\( z_3 \)</span>, <span class="math notranslate nohighlight">\( z_2 \)</span> against <span class="math notranslate nohighlight">\( z_3 \)</span>, and so forth. Geometrically, this amounts to projecting the original data down onto the subspace spanned by <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_1 \)</span>, <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_2 \)</span>, and <span class="math notranslate nohighlight">\( \boldsymbol{\phi}_3 \)</span>, and plotting the projected points.</p>
</div>
<div class="section" id="pca-on-usarrests-dataset">
<h2><span class="section-number">9.1.2. </span>PCA on <code class="docutils literal notranslate"><span class="pre">USArrests</span></code> dataset<a class="headerlink" href="#pca-on-usarrests-dataset" title="Permalink to this headline">¶</a></h2>
<p>We illustrate the use of PCA on the <a class="reference external" href="https://github.com/pykale/transparentML/blob/main/data/USArrests.csv">USArrests dataset</a>. For each of the 50 states in the United States, the data set contains the number of arrests per 100,000 residents for each of three crimes: <code class="docutils literal notranslate"><span class="pre">Assault</span></code>, <code class="docutils literal notranslate"><span class="pre">Murder</span></code>, and <code class="docutils literal notranslate"><span class="pre">Rape</span></code>. We also record <code class="docutils literal notranslate"><span class="pre">UrbanPop</span></code> (the percent of the population in each state living in urban areas). The principal component score vectors have length <span class="math notranslate nohighlight">\(N = 50\)</span>, and the principal component loading vectors have length <span class="math notranslate nohighlight">\(D = 4\)</span>. PCA was performed after standardizing each variable to have mean zero and standard deviation one.</p>
<p><strong>Import libraries and load data</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import pandas as pd
import numpy as np
from numpy.linalg import svd
import matplotlib as mpl
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.cluster import hierarchy

%matplotlib inline
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>data_url = &quot;https://github.com/pykale/transparentML/raw/main/data/USArrests.csv&quot;

USArrests = pd.read_csv(data_url, header=0, index_col=0)
USArrests.head()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Murder</th>
      <th>Assault</th>
      <th>UrbanPop</th>
      <th>Rape</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Alabama</th>
      <td>13.2</td>
      <td>236</td>
      <td>58</td>
      <td>21.2</td>
    </tr>
    <tr>
      <th>Alaska</th>
      <td>10.0</td>
      <td>263</td>
      <td>48</td>
      <td>44.5</td>
    </tr>
    <tr>
      <th>Arizona</th>
      <td>8.1</td>
      <td>294</td>
      <td>80</td>
      <td>31.0</td>
    </tr>
    <tr>
      <th>Arkansas</th>
      <td>8.8</td>
      <td>190</td>
      <td>50</td>
      <td>19.5</td>
    </tr>
    <tr>
      <th>California</th>
      <td>9.0</td>
      <td>276</td>
      <td>91</td>
      <td>40.6</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(USArrests.mean())
print(USArrests.var())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Murder        7.788
Assault     170.760
UrbanPop     65.540
Rape         21.232
dtype: float64
Murder        18.970465
Assault     6945.165714
UrbanPop     209.518776
Rape          87.729159
dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>scaler = StandardScaler()
X = pd.DataFrame(
    scaler.fit_transform(USArrests), index=USArrests.index, columns=USArrests.columns
)
# The loading vectors (i.e. these are the projection of the data onto the principal components)
pca_loadings = pd.DataFrame(
    PCA().fit(X).components_.T,
    index=USArrests.columns,
    columns=[&quot;V1&quot;, &quot;V2&quot;, &quot;V3&quot;, &quot;V4&quot;],
)
pca_loadings

&quot;&quot;&quot; 
Depends on the version of python/module, you may see a flipped loading vector in signs. 
This is normal because the orientation of the principal components is not deterministic. 
&quot;&quot;&quot;
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39; \nDepends on the version of python/module, you may see a flipped loading vector in signs. \nThis is normal because the orientation of the principal components is not deterministic. \n&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># fit the PCA model and transform X to get the principal components
pca = PCA()
df_plot = pd.DataFrame(
    pca.fit_transform(X), columns=[&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;PC4&quot;], index=X.index
)
df_plot
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PC1</th>
      <th>PC2</th>
      <th>PC3</th>
      <th>PC4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Alabama</th>
      <td>0.985566</td>
      <td>1.133392</td>
      <td>-0.444269</td>
      <td>0.156267</td>
    </tr>
    <tr>
      <th>Alaska</th>
      <td>1.950138</td>
      <td>1.073213</td>
      <td>2.040003</td>
      <td>-0.438583</td>
    </tr>
    <tr>
      <th>Arizona</th>
      <td>1.763164</td>
      <td>-0.745957</td>
      <td>0.054781</td>
      <td>-0.834653</td>
    </tr>
    <tr>
      <th>Arkansas</th>
      <td>-0.141420</td>
      <td>1.119797</td>
      <td>0.114574</td>
      <td>-0.182811</td>
    </tr>
    <tr>
      <th>California</th>
      <td>2.523980</td>
      <td>-1.542934</td>
      <td>0.598557</td>
      <td>-0.341996</td>
    </tr>
    <tr>
      <th>Colorado</th>
      <td>1.514563</td>
      <td>-0.987555</td>
      <td>1.095007</td>
      <td>0.001465</td>
    </tr>
    <tr>
      <th>Connecticut</th>
      <td>-1.358647</td>
      <td>-1.088928</td>
      <td>-0.643258</td>
      <td>-0.118469</td>
    </tr>
    <tr>
      <th>Delaware</th>
      <td>0.047709</td>
      <td>-0.325359</td>
      <td>-0.718633</td>
      <td>-0.881978</td>
    </tr>
    <tr>
      <th>Florida</th>
      <td>3.013042</td>
      <td>0.039229</td>
      <td>-0.576829</td>
      <td>-0.096285</td>
    </tr>
    <tr>
      <th>Georgia</th>
      <td>1.639283</td>
      <td>1.278942</td>
      <td>-0.342460</td>
      <td>1.076797</td>
    </tr>
    <tr>
      <th>Hawaii</th>
      <td>-0.912657</td>
      <td>-1.570460</td>
      <td>0.050782</td>
      <td>0.902807</td>
    </tr>
    <tr>
      <th>Idaho</th>
      <td>-1.639800</td>
      <td>0.210973</td>
      <td>0.259801</td>
      <td>-0.499104</td>
    </tr>
    <tr>
      <th>Illinois</th>
      <td>1.378911</td>
      <td>-0.681841</td>
      <td>-0.677496</td>
      <td>-0.122021</td>
    </tr>
    <tr>
      <th>Indiana</th>
      <td>-0.505461</td>
      <td>-0.151563</td>
      <td>0.228055</td>
      <td>0.424666</td>
    </tr>
    <tr>
      <th>Iowa</th>
      <td>-2.253646</td>
      <td>-0.104054</td>
      <td>0.164564</td>
      <td>0.017556</td>
    </tr>
    <tr>
      <th>Kansas</th>
      <td>-0.796881</td>
      <td>-0.270165</td>
      <td>0.025553</td>
      <td>0.206496</td>
    </tr>
    <tr>
      <th>Kentucky</th>
      <td>-0.750859</td>
      <td>0.958440</td>
      <td>-0.028369</td>
      <td>0.670557</td>
    </tr>
    <tr>
      <th>Louisiana</th>
      <td>1.564818</td>
      <td>0.871055</td>
      <td>-0.783480</td>
      <td>0.454728</td>
    </tr>
    <tr>
      <th>Maine</th>
      <td>-2.396829</td>
      <td>0.376392</td>
      <td>-0.065682</td>
      <td>-0.330460</td>
    </tr>
    <tr>
      <th>Maryland</th>
      <td>1.763369</td>
      <td>0.427655</td>
      <td>-0.157250</td>
      <td>-0.559070</td>
    </tr>
    <tr>
      <th>Massachusetts</th>
      <td>-0.486166</td>
      <td>-1.474496</td>
      <td>-0.609497</td>
      <td>-0.179599</td>
    </tr>
    <tr>
      <th>Michigan</th>
      <td>2.108441</td>
      <td>-0.155397</td>
      <td>0.384869</td>
      <td>0.102372</td>
    </tr>
    <tr>
      <th>Minnesota</th>
      <td>-1.692682</td>
      <td>-0.632261</td>
      <td>0.153070</td>
      <td>0.067317</td>
    </tr>
    <tr>
      <th>Mississippi</th>
      <td>0.996494</td>
      <td>2.393796</td>
      <td>-0.740808</td>
      <td>0.215508</td>
    </tr>
    <tr>
      <th>Missouri</th>
      <td>0.696787</td>
      <td>-0.263355</td>
      <td>0.377444</td>
      <td>0.225824</td>
    </tr>
    <tr>
      <th>Montana</th>
      <td>-1.185452</td>
      <td>0.536874</td>
      <td>0.246889</td>
      <td>0.123742</td>
    </tr>
    <tr>
      <th>Nebraska</th>
      <td>-1.265637</td>
      <td>-0.193954</td>
      <td>0.175574</td>
      <td>0.015893</td>
    </tr>
    <tr>
      <th>Nevada</th>
      <td>2.874395</td>
      <td>-0.775600</td>
      <td>1.163380</td>
      <td>0.314515</td>
    </tr>
    <tr>
      <th>New Hampshire</th>
      <td>-2.383915</td>
      <td>-0.018082</td>
      <td>0.036855</td>
      <td>-0.033137</td>
    </tr>
    <tr>
      <th>New Jersey</th>
      <td>0.181566</td>
      <td>-1.449506</td>
      <td>-0.764454</td>
      <td>0.243383</td>
    </tr>
    <tr>
      <th>New Mexico</th>
      <td>1.980024</td>
      <td>0.142849</td>
      <td>0.183692</td>
      <td>-0.339534</td>
    </tr>
    <tr>
      <th>New York</th>
      <td>1.682577</td>
      <td>-0.823184</td>
      <td>-0.643075</td>
      <td>-0.013484</td>
    </tr>
    <tr>
      <th>North Carolina</th>
      <td>1.123379</td>
      <td>2.228003</td>
      <td>-0.863572</td>
      <td>-0.954382</td>
    </tr>
    <tr>
      <th>North Dakota</th>
      <td>-2.992226</td>
      <td>0.599119</td>
      <td>0.301277</td>
      <td>-0.253987</td>
    </tr>
    <tr>
      <th>Ohio</th>
      <td>-0.225965</td>
      <td>-0.742238</td>
      <td>-0.031139</td>
      <td>0.473916</td>
    </tr>
    <tr>
      <th>Oklahoma</th>
      <td>-0.311783</td>
      <td>-0.287854</td>
      <td>-0.015310</td>
      <td>0.010332</td>
    </tr>
    <tr>
      <th>Oregon</th>
      <td>0.059122</td>
      <td>-0.541411</td>
      <td>0.939833</td>
      <td>-0.237781</td>
    </tr>
    <tr>
      <th>Pennsylvania</th>
      <td>-0.888416</td>
      <td>-0.571100</td>
      <td>-0.400629</td>
      <td>0.359061</td>
    </tr>
    <tr>
      <th>Rhode Island</th>
      <td>-0.863772</td>
      <td>-1.491978</td>
      <td>-1.369946</td>
      <td>-0.613569</td>
    </tr>
    <tr>
      <th>South Carolina</th>
      <td>1.320724</td>
      <td>1.933405</td>
      <td>-0.300538</td>
      <td>-0.131467</td>
    </tr>
    <tr>
      <th>South Dakota</th>
      <td>-1.987775</td>
      <td>0.823343</td>
      <td>0.389293</td>
      <td>-0.109572</td>
    </tr>
    <tr>
      <th>Tennessee</th>
      <td>0.999742</td>
      <td>0.860251</td>
      <td>0.188083</td>
      <td>0.652864</td>
    </tr>
    <tr>
      <th>Texas</th>
      <td>1.355138</td>
      <td>-0.412481</td>
      <td>-0.492069</td>
      <td>0.643195</td>
    </tr>
    <tr>
      <th>Utah</th>
      <td>-0.550565</td>
      <td>-1.471505</td>
      <td>0.293728</td>
      <td>-0.082314</td>
    </tr>
    <tr>
      <th>Vermont</th>
      <td>-2.801412</td>
      <td>1.402288</td>
      <td>0.841263</td>
      <td>-0.144890</td>
    </tr>
    <tr>
      <th>Virginia</th>
      <td>-0.096335</td>
      <td>0.199735</td>
      <td>0.011713</td>
      <td>0.211371</td>
    </tr>
    <tr>
      <th>Washington</th>
      <td>-0.216903</td>
      <td>-0.970124</td>
      <td>0.624871</td>
      <td>-0.220848</td>
    </tr>
    <tr>
      <th>West Virginia</th>
      <td>-2.108585</td>
      <td>1.424847</td>
      <td>0.104775</td>
      <td>0.131909</td>
    </tr>
    <tr>
      <th>Wisconsin</th>
      <td>-2.079714</td>
      <td>-0.611269</td>
      <td>-0.138865</td>
      <td>0.184104</td>
    </tr>
    <tr>
      <th>Wyoming</th>
      <td>-0.629427</td>
      <td>0.321013</td>
      <td>-0.240659</td>
      <td>-0.166652</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax1 = plt.subplots(figsize=(9, 7))

ax1.set_xlim(-3.5, 3.5)
ax1.set_ylim(-3.5, 3.5)

# plot Principal Components 1 and 2
for i in df_plot.index:
    ax1.annotate(i, (df_plot.PC1.loc[i], -df_plot.PC2.loc[i]), ha=&quot;center&quot;)

# plot reference lines
ax1.hlines(0, -3.5, 3.5, linestyles=&quot;dotted&quot;, colors=&quot;grey&quot;)
ax1.vlines(0, -3.5, 3.5, linestyles=&quot;dotted&quot;, colors=&quot;grey&quot;)

ax1.set_xlabel(&quot;First Principal Component&quot;)
ax1.set_ylabel(&quot;Second Principal Component&quot;)

# plot Principal Component loading vectors, using a second y-axis.
ax2 = ax1.twinx().twiny()

ax2.set_ylim(-1, 1)
ax2.set_xlim(-1, 1)
ax2.tick_params(axis=&quot;y&quot;, colors=&quot;orange&quot;)
ax2.set_xlabel(&quot;Principal Component loading vectors&quot;, color=&quot;orange&quot;)

# plot labels for vectors. Variable &#39;a&#39; is a small offset parameter to separate arrow tip and text.
a = 1.07
for i in pca_loadings[[&quot;V1&quot;, &quot;V2&quot;]].index:
    ax2.annotate(
        i, (pca_loadings.V1.loc[i] * a, -pca_loadings.V2.loc[i] * a), color=&quot;orange&quot;
    )

# plot vectors
ax2.arrow(0, 0, pca_loadings.V1[0], -pca_loadings.V2[0])
ax2.arrow(0, 0, pca_loadings.V1[1], -pca_loadings.V2[1])
ax2.arrow(0, 0, pca_loadings.V1[2], -pca_loadings.V2[2])
ax2.arrow(0, 0, pca_loadings.V1[3], -pca_loadings.V2[3])
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pca_9_0.png" src="../_images/pca_9_0.png" />
</div>
</div>
<p>In the figure above, we can see that the first loading vector places approximately equal weight on <code class="docutils literal notranslate"><span class="pre">Assault</span></code>, <code class="docutils literal notranslate"><span class="pre">Murder</span></code>, and <code class="docutils literal notranslate"><span class="pre">Rape</span></code>, but with much less weight on <code class="docutils literal notranslate"><span class="pre">UrbanPop</span></code>. Hence this component roughly corresponds to a measure of overall rates of serious crimes. The second loading vector places most of its weight on <code class="docutils literal notranslate"><span class="pre">UrbanPop</span></code> and much less weight on the other three features. Hence, this component roughly corresponds to the level of urbanisation of the state. Overall, we see that the crime-related variables ( <code class="docutils literal notranslate"><span class="pre">Murder</span></code>, <code class="docutils literal notranslate"><span class="pre">Assault</span></code>, and <code class="docutils literal notranslate"><span class="pre">Rape</span></code>) are located close to each other, and that the <code class="docutils literal notranslate"><span class="pre">UrbanPop</span></code> variable is far from the other three. This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the <code class="docutils literal notranslate"><span class="pre">UrbanPop</span></code> variable is less correlated with the other three.</p>
<p>We can examine differences between the states via the two principal component score vectors shown in the figure above. Our discussion of the loading vectors suggests that states with large positive scores on the first component, such as California, Nevada and Florida, have high crime rates, while states like North Dakota, with negative scores on the first component, have low crime rates. California also has a high score on the second component, indicating a high level of urbanisation, while the opposite is true for states like Mississippi. States close to zero on both components, such as Indiana, have approximately average levels of both crime and urbanisation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pca_loadings
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Murder</th>
      <td>0.535899</td>
      <td>0.418181</td>
      <td>-0.341233</td>
      <td>0.649228</td>
    </tr>
    <tr>
      <th>Assault</th>
      <td>0.583184</td>
      <td>0.187986</td>
      <td>-0.268148</td>
      <td>-0.743407</td>
    </tr>
    <tr>
      <th>UrbanPop</th>
      <td>0.278191</td>
      <td>-0.872806</td>
      <td>-0.378016</td>
      <td>0.133878</td>
    </tr>
    <tr>
      <th>Rape</th>
      <td>0.543432</td>
      <td>-0.167319</td>
      <td>0.817778</td>
      <td>0.089024</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="the-proportion-of-variance-explained">
<h2><span class="section-number">9.1.3. </span>The proportion of variance explained<a class="headerlink" href="#the-proportion-of-variance-explained" title="Permalink to this headline">¶</a></h2>
<p>We can now ask a natural question: how much of the information in a given data set is lost by projecting the observations onto the first few principal components? That is, how much of the variance in the data is not contained in the first few principal components? More generally, we are interested in knowing the proportion of variance explained (PVE) by each principal component. The total variance present in a data set (assuming that the variables have been centred to have mean zero) is defined as</p>
<div class="math notranslate nohighlight">
\[
\sum_{d=1}^D \textrm{Var}(\mathbf{x}_d) = \sum_{d=1}^D \frac{1}{N} \sum_{n=1}^N x_{d,n}^2.
\]</div>
<p>and the variance explained by the <span class="math notranslate nohighlight">\(i\textrm{th}\)</span> principal component is</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{N}\sum_{n=1}^N z_{n,i}^2 = \frac{1}{N}\sum_{n=1}^N \left(\sum_{d=1}^D \phi_{d,i} x_{n,d}\right)^2.
\]</div>
<p>Therefore, the PVE of the <span class="math notranslate nohighlight">\(i\textrm{th}\)</span> principal component is given by</p>
<div class="math notranslate nohighlight" id="equation-eq-pve">
<span class="eqno">(9.2)<a class="headerlink" href="#equation-eq-pve" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{\sum_{n=1}^N z_{n,i}^2}{\sum_{d=1}^D\sum_{n=1}^N  x_{d,n}} = \frac{\sum_{n=1}^N \left(\sum_{d=1}^D \phi_{d,i} x_{n,d}\right)^2}{\sum_{d=1}^D\sum_{n=1}^N  x_{d,n}}.
\end{equation}\]</div>
<p>The PVE of each principal component is a positive quantity. In order to compute the cumulative PVE of the first <span class="math notranslate nohighlight">\(i\)</span> principal components, we can simply sum Equation <a class="reference internal" href="#equation-eq-pve">(9.2)</a> over each of the first i PVEs. In total, there are <span class="math notranslate nohighlight">\( \min (N − 1, D) \)</span> principal components, and their PVEs sum to one.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, the portion of explained variance to select the number of PCs are  available in the <code class="docutils literal notranslate"><span class="pre">PCA</span></code> object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(pca.explained_variance_)
print(pca.explained_variance_ratio_)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2.53085875 1.00996444 0.36383998 0.17696948]
[0.62006039 0.24744129 0.0891408  0.04335752]
</pre></div>
</div>
</div>
</div>
<p>In the <code class="docutils literal notranslate"><span class="pre">USArrests</span></code> data, the first principal component explains 62.0 % of the variance in the data, and the next principal component explains 24.7 % of the variance. Together, the first two principal components explain almost 87 % of the variance in the data, and the last two principal components explain only 13 % of the variance. Run the code cell below to plot The PVE of each principal component, as well as the cumulative PVE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.figure(figsize=(7, 5))

plt.plot(
    [1, 2, 3, 4], pca.explained_variance_ratio_, &quot;-o&quot;, label=&quot;Individual component&quot;
)
plt.plot(
    [1, 2, 3, 4], np.cumsum(pca.explained_variance_ratio_), &quot;-s&quot;, label=&quot;Cumulative&quot;
)

plt.ylabel(&quot;Proportion of Variance Explained&quot;)
plt.xlabel(&quot;Principal Component&quot;)
plt.xlim(0.75, 4.25)
plt.ylim(0, 1.05)
plt.xticks([1, 2, 3, 4])
plt.legend(loc=2)
plt.show()

&quot;&quot;&quot;
In this case, if we want to preserve 80% of variance of the data, we need to select 2 PCs.
&quot;&quot;&quot;
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pca_15_0.png" src="../_images/pca_15_0.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;\nIn this case, if we want to preserve 80% of variance of the data, we need to select 2 PCs.\n&#39;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="pca-on-toy-data-and-system-transparency">
<h2><span class="section-number">9.1.4. </span>PCA on toy data and system transparency<a class="headerlink" href="#pca-on-toy-data-and-system-transparency" title="Permalink to this headline">¶</a></h2>
<p>The example is adapted from <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html#sphx-glr-auto-examples-cross-decomposition-plot-pcr-vs-pls-py">“Principal Component Regression vs Partial Least Squares Regression”</a> in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>rng = np.random.RandomState(0)
n_samples = 500
cov = [[3, 3], [3, 4]]
X = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)
pca = PCA(n_components=2).fit(X)

fig = plt.figure(figsize=(5, 8))

plt.scatter(X[:, 0], X[:, 1], alpha=0.3, label=&quot;samples&quot;)

for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):
    # comp = comp * var  # scale component by its variance explanation power
    comp = comp
    # plt.plot(
    #     [0, comp[0]],
    #     [0, comp[1]],
    #     label=f&quot;Component {i}&quot;,
    #     linewidth=5,
    #     color=f&quot;C{i + 2}&quot;,
    # )
    plt.arrow(
        x=0,
        y=0,
        dx=comp[0],
        dy=comp[1],
        color=f&quot;C{i + 2}&quot;,
        width=0.1,
        label=f&quot;Component {i}&quot;,
        head_width=0.3
        # [comp[0], comp[1]],
        # width=3,
        # label=f&quot;Component {i}&quot;,
        # # linewidth=5,
        # color=f&quot;C{i + 2}&quot;,
    )

# plt.plot([0, X[1, 0]], [0, X[1, 1]], linewidth=2, color=&quot;k&quot;)
plt.arrow(
    x=0,
    y=0,
    dx=X[1, 0] * 0.85,
    dy=X[1, 1] * 0.85,
    color=&quot;k&quot;,
    width=0.1,
    head_width=0.3,
    label=&quot;Input Data Sample&quot;,
)
plt.scatter(
    X[1, 0],
    X[1, 1],
    alpha=0.8,
    facecolors=&quot;none&quot;,
    marker=&quot;o&quot;,
    edgecolors=&quot;r&quot;,
    linewidths=3,
)

# plt.plot(
#     [0, -pca.components_[1, 0]],
#     [0, -pca.components_[1, 1]],
#     linewidth=2,
#     color=&quot;orange&quot;,
# )

# plt.scatter(pca.transform(X[1, :].reshape(1, -1))[:, 0], pca.transform(X[1, :].reshape(1, -1))[:, 1],
#             alpha=0.3, facecolors=&quot;none&quot;, marker=&quot;o&quot;, edgecolors=&quot;k&quot;, linewidths=3)

plt.gca().set(
    aspect=&quot;equal&quot;,
    title=&quot;2-dimensional dataset with principal components&quot;,
    xlabel=&quot;first feature&quot;,
    ylabel=&quot;second feature&quot;,
)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pca_18_0.png" src="../_images/pca_18_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&quot;The numbers display below round off to 2 decimal places.&quot;)
print(&quot;Data point: &quot;, np.around(X[1, :], 2))
print(
    &quot;Scores of the data poirnt: &quot;, np.around(pca.transform(X[1, :].reshape(1, -1)), 2)
)
print(&quot;Mean of the training data: &quot;, np.around(pca.mean_, 2))
print(&quot;PCA loadings: \n&quot;, np.around(pca.components_, 2))
print(&quot;Explained variance: &quot;, np.around(pca.explained_variance_, 2))
print(&quot;Explained variance ratio: &quot;, np.around(pca.explained_variance_ratio_, 2))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The numbers display below round off to 2 decimal places.
Data point:  [-2.78 -0.93]
Scores of the data poirnt:  [[ 2.67 -1.54]]
Mean of the training data:  [0.12 0.12]
PCA loadings: 
 [[-0.64 -0.77]
 [ 0.77 -0.64]]
Explained variance:  [6.21 0.46]
Explained variance ratio:  [0.93 0.07]
</pre></div>
</div>
</div>
</div>
<div class="important admonition">
<p class="admonition-title">System transparency</p>
<p>Numbers are limited to two decimal places for the sake of clarity.</p>
<ul class="simple">
<li><p>For data point <span class="math notranslate nohighlight">\( [-2.78 -0.93]^{\top} \)</span>, the score vector of PCA can be obtained by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} \underbrace{\left[\begin{matrix} -0.64 &amp; -0.77 \\ 0.77 &amp; -0.64 \end{matrix}\right]}_{\text{Loading}}\left(\underbrace{\left[\begin{matrix} -2.78 \\ -0.93 \end{matrix}\right]}_{\text{Input Data}} - \underbrace{\left[\begin{matrix}0.12 \\ 0.12 \end{matrix}\right]}_{\text{Mean}}\right) = \underbrace{\left[\begin{matrix} 2.67 \\ -1.54 \end{matrix}\right]}_{\text{Scores}} \end{split}\]</div>
<ul class="simple">
<li><p>The above process can be used inversely to obtain the input data from the scores:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split} 
\underbrace{\left[\begin{matrix} -0.64 &amp; -0.77   \\ 0.77 &amp;  -0.64  \end{matrix}\right]^{\top}}_{\text{Loading}} \underbrace{\left[\begin{matrix} 2.67 \\ -1.54 \end{matrix}\right]}_{\text{Scores}} + \underbrace{\left[\begin{matrix}0.12 \\ 0.12 \end{matrix}\right]}_{\text{Mean}} = \underbrace{\left[\begin{matrix} -2.78 \\ -0.93 \end{matrix}\right]}_{\text{Reconstructed Input}}
\end{split}\]</div>
<p>The reconstruction can also be interpreted from the geometric perspective. As displayed in the figure above, the input data point <span class="math notranslate nohighlight">\( [-2.78 -0.93]^{\top} \)</span> (the black vector), can be viewed as the linear combination of the two loading vectors (components 0 and 1 in green and red, respectively). The scores of the input data point are the coefficients of the linear combination. The reconstructed input data point is the linear combination of the scores and the loading vectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\underbrace{\left[\begin{matrix} -0.64 \\ -0.77 \end{matrix}\right]}_{\text{Component 0}} \times \underbrace{2.67}_{\text{Score 0}} + \underbrace{\left[\begin{matrix} 0.77 \\ -0.64 \end{matrix}\right]}_{\text{Component 1}} \times \underbrace{(-1.54)}_{\text{Score 1}} + \underbrace{\left[\begin{matrix}0.12 \\ 0.12 \end{matrix}\right]}_{\text{Mean}} = \underbrace{\left[\begin{matrix} -2.78 \\ -0.93 \end{matrix}\right]}_{\text{Reconstructed Input}}
\end{split}\]</div>
</div>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">9.1.5. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p>min 3 max 5</p>
<p>PCA on the NCI60 Data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = pd.read_csv(&quot;https://github.com/pykale/transparentML/raw/main/data/NCI60_data.csv&quot;)
y = pd.read_csv(&quot;https://github.com/pykale/transparentML/raw/main/data/NCI60_labs.csv&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pca2 = PCA()
X_scaled = StandardScaler().fit_transform(X)
df2_plot = pd.DataFrame(pca2.fit_transform(X_scaled))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

color_idx = pd.factorize(y.iloc[:, 0])[0]
cmap = plt.cm.hsv

# left plot
ax1.scatter(
    df2_plot.iloc[:, 0], -df2_plot.iloc[:, 1], c=color_idx, cmap=cmap, alpha=0.5, s=50
)
ax1.set_ylabel(&quot;Principal Component 2&quot;)

# right plot
ax2.scatter(
    df2_plot.iloc[:, 0], df2_plot.iloc[:, 2], c=color_idx, cmap=cmap, alpha=0.5, s=50
)
ax2.set_ylabel(&quot;Principal Component 3&quot;)

# custom legend for the classes (y) since we do not create scatter plots per class (which could have their own labels).
handles = []
labels = pd.factorize(y.iloc[:, 0].unique())
norm = mpl.colors.Normalize(vmin=0.0, vmax=14.0)

for i, v in zip(labels[0], labels[1]):
    handles.append(mpl.patches.Patch(color=cmap(norm(i)), label=v, alpha=0.5))

ax2.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)

# xlabel for both plots
for ax in fig.axes:
    ax.set_xlabel(&quot;Principal Component 1&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pca_25_0.png" src="../_images/pca_25_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pd.DataFrame(
    [
        df2_plot.iloc[:, :5].std(axis=0, ddof=0).array,
        pca2.explained_variance_ratio_[:5],
        np.cumsum(pca2.explained_variance_ratio_[:5]),
    ],
    index=[&quot;Standard Deviation&quot;, &quot;Proportion of Variance&quot;, &quot;Cumulative Proportion&quot;],
    columns=[&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;PC4&quot;, &quot;PC5&quot;],
)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PC1</th>
      <th>PC2</th>
      <th>PC3</th>
      <th>PC4</th>
      <th>PC5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Standard Deviation</th>
      <td>27.853469</td>
      <td>21.481355</td>
      <td>19.820465</td>
      <td>17.032556</td>
      <td>15.971807</td>
    </tr>
    <tr>
      <th>Proportion of Variance</th>
      <td>0.113589</td>
      <td>0.067562</td>
      <td>0.057518</td>
      <td>0.042476</td>
      <td>0.037350</td>
    </tr>
    <tr>
      <th>Cumulative Proportion</th>
      <td>0.113589</td>
      <td>0.181151</td>
      <td>0.238670</td>
      <td>0.281145</td>
      <td>0.318495</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df2_plot.iloc[:, :10].var(axis=0, ddof=0).plot(kind=&quot;bar&quot;, rot=0)
plt.ylabel(&quot;Variances&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pca_27_0.png" src="../_images/pca_27_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# left plot
ax1.plot(pca2.explained_variance_ratio_, &quot;-o&quot;)
ax1.set_ylabel(&quot;Proportion of Variance Explained&quot;)
ax1.set_ylim(ymin=-0.01)

# right plot
ax2.plot(np.cumsum(pca2.explained_variance_ratio_), &quot;-ro&quot;)
ax2.set_ylabel(&quot;Cumulative Proportion of Variance Explained&quot;)
ax2.set_ylim(ymax=1.05)

for ax in fig.axes:
    ax.set_xlabel(&quot;Principal Component&quot;)
    ax.set_xlim(-1, 65)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pca_28_0.png" src="../_images/pca_28_0.png" />
</div>
</div>
<!-- For data point $ [-2.77595836 -0.929101]^{\top} $, the score vector of PCA can be obtained by 

$$ \underbrace{\left[\begin{matrix} -0.64402153, -0.76500736 \\ 0.76500736 -0.64402153 \end{matrix}\right]^{\top}}_{\text{Loading}}\left(\underbrace{\left[\begin{matrix} -2.77595836 \\ -0.929101\end{matrix}\right]}_{\text{Input Data}} - \underbrace{\left[\begin{matrix}0.12105555 \\ 0.11655374\end{matrix}\right]}_{\text{Mean of Training Data}}\right) = \left[\begin{matrix}-2.77595836 \\ -0.929101\end{matrix}\right] $$ -->
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./09-pca-kmeans"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="overview.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">9. </span>Principal Component Analysis &amp; <span class="math notranslate nohighlight">\(K\)</span>-Means Clustering</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="kmeans.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9.2. </span><span class="math notranslate nohighlight">\(K\)</span>-means clustering</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Haiping Lu and Shuo Zhou<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>